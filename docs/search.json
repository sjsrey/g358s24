[
  {
    "objectID": "week-10/index.html",
    "href": "week-10/index.html",
    "title": "Global Autocorrelation Tests",
    "section": "",
    "text": "Zoom recording (Canvas access)",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Global Autocorrelation Tests"
    ]
  },
  {
    "objectID": "week-11/index.html",
    "href": "week-11/index.html",
    "title": "Exercise Two",
    "section": "",
    "text": "Zoom recording (Canvas access)",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Exercise Two"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#point-pattern-analysis-objectives",
    "href": "week-12/point-patterns.html#point-pattern-analysis-objectives",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Objectives",
    "text": "Point Pattern Analysis Objectives\nGoals\n\nPattern detection\nAssessing the presence of clustering\nIdentification of individual clusters\n\nGeneral Approaches\n\nEstimate intensity of the process\nFormulating an idealized model and investigating deviations from expectations\nFormulating a stochastic model and fitting it to the data",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#point-pattern-analysis-definitions",
    "href": "week-12/point-patterns.html#point-pattern-analysis-definitions",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Definitions",
    "text": "Point Pattern Analysis Definitions\nSpatial Point Pattern: A set of events, irregularly distributed within a region \\(A\\) and presumed to have been generated by some form of stochastic mechanism.\nRepresentation \\(\\left\\{Y(A),\n    A \\subset \\Re \\right\\}\\), where \\(Y(A)\\) is the number of events occurring in area \\(A\\).\nEvents, points, locations\n\nEvent\n\nan occurrence of interest\n\nPoint\n\nany location in study area\n\nEvent location\n\na particular point where an event occurs",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#point-pattern-analysis-definitions-1",
    "href": "week-12/point-patterns.html#point-pattern-analysis-definitions-1",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Definitions",
    "text": "Point Pattern Analysis Definitions\nRegion: \\(A\\)\n\nMost often planar (two-dimensional Euclidean space)\nOne dimensional applications also possible\nThree-dimensional increasingly popular (space + time)\nPoint processes on networks (non-planar)",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#space-time-point-patterns",
    "href": "week-12/point-patterns.html#space-time-point-patterns",
    "title": "Point Pattern Basics",
    "section": "Space-Time Point Patterns",
    "text": "Space-Time Point Patterns",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#space-time-point-patterns-1",
    "href": "week-12/point-patterns.html#space-time-point-patterns-1",
    "title": "Point Pattern Basics",
    "section": "Space-Time Point Patterns",
    "text": "Space-Time Point Patterns",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#point-patterns-on-networks",
    "href": "week-12/point-patterns.html#point-patterns-on-networks",
    "title": "Point Pattern Basics",
    "section": "Point Patterns on Networks",
    "text": "Point Patterns on Networks",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#point-patterns",
    "href": "week-12/point-patterns.html#point-patterns",
    "title": "Point Pattern Basics",
    "section": "Point Patterns",
    "text": "Point Patterns\nUnmarked Point Patterns\n\nOnly location is recorded\nAttribute is binary (presence, absence)\n\nMarked Point Patterns\n\nLocation is recorded\nNon-binary stochastic attribute\ne.g., sales at a retail store, dbh of tree",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#realizations",
    "href": "week-12/point-patterns.html#realizations",
    "title": "Point Pattern Basics",
    "section": "Realizations",
    "text": "Realizations\nMapped Point Patterns\n\nAll events are recorded and mapped\nComplete enumeration of events\nFull information on the realization from the process\n\nSampled Point Patterns\n\nSample of events are recorded and mapped\nComplete enumeration of events impossible or intractable\nPartial information on the realization from the process\nPresence/“absence” data (ecology, forestry)",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#research-questions-1",
    "href": "week-12/point-patterns.html#research-questions-1",
    "title": "Point Pattern Basics",
    "section": "Research Questions",
    "text": "Research Questions\n\nLocation Only are points randomly located or patterned\nLocation and Value\n\nmarked point pattern\nis combination of location and value random or patterned\n\n\nBoth Cases: What is the Underlying Process?",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#points-on-a-plane-planar-point-pattern-anaysis",
    "href": "week-12/point-patterns.html#points-on-a-plane-planar-point-pattern-anaysis",
    "title": "Point Pattern Basics",
    "section": "Points on a Plane (Planar Point Pattern Anaysis)",
    "text": "Points on a Plane (Planar Point Pattern Anaysis)\nClassic Point Pattern Analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nno directional effects\nno translational effects\nstraight line distance only",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#events-point-map",
    "href": "week-12/point-patterns.html#events-point-map",
    "title": "Point Pattern Basics",
    "section": "Events: Point Map",
    "text": "Events: Point Map",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#points-in-context",
    "href": "week-12/point-patterns.html#points-in-context",
    "title": "Point Pattern Basics",
    "section": "Points in Context",
    "text": "Points in Context",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#intensity",
    "href": "week-12/point-patterns.html#intensity",
    "title": "Point Pattern Basics",
    "section": "Intensity",
    "text": "Intensity\nFirst Moment\n\nnumber of points \\(N\\), area of study \\(|A|\\)\nintensity: \\(\\lambda = N/|A|\\)\narea depends on bounds, often arbitrary\n\nArtificial Boundaries\n\nbounding box (rectangle, square)\nother (city boundary)",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#bounding-box",
    "href": "week-12/point-patterns.html#bounding-box",
    "title": "Point Pattern Basics",
    "section": "Bounding Box",
    "text": "Bounding Box",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#district-boundary",
    "href": "week-12/point-patterns.html#district-boundary",
    "title": "Point Pattern Basics",
    "section": "District Boundary",
    "text": "District Boundary",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#convex-hull",
    "href": "week-12/point-patterns.html#convex-hull",
    "title": "Point Pattern Basics",
    "section": "Convex Hull",
    "text": "Convex Hull\n\nTightest fit various algorithms\nRescaled Convex Hull (Ripley-Rasson)\n\nadjust to properly reflect spatial domain of point process\nuse centroid of convex hull\nrescale by \\(1/[\\sqrt{(1-m/N)}]\\)\n\\(m\\): number of vertices of convex hull",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#convex-hull-1",
    "href": "week-12/point-patterns.html#convex-hull-1",
    "title": "Point Pattern Basics",
    "section": "Convex Hull",
    "text": "Convex Hull",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#multiple-boundaries",
    "href": "week-12/point-patterns.html#multiple-boundaries",
    "title": "Point Pattern Basics",
    "section": "Multiple Boundaries",
    "text": "Multiple Boundaries",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-12/point-patterns.html#intensity-estimates",
    "href": "week-12/point-patterns.html#intensity-estimates",
    "title": "Point Pattern Basics",
    "section": "Intensity Estimates",
    "text": "Intensity Estimates\n\n\n\n\nArea\nIntensity\n\n\n\n\\(km^2\\)\n\\(cases/km^2\\)\n\n\nDistrict Boundary\n315.155\n3.29\n\n\nBounding Box\n310.951\n3.33\n\n\nConvex Hull\n229.421\n4.52\n\n\n\n\nN=1036",
    "crumbs": [
      "Week 12 4/09, 4/11",
      "Point Pattern Basics"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#what-is-unix",
    "href": "week-02/files-dirs.html#what-is-unix",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "What is Unix?",
    "text": "What is Unix?\n\nUnix is a powerful operating system used in many fields, including data science and computer science.\nUnderstanding its file system is crucial for efficient work.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#unix-file-system-basics",
    "href": "week-02/files-dirs.html#unix-file-system-basics",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Unix File System Basics",
    "text": "Unix File System Basics\n\nThe Unix file system is hierarchical.\nIt consists of directories and files organized in a tree-like structure.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#directory-structure",
    "href": "week-02/files-dirs.html#directory-structure",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Directory Structure",
    "text": "Directory Structure\n\nThe root directory is the top-level directory.\nHome directories are where users have their own spaces.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#where-am-i",
    "href": "week-02/files-dirs.html#where-am-i",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Where am I?",
    "text": "Where am I?\nUsing the File Browser\n\n\n\n\n\n\n\n\n\n\n\n(a) home\n\n\n\n\n\n\n\n\n\n\n\n(b) data\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#where-am-i-1",
    "href": "week-02/files-dirs.html#where-am-i-1",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Where am I?",
    "text": "Where am I?\nUsing the terminal\n\n\n\n\n\n\n\n\n\n\n\n(a) terminal home\n\n\n\n\n\n\n\n\n\n\n\n(b) terminal pwd\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#navigating-the-file-system",
    "href": "week-02/files-dirs.html#navigating-the-file-system",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Navigating the File System",
    "text": "Navigating the File System\n\nUse ls to list the contents of a directory.\nUse tree to list the hierarchical structure of the directory\nLearn how to change directories using cd.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#ls",
    "href": "week-02/files-dirs.html#ls",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "ls",
    "text": "ls\nls lists the contents of the pwd",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#tree",
    "href": "week-02/files-dirs.html#tree",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "tree",
    "text": "tree\ntree lists the directory structure/tree of the pwd",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#first-level-tree-of-the-root-system",
    "href": "week-02/files-dirs.html#first-level-tree-of-the-root-system",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "First level tree of the root system",
    "text": "First level tree of the root system",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#first-level-tree-of-the-home-directory",
    "href": "week-02/files-dirs.html#first-level-tree-of-the-home-directory",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "First level tree of the /home directory",
    "text": "First level tree of the /home directory",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#my-home",
    "href": "week-02/files-dirs.html#my-home",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "My Home",
    "text": "My Home",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#all-homes",
    "href": "week-02/files-dirs.html#all-homes",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "All Home(s)",
    "text": "All Home(s)",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#clearing-the-terminal",
    "href": "week-02/files-dirs.html#clearing-the-terminal",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Clearing the terminal",
    "text": "Clearing the terminal\n\nclear will clear the terminal",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#history",
    "href": "week-02/files-dirs.html#history",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "history",
    "text": "history\nTo recall what you have done, use history",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#rerunning-a-command",
    "href": "week-02/files-dirs.html#rerunning-a-command",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Rerunning a command",
    "text": "Rerunning a command\nAfter using history you can rerun a command without having to retype it:\n\n\n\n\n\n\n\n\n\n\n\n(a) recall\n\n\n\n\n\n\n\n\n\n\n\n(b) result\n\n\n\n\n\n\n\nFigure 3",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#tab-completion",
    "href": "week-02/files-dirs.html#tab-completion",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Tab completion",
    "text": "Tab completion\nAnother efficiency feature is tab completion:\n\n\n\n\n\n\n\n\n\n\n\n(a) tab start\n\n\n\n\n\n\n\n\n\n\n\n(b) tab completion\n\n\n\n\n\n\n\nFigure 4",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#paths",
    "href": "week-02/files-dirs.html#paths",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Paths",
    "text": "Paths\n\nPaths are used to specify the location of files and directories.\nThere are relative and absolute paths.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#cd-change-directory",
    "href": "week-02/files-dirs.html#cd-change-directory",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "cd change directory",
    "text": "cd change directory",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#cd-shortcuts",
    "href": "week-02/files-dirs.html#cd-shortcuts",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "cd shortcuts",
    "text": "cd shortcuts\n\n\ncd .. moves up one directory to the parent of the current directory\ncd will move you back to your home",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-and-managing-directories-1",
    "href": "week-02/files-dirs.html#creating-and-managing-directories-1",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating and Managing Directories",
    "text": "Creating and Managing Directories\n\nLearn how to create directories using mkdir.\nUse rmdir and rm -r to remove directories.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#mkdir",
    "href": "week-02/files-dirs.html#mkdir",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "mkdir",
    "text": "mkdir\n\n\n\n\n\n\n\n\n\n\n\n(a) before\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) after\n\n\n\n\n\n\n\nFigure 5",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#file-operations-1",
    "href": "week-02/files-dirs.html#file-operations-1",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "File Operations",
    "text": "File Operations\n\nCreating files with touch.\nCopying, moving, and deleting files.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-a-file-in-a-directory",
    "href": "week-02/files-dirs.html#creating-a-file-in-a-directory",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating a file in a directory",
    "text": "Creating a file in a directory\ncd handson\ntouch data.txt\n\n\nFigure 6",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-a-file-in-a-directory-2",
    "href": "week-02/files-dirs.html#creating-a-file-in-a-directory-2",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating a file in a directory (2)",
    "text": "Creating a file in a directory (2)\nYou can also create a file in the current directory by selecting the + in the toolbar and selecting Markdown File\n\n\nFigure 7: File Browser",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-a-file-in-a-directory-2-1",
    "href": "week-02/files-dirs.html#creating-a-file-in-a-directory-2-1",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating a file in a directory (2)",
    "text": "Creating a file in a directory (2)\nYou can also create a file in the current directory by selecting the + in the toolbar and selecting Markdown File\n\n\nFigure 8: README.md",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#copying-files",
    "href": "week-02/files-dirs.html#copying-files",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Copying Files",
    "text": "Copying Files\ncp data.txt data.txt.bak\n\n\nFigure 9",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#deleting-files",
    "href": "week-02/files-dirs.html#deleting-files",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Deleting Files",
    "text": "Deleting Files\nrm data.txt\n\n\nFigure 10",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#copying-again",
    "href": "week-02/files-dirs.html#copying-again",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Copying Again",
    "text": "Copying Again\nLet’s restore our backup\n\n\nFigure 11",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#copying-between-directories",
    "href": "week-02/files-dirs.html#copying-between-directories",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Copying Between Directories",
    "text": "Copying Between Directories\n\n\nFigure 12",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#moving-files-between-directories",
    "href": "week-02/files-dirs.html#moving-files-between-directories",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Moving Files Between Directories",
    "text": "Moving Files Between Directories\n\n\nFigure 13",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-and-removing-directories",
    "href": "week-02/files-dirs.html#creating-and-removing-directories",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating and Removing Directories",
    "text": "Creating and Removing Directories\n\n\nFigure 14",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#creating-and-removing-directories-1",
    "href": "week-02/files-dirs.html#creating-and-removing-directories-1",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Creating and Removing Directories",
    "text": "Creating and Removing Directories\n\n\nFigure 15",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#exercise-0",
    "href": "week-02/files-dirs.html#exercise-0",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Exercise 0",
    "text": "Exercise 0\n\nCreate a directory called notes\nIn that directory create a new jupyter notebook and rename it to files.ipynb\nPractice using Markdown in files.ipynb to structure your notes with headings\nPractice taking screenshots and dragging the images into your notebook.\nPractice formatting code in Markdown cells.\n\nYou will use this notebook to take notes on the remaining exercises.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#exercise-2",
    "href": "week-02/files-dirs.html#exercise-2",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Exercise 2",
    "text": "Exercise 2\nUsing the File Browser, delete your handson directory and the files living in there",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#exercise-3",
    "href": "week-02/files-dirs.html#exercise-3",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Exercise 3",
    "text": "Exercise 3\nUsing a terminal:\n\nCreate a directory called handson in your home directory\nSelect the new directory in the File Browser\nFrom the Launcher create a new Markdown file with the name README.md\nAdd some content (using Markdown) inside the README.md file\nBefore using any unix commands, ask youself what the path is for the README.md file.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#exercise-4",
    "href": "week-02/files-dirs.html#exercise-4",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Exercise 4",
    "text": "Exercise 4\nUsing a terminal:\n\nCreate a directory called week_2 inside your handson directory\nCopy the README.md file into the week_2 directory.",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-02/files-dirs.html#exercise-5",
    "href": "week-02/files-dirs.html#exercise-5",
    "title": "Introduction to Unix File System with Jupyter Lab",
    "section": "Exercise 5",
    "text": "Exercise 5\nUsing a terminal\n\nCreate a directory called temp in the week_2 directory.\nCreate a file in the temp directory called data.txt\nDelete the directory temp in the week_2 directory",
    "crumbs": [
      "Week 2 1/23, 1/25",
      "Introduction to Unix File System with Jupyter Lab"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html",
    "href": "week-05/pandas_intro.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html#pandas",
    "href": "week-05/pandas_intro.html#pandas",
    "title": "Introduction to Pandas",
    "section": "Pandas",
    "text": "Pandas\n\nSeries\nDataFrame\n\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\n\n\ns\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\n\ndates = pd.date_range(\"20130101\", periods=6)\n\n\ndates\n\nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf = pd.DataFrame(np.random.rand(6,4), index=dates,\n                  columns=list(\"ABCD\"))\n\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\npd.DataFrame(np.random.rand(6,4),\n                  columns=list(\"ABCD\"))\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n0.370312\n0.766785\n0.856606\n0.574230\n\n\n1\n0.477143\n0.770306\n0.154779\n0.427858\n\n\n2\n0.604037\n0.747945\n0.132992\n0.262011\n\n\n3\n0.671085\n0.679174\n0.381207\n0.954910\n\n\n4\n0.662184\n0.029351\n0.363693\n0.170847\n\n\n5\n0.760570\n0.711854\n0.091584\n0.687423\n\n\n\n\n\n\n\n\n\nnp.random.rand(3, 2)\n\narray([[0.31089479, 0.7735978 ],\n       [0.82751496, 0.83485609],\n       [0.86671021, 0.99840853]])\n\n\n\nnp.random.rand?\n\n\nDocstring:\nrand(d0, d1, ..., dn)\nRandom values in a given shape.\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\nSee Also\n--------\nrandom\nExamples\n--------\n&gt;&gt;&gt; np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random\nType:      builtin_function_or_method\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf.dtypes\n\nA    float64\nB    float64\nC    float64\nD    float64\ndtype: object\n\n\n\ndf2 = pd.DataFrame(\n    {\n        \"A\": 1.0,\n        \"B\": pd.Timestamp(\"20130102\"),\n        \"C\": pd.Series(1, index=list(range(4)), dtype='float32'),\n        \"D\": np.array([3] * 4, dtype='int32'),\n        \"E\": pd.Categorical(['test', 'train', 'test', 'train']),\n        \"F\": 'foo',\n    }\n)\n       \n                       \n        \n\n\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n1.0\n2013-01-02\n1.0\n3\ntest\nfoo\n\n\n1\n1.0\n2013-01-02\n1.0\n3\ntrain\nfoo\n\n\n2\n1.0\n2013-01-02\n1.0\n3\ntest\nfoo\n\n\n3\n1.0\n2013-01-02\n1.0\n3\ntrain\nfoo\n\n\n\n\n\n\n\n\n\ndf2.dtypes\n\nA          float64\nB    datetime64[s]\nC          float32\nD            int32\nE         category\nF           object\ndtype: object\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf.index\n\nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf2.index\n\nIndex([0, 1, 2, 3], dtype='int64')\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\ncount\n6.000000\n6.000000\n6.000000\n6.000000\n\n\nmean\n0.627357\n0.427315\n0.274303\n0.594351\n\n\nstd\n0.305672\n0.255655\n0.254303\n0.226134\n\n\nmin\n0.123874\n0.127217\n0.000828\n0.384033\n\n\n25%\n0.514365\n0.202903\n0.092312\n0.464364\n\n\n50%\n0.640363\n0.459740\n0.255408\n0.476319\n\n\n75%\n0.870639\n0.622610\n0.360297\n0.778844\n\n\nmax\n0.939781\n0.723002\n0.700967\n0.889367\n\n\n\n\n\n\n\n\n\ndf.sort_values(by='B')\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n\n\n\n\n\n\n\ndf.sort_values(by='B', ascending=False)\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html#selection",
    "href": "week-05/pandas_intro.html#selection",
    "title": "Introduction to Pandas",
    "section": "selection",
    "text": "selection\n\ndf['C']\n\n2013-01-01    0.229421\n2013-01-02    0.000828\n2013-01-03    0.700967\n2013-01-04    0.386597\n2013-01-05    0.281396\n2013-01-06    0.046609\nFreq: D, Name: C, dtype: float64\n\n\n\ndf[['C', 'D']]\n\n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n2013-01-01\n0.229421\n0.465291\n\n\n2013-01-02\n0.000828\n0.889367\n\n\n2013-01-03\n0.700967\n0.384033\n\n\n2013-01-04\n0.386597\n0.487347\n\n\n2013-01-05\n0.281396\n0.464055\n\n\n2013-01-06\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf[['D', 'C']]\n\n\n\n\n\n\n\n\n\nD\nC\n\n\n\n\n2013-01-01\n0.465291\n0.229421\n\n\n2013-01-02\n0.889367\n0.000828\n\n\n2013-01-03\n0.384033\n0.700967\n\n\n2013-01-04\n0.487347\n0.386597\n\n\n2013-01-05\n0.464055\n0.281396\n\n\n2013-01-06\n0.876010\n0.046609\n\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf[0:3] # selection by location\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n\n\n\n\n\n\n\n# select by label\ndf.loc[:, ['A', 'C']]\n\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n2013-01-01\n0.501461\n0.229421\n\n\n2013-01-02\n0.939781\n0.000828\n\n\n2013-01-03\n0.918302\n0.700967\n\n\n2013-01-04\n0.727648\n0.386597\n\n\n2013-01-05\n0.553077\n0.281396\n\n\n2013-01-06\n0.123874\n0.046609\n\n\n\n\n\n\n\n\n\ndf.loc[dates[0]] # return the records that match the first datestam\n\nA    0.501461\nB    0.130345\nC    0.229421\nD    0.465291\nName: 2013-01-01 00:00:00, dtype: float64\n\n\n\ndates[0]\n\nTimestamp('2013-01-01 00:00:00')\n\n\n\ntype(dates)\n\npandas.core.indexes.datetimes.DatetimeIndex\n\n\n\ndf.loc[dates[0], ['A','C']]\n\nA    0.501461\nC    0.229421\nName: 2013-01-01 00:00:00, dtype: float64",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html#boolean-indexing",
    "href": "week-05/pandas_intro.html#boolean-indexing",
    "title": "Introduction to Pandas",
    "section": "Boolean indexing",
    "text": "Boolean indexing\n\ndf\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf['B']\n\n2013-01-01    0.130345\n2013-01-02    0.723002\n2013-01-03    0.127217\n2013-01-04    0.420577\n2013-01-05    0.663846\n2013-01-06    0.498903\nFreq: D, Name: B, dtype: float64\n\n\n\ndf.B\n\n2013-01-01    0.130345\n2013-01-02    0.723002\n2013-01-03    0.127217\n2013-01-04    0.420577\n2013-01-05    0.663846\n2013-01-06    0.498903\nFreq: D, Name: B, dtype: float64\n\n\n\ndf.B &gt; .5\n\n2013-01-01    False\n2013-01-02     True\n2013-01-03    False\n2013-01-04    False\n2013-01-05     True\n2013-01-06    False\nFreq: D, Name: B, dtype: bool\n\n\n\ndf[df.B &gt; .5]\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n\n\n\n\n\n\n\ndf2 = df.copy()\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\n\n\n\n\n\n\n\n\n\ndf2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\none\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\none\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\ntwo\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\nthree\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\nfour\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\nthree\n\n\n\n\n\n\n\n\n\ndf2['E'].isin([\"one\", \"two\"])\n\n2013-01-01     True\n2013-01-02     True\n2013-01-03     True\n2013-01-04    False\n2013-01-05    False\n2013-01-06    False\nFreq: D, Name: E, dtype: bool\n\n\n\ndf2[df2['E'].isin([\"one\", \"two\"])]\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\none\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\none\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\ntwo\n\n\n\n\n\n\n\n\n\ndf2[df2['E'].isin([\"one\", \"two\", 'four'])]\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\none\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\none\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\ntwo\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\nfour\n\n\n\n\n\n\n\n\n\ndf2[~df2['E'].isin([\"one\", \"two\", 'four'])]\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\nthree\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\nthree",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html#group-by",
    "href": "week-05/pandas_intro.html#group-by",
    "title": "Introduction to Pandas",
    "section": "Group by",
    "text": "Group by\n\ndf2\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2013-01-01\n0.501461\n0.130345\n0.229421\n0.465291\none\n\n\n2013-01-02\n0.939781\n0.723002\n0.000828\n0.889367\none\n\n\n2013-01-03\n0.918302\n0.127217\n0.700967\n0.384033\ntwo\n\n\n2013-01-04\n0.727648\n0.420577\n0.386597\n0.487347\nthree\n\n\n2013-01-05\n0.553077\n0.663846\n0.281396\n0.464055\nfour\n\n\n2013-01-06\n0.123874\n0.498903\n0.046609\n0.876010\nthree\n\n\n\n\n\n\n\n\n\ndf2.groupby(by='E').count()\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\nE\n\n\n\n\n\n\n\n\nfour\n1\n1\n1\n1\n\n\none\n2\n2\n2\n2\n\n\nthree\n2\n2\n2\n2\n\n\ntwo\n1\n1\n1\n1",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-05/pandas_intro.html#plotting",
    "href": "week-05/pandas_intro.html#plotting",
    "title": "Introduction to Pandas",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(123456)\n\n\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\n\n\nts = ts.cumsum()\n\n\nts.plot()",
    "crumbs": [
      "Week 5 2/13, 2/15",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#outline",
    "href": "week-09/spatial_dependence.html#outline",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Outline",
    "text": "Outline\n\nConcepts and Issues\nNull and Alternative Hypotheses\nSpatial Autocorrelation Tests",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence",
    "href": "week-09/spatial_dependence.html#spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\n\nThere is no question with respect to emergent geospatial science. The important harbingers were Geary’s article on spatial autocorrelation, Dacey’s paper about two- and K-color maps, and that of Bachi on geographic series.\n– Berry, Griifth, Tiefelsdorf (2008)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-1",
    "href": "week-09/spatial_dependence.html#spatial-dependence-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nWorking Concept\n\nwhat happens at one place depends on events in nearby places\nall things are related but nearby things are more related than distant things (Tobler)\ncentral focus in lattice data analysis",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#goodchild-1991",
    "href": "week-09/spatial_dependence.html#goodchild-1991",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Goodchild 1991",
    "text": "Goodchild 1991\n\na world without positive spatial dependence would be an impossible world\nimpossible to describe\nimpossible to live in\nhell is a place with no spatial dependence",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-2",
    "href": "week-09/spatial_dependence.html#spatial-dependence-2",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nCategorizing\n\nType: Substantive versus nuisance\nDirection: Positive versus negative\n\nIssues\n\nTime versus space\nInference",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#substantive-spatial-dependence",
    "href": "week-09/spatial_dependence.html#substantive-spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Substantive Spatial Dependence",
    "text": "Substantive Spatial Dependence\nProcess Based\n\nPart of the process under study\nLeaving it out\n\nIncomplete understanding\nBiased inferences",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#nuisance-spatial-dependence",
    "href": "week-09/spatial_dependence.html#nuisance-spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Nuisance Spatial Dependence",
    "text": "Nuisance Spatial Dependence\nNot Process Based\n\nArtifact of data collection\nProcess boundaries not matching data boundaries\nScattering across pixels\nGIS induced",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#boundary",
    "href": "week-09/spatial_dependence.html#boundary",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Boundary",
    "text": "Boundary",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#boundary-mismatch",
    "href": "week-09/spatial_dependence.html#boundary-mismatch",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Boundary Mismatch",
    "text": "Boundary Mismatch\n\n\nEven if \\(A\\) and \\(B\\) are independent\n\\(A'\\) and \\(B'\\) will be dependent",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#nusiance-vs.-substantive-dependence",
    "href": "week-09/spatial_dependence.html#nusiance-vs.-substantive-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Nusiance vs. Substantive Dependence",
    "text": "Nusiance vs. Substantive Dependence\nIssues\n\nNot always easy to differentiate from substantive\nDifferent implications for each type\nSpecification strategies (Econometrics)\nBoth can be operating jointly",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#space-versus-time",
    "href": "week-09/spatial_dependence.html#space-versus-time",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Space versus Time",
    "text": "Space versus Time\nTemporal Dependence\n\nPast influences the future\nRecursive\nOne dimension",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#space-versus-time-1",
    "href": "week-09/spatial_dependence.html#space-versus-time-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Space versus Time",
    "text": "Space versus Time\nSpatial Dependence\n\nMulti-directional\nSimultaneous",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#terminology",
    "href": "week-09/spatial_dependence.html#terminology",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Terminology",
    "text": "Terminology\nRelated Concepts\n\nSpatial Dependence\nSpatial Autocorrelation\nSpatial Association",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-3",
    "href": "week-09/spatial_dependence.html#spatial-dependence-3",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nDistributional Characteristic\n\nMultivariate density function\ndifficult/impossible to verify empirically\n\nDependent Distribution\n\ndoes not factor in marginal densities",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-autocorrelation",
    "href": "week-09/spatial_dependence.html#spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\nAuto = same variable\nCorrelation = scaled covariance\nSpatial - geographic pattern to the correlation",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-autocorrelation-1",
    "href": "week-09/spatial_dependence.html#spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nMeasurement of Moment of Distribution\n\noff-diagonal elements of variance-covariance matrix\nautocovariance\n\\(C[y_i,y_j] \\ne 0 \\ \\forall i\\ne j\\)\ncan be estimated\n\nSpatial Autocorrelation Coefficient\n\nsignificance test on coefficient = 0",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-autocorrelation-2",
    "href": "week-09/spatial_dependence.html#spatial-autocorrelation-2",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nJoint multivariate distribution function \\[f(y) = \\frac{ \\exp\\left[\n-\\frac{1}{2}\n(y-\\mu)'\n\\Sigma^{-1}\n(y-\\mu)\n\\right]}\n{\\sqrt{(2\\pi)^n|\\Sigma|}}\\]",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#variance-covariance-matrix",
    "href": "week-09/spatial_dependence.html#variance-covariance-matrix",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Variance-Covariance Matrix",
    "text": "Variance-Covariance Matrix\n\\[\\Sigma=\n\\left[\n\\begin{array}{rrrr}\n\\sigma_{1,1}&\\sigma_{1,2}&\\ldots&\\sigma_{1,n}\\\\\n\\sigma_{2,1}&\\sigma_{2,2}&\\ldots&\\sigma_{2,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\sigma_{n,1}&\\sigma_{n,2}&\\ldots&\\sigma_{n,n}\n\\end{array}\n\\right]\\]\n\ncovariance: \\(\\sigma_{i,j} = E[(y_i - \\mu_i)(y_j-\\mu_j)      ]\\)\nsymmetry: \\(\\sigma_{i,j} =\\sigma_{i,j}\\)\nvariance: \\(\\sigma_{i,i} = E[(y_i - \\mu_i)(y_i-\\mu_i)      ]\\)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#correlation",
    "href": "week-09/spatial_dependence.html#correlation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Correlation",
    "text": "Correlation\n\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{i}^2}\\sqrt{\\sigma_{j}^2}}\\] \\[-1.0 \\le \\rho_{ij} \\le 1.0\\]",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#data-types-and-autocorrelation",
    "href": "week-09/spatial_dependence.html#data-types-and-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Data Types and Autocorrelation",
    "text": "Data Types and Autocorrelation\nPoint Data\n\nfocus on geometric pattern\nrandom vs. nonrandom\nclustered vs. uniform\n\nGeostatistics\n\n2-D modeling of spatial covariance (pairs of observations in function of distance)\nkriging, spatial prediction",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#data-types-and-autocorrelation-1",
    "href": "week-09/spatial_dependence.html#data-types-and-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Data Types and Autocorrelation",
    "text": "Data Types and Autocorrelation\nLattice Data\n\nareal units: states, counties, census tracts, watersheds\npoints: centroids of areal units\nfocus on the spatial nonrandomness of attribute values",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-association",
    "href": "week-09/spatial_dependence.html#spatial-association",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Association",
    "text": "Spatial Association\nNot a Rigorously Defined Term\n\nUsually the same as spatial autocorrelation\noften used in non-technical discussion\navoid unless meaning is clear",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-4",
    "href": "week-09/spatial_dependence.html#spatial-dependence-4",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nGood News (for geographers)\n\nSpace matters\nSuggestive of underlying process\n\nBad news\n\ninvalidates random sampling assumption\nnecessitates new methods = spatial statistics and spatial econometrics",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-implications",
    "href": "week-09/spatial_dependence.html#spatial-dependence-implications",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence: Implications",
    "text": "Spatial Dependence: Implications\nThe specific process we are simulating is as follows:\\[\\begin{aligned}\n\\label{eq:simdgp}y&=&X\\beta + \\epsilon \\\\ \\nonumber\\epsilon &=& \\lambda W \\epsilon + \\nu  \\end{aligned}\\] where \\(\\nu^{\\sim}N(0,\\sigma^{2}I)\\), \\(\\lambda\\) is a spatial autocorrelation parameter (scalar) and \\(W\\) is a spatial weights matrix. If \\(\\lambda=0\\) then the \\(i.i.d.\\) assumption holds, otherwise there is spatial dependence.\n\\(\\beta=40, \\ \\sigma^2=16, \\ x=[1,1,\\ldots]\\)\n\\(\\lambda=[0.0, 0.25, 0.50, 0.75], \\ n=25\\)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-dependence-implications-1",
    "href": "week-09/spatial_dependence.html#spatial-dependence-implications-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence: Implications",
    "text": "Spatial Dependence: Implications\nFor each D.G.P. we are going to generate 500 samples of size \\(n=25\\) for our map. You can think of this as generating 500 maps using the same D.G.P.. For each sample we will then do the following:\n\nEstimate \\(\\mu\\) with \\(\\bar{y}\\)\nTest the hypothesis that \\(\\mu=40\\)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#implications",
    "href": "week-09/spatial_dependence.html#implications",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Implications",
    "text": "Implications\n\n\nMonte Carlo Results\n\n\n\\(\\lambda\\)\n0.00\n0.25\n0.50\n0.75\n\n\n\n\n\\(\\hat{\\mu}\\)\n39.947\n39.931\n39.901\n39.814\n\n\n\\(\\sigma_{\\bar{x}}\\)\n0.816\n1.090\n1.641\n3.304\n\n\n\\(p\\)\n0.056\n0.148\n0.278\n0.492",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-randomness",
    "href": "week-09/spatial_dependence.html#spatial-randomness",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Randomness",
    "text": "Spatial Randomness\nNull Hypothesis\n\nobserved spatial pattern of values is equally likely as any other spatial pattern\nvalues at one location do no depend on values at other (neighboring) locations\nunder spatial randomness, the location of values may be altered without affecting the information content of the data",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-autocorrelation-on-a-grid",
    "href": "week-09/spatial_dependence.html#spatial-autocorrelation-on-a-grid",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation on a Grid",
    "text": "Spatial Autocorrelation on a Grid\n\nNegative, Random, Positive",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#positive-spatial-autocorrelation",
    "href": "week-09/spatial_dependence.html#positive-spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Positive Spatial Autocorrelation",
    "text": "Positive Spatial Autocorrelation\nClustering\n\nlike values tend to be in similar locations\n\nNeighbor similarity\n\nmore alike than they would be under spatial randomness\n\nCompatible with Diffusion\n\nbut not necessarily caused by diffusion",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#positive-spatial-autocorrelation-1",
    "href": "week-09/spatial_dependence.html#positive-spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Positive Spatial Autocorrelation",
    "text": "Positive Spatial Autocorrelation",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#negative-spatial-autocorrelation",
    "href": "week-09/spatial_dependence.html#negative-spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Negative Spatial Autocorrelation",
    "text": "Negative Spatial Autocorrelation\nCheckerboard pattern\n\nanti-clustering\n\nNeighbor dissimilarity\n\nmore dissimilar than they would be under spatial randomness\n\nCompatible with Competition\n\nbut not necessarily caused by competition",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#negative-spatial-autocorrelation-1",
    "href": "week-09/spatial_dependence.html#negative-spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Negative Spatial Autocorrelation",
    "text": "Negative Spatial Autocorrelation",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#autocorrelation-and-diffusion",
    "href": "week-09/spatial_dependence.html#autocorrelation-and-diffusion",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Autocorrelation and Diffusion",
    "text": "Autocorrelation and Diffusion\nOne does not necessarily imply the other\n\ndiffusion tends to yield positive spatial autocorrelation but the reverse is not necessary\npositive spatial correlation may be due to structural factors, without contagion or diffusion",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#true-vs.-apparent-contagion",
    "href": "week-09/spatial_dependence.html#true-vs.-apparent-contagion",
    "title": "Spatial Autocorrelation Concepts",
    "section": "True vs. Apparent Contagion",
    "text": "True vs. Apparent Contagion\nWhat is the Cause behind the clustering?\n\nTrue contagion\n\nresult of a contagious process, social interaction, dynamic process\n\nApparent contagion\n\nspatial heterogeneity\nstratification\n\nCannot be distinguished in a pure cross section\nEquifinality or Identification Problem",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#clustering",
    "href": "week-09/spatial_dependence.html#clustering",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Clustering",
    "text": "Clustering\nGlobal characeristic\n\nproperty of overall pattern = all observations\nare like values more grouped in space than random\ntest by means of a global spatial autocorrelation statistic\nno location of the clusters determined",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#clusters",
    "href": "week-09/spatial_dependence.html#clusters",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Clusters",
    "text": "Clusters\nLocal characeristic\n\nwhere are the like values more grouped in space than random?\nproperty of local pattern = location-specific\ntest by means of a local spatial autocorrelation statistic\nlocal clusters may be compatible with global spatial randomness",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#spatial-autocorrelation-statistic",
    "href": "week-09/spatial_dependence.html#spatial-autocorrelation-statistic",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation Statistic",
    "text": "Spatial Autocorrelation Statistic\nStructure\n\nFormal Test of Match between Value Similarity and Locational Similarity\nStatistic Summarizes Both Aspects\nSignificance\n\nhow likely is it (p-value) that the computed statistic would take this (extreme) value in a spatially random pattern",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#attribute-similarity",
    "href": "week-09/spatial_dependence.html#attribute-similarity",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Attribute Similarity",
    "text": "Attribute Similarity\n\nSummary of the similarity or dissimilarity of a variable at different locations\n\nvariable \\(y\\) at locations \\(i,j\\) with \\(i\\ne j\\)\n\nMeasures of similarity\n\ncross product: \\(y_i y_j\\)\n\nMeasures of dissimilarity\n\nsquared differences: \\((y_i - y_j)^2\\)\nabsolute differences: \\(|y_i - y_j|\\)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#locational-similarity",
    "href": "week-09/spatial_dependence.html#locational-similarity",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Locational Similarity",
    "text": "Locational Similarity\n\nFormalizing the notion of Neighbor\n\nwhen two spatial units a-priori are likely to interact\n\nSpatial Weights\n\nnot necessarility geographical\nmany approaches",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "week-09/spatial_dependence.html#summary",
    "href": "week-09/spatial_dependence.html#summary",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Summary",
    "text": "Summary\nSpatial Dependence\n\nCore of Lattice Analysis\nSpatial Autocorrelation More Complex than Temporal Autocorrelation\nCombine Value and Locational Similarities",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Spatial Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Attribution-ShareAlike 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nAdditional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "week-08/spatial_weights.html",
    "href": "week-08/spatial_weights.html",
    "title": "Spatial Weights",
    "section": "",
    "text": "Spatial weights are mathematical structures used to represent spatial relationships. Many spatial analytics, such as spatial autocorrelation statistics and regionalization algorithms rely on spatial weights. Generally speaking, a spatial weight \\(w_{i,j}\\) expresses the notion of a geographical relationship between locations \\(i\\) and \\(j\\). These relationships can be based on a number of criteria including contiguity, geospatial distance and general distances.\nlibpysal offers functionality for the construction, manipulation, analysis, and conversion of a wide array of spatial weights.\nWe begin with construction of weights from common spatial data formats.\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nimport libpysal \nfrom libpysal.weights import Queen, Rook, KNN, Kernel, DistanceBand\nimport numpy as np\nimport geopandas\nimport pandas\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nfrom splot.libpysal import plot_spatial_weights\n\nThere are functions to construct weights directly from a file path.\n\n\n\n\n\n\nA commonly-used type of weight is a queen contigutiy weight, which reflects adjacency relationships as a binary indicator variable denoting whether or not a polygon shares an edge or a vertex with another polygon. These weights are symmetric, in that when polygon \\(A\\) neighbors polygon \\(B\\), both \\(w_{AB} = 1\\) and \\(w_{BA} = 1\\).\nTo construct queen weights from a shapefile, we will use geopandas to read the file into a GeoDataFrame, and then use libpysal to construct the weights:\n\npath = \"~/data/scag_region.parquet\"\ndf = geopandas.read_parquet(path)\ndf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\ndf = df.to_crs(26911)  #UTM zone 11N\n\n\nqW = Queen.from_dataframe(df)\n\n\nqW\n\n&lt;libpysal.weights.contiguity.Queen at 0x7d1d8341d930&gt;\n\n\nAll weights objects have a few traits that you can use to work with the weights object, as well as to get information about the weights object.\nTo get the neighbors & weights around an observation, use the observation’s index on the weights object, like a dictionary:\n\nqW[155] #neighbors & weights of the 156th observation (0-index remember)\n\n{4528: 1.0, 547: 1.0, 2133: 1.0, 2744: 1.0}\n\n\nBy default, the weights and the pandas dataframe will use the same index. So, we can view the observation and its neighbors in the dataframe by putting the observation’s index and its neighbors’ indexes together in one list:\n\nself_and_neighbors = [155]\nself_and_neighbors.extend(qW.neighbors[155])\nprint(self_and_neighbors)\n\n[155, 4528, 547, 2133, 2744]\n\n\nand grabbing those elements from the dataframe:\n\ndf.loc[self_and_neighbors]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n155\n06037552302\n26.0\n80.0\n630.0\n0.0\n0.0\n913.0\nNaN\nNaN\nNaN\n...\n2010\n1268.0\n22.060410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401577.943 3752226.320, 401578.535 3...\n\n\n4528\n06037552400\n0.0\n20.0\n670.0\n0.0\n24.0\n821.0\nNaN\nNaN\nNaN\n...\n2010\n588.0\n5.576363\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401878.957 3751827.308, 402203.594 3...\n\n\n547\n06037552301\n59.0\n103.0\n1079.0\n5.0\n0.0\n1777.0\nNaN\nNaN\nNaN\n...\n2010\n1272.0\n8.472352\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400855.726 3753114.791, 400857.855 3...\n\n\n2133\n06037552200\n38.0\n141.0\n1484.0\n0.0\n52.0\n2235.0\nNaN\nNaN\nNaN\n...\n2010\n1902.0\n6.858581\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399998.860 3752819.258, 400004.819 3...\n\n\n2744\n06037504102\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n...\n2010\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401631.531 3751829.535, 401878.957 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\nA full, dense matrix describing all of the pairwise relationships is constructed using the .full method, or when libpysal.weights.full is called on a weights object:\n\nWmatrix, ids = qW.full()\n#Wmatrix, ids = libpysal.weights.full(qW)\n\n\nWmatrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nn_neighbors = Wmatrix.sum(axis=1) # how many neighbors each region has\n\n\nn_neighbors[155]\n\n4.0\n\n\n\nqW.cardinalities[155]\n\n4\n\n\nNote that this matrix is binary, in that its elements are either zero or one, since an observation is either a neighbor or it is not a neighbor.\nHowever, many common use cases of spatial weights require that the matrix is row-standardized. This is done simply in PySAL using the .transform attribute\n\nqW.transform = 'r'\n\n('WARNING: ', 4285, ' is an island (no neighbors)')\n\n\nNow, if we build a new full matrix, its rows should sum to one:\n\nWmatrix, ids = qW.full()\n\n\nWmatrix.sum(axis=1) #numpy axes are 0:column, 1:row, 2:facet, into higher dimensions\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\nSince weight matrices are typically very sparse, there is also a sparse weights matrix constructor:\n\nqW.sparse\n\n&lt;4580x4580 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 29374 stored elements in Compressed Sparse Row format&gt;\n\n\n\nqW.pct_nonzero #Percentage of nonzero neighbor counts\n\n0.14003356152628668\n\n\nLet’s look at the neighborhoods of the 101th observation\n\ndf.iloc[100]\n\ngeoid                                                        06037910606\nn_asian_under_15                                                     0.0\nn_black_under_15                                                   210.0\nn_hispanic_under_15                                                757.0\nn_native_under_15                                                    3.0\n                                             ...                        \np_hispanic_over_60                                                   NaN\np_native_over_60                                                     NaN\np_asian_over_60                                                      NaN\np_disabled                                                           NaN\ngeometry               POLYGON ((401275.2896923868 3825401.4434467247...\nName: 100, Length: 194, dtype: object\n\n\n\nqW.neighbors[100]\n\n[789, 790, 1991, 3676, 791]\n\n\n\nlen(qW.neighbors[100])\n\n5\n\n\n\ndf.iloc[qW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(qW, df)\n\n\n\n\n\n\n\n\nBy default, PySAL assigns each observation an index according to the order in which the observation was read in. This means that, by default, all of the observations in the weights object are indexed by table order.\n\npandas.Series(qW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\nqW.cardinalities.values()\n\ndict_values([9, 9, 4, 7, 7, 5, 5, 6, 5, 8, 9, 8, 4, 3, 5, 5, 6, 6, 4, 5, 5, 6, 7, 9, 6, 4, 7, 8, 7, 5, 7, 2, 6, 6, 8, 3, 7, 7, 5, 8, 6, 5, 5, 4, 6, 6, 7, 7, 4, 6, 7, 4, 5, 6, 13, 6, 7, 6, 8, 6, 6, 6, 2, 6, 6, 8, 6, 7, 7, 6, 3, 5, 6, 6, 3, 7, 6, 5, 5, 5, 8, 8, 6, 8, 9, 7, 7, 6, 7, 5, 5, 7, 7, 6, 5, 7, 8, 8, 4, 7, 5, 4, 4, 6, 9, 6, 6, 7, 4, 8, 6, 6, 5, 6, 6, 6, 6, 7, 6, 8, 6, 6, 6, 8, 6, 6, 5, 8, 4, 5, 7, 5, 5, 5, 5, 5, 2, 4, 4, 7, 6, 8, 6, 9, 4, 6, 7, 5, 5, 6, 6, 4, 8, 9, 7, 4, 8, 4, 6, 5, 5, 4, 5, 6, 7, 8, 4, 6, 5, 6, 6, 6, 4, 6, 7, 6, 5, 6, 7, 6, 7, 7, 7, 7, 3, 10, 6, 6, 7, 7, 5, 6, 7, 8, 6, 5, 9, 7, 9, 6, 6, 4, 6, 6, 5, 7, 7, 7, 6, 4, 7, 8, 7, 5, 6, 6, 4, 6, 6, 5, 9, 7, 5, 7, 4, 7, 7, 3, 6, 7, 5, 5, 6, 6, 5, 4, 6, 5, 5, 6, 5, 10, 4, 3, 6, 1, 8, 6, 4, 5, 5, 7, 6, 4, 7, 4, 5, 6, 6, 5, 10, 3, 5, 5, 9, 5, 7, 5, 5, 7, 5, 8, 4, 6, 5, 7, 7, 7, 5, 6, 7, 5, 3, 7, 5, 4, 6, 3, 5, 6, 5, 5, 5, 4, 4, 7, 7, 5, 5, 5, 7, 9, 6, 4, 4, 5, 7, 4, 4, 7, 4, 6, 6, 4, 8, 6, 7, 5, 8, 6, 7, 6, 8, 8, 4, 5, 7, 6, 3, 5, 5, 4, 6, 6, 7, 5, 5, 5, 3, 5, 7, 6, 8, 5, 5, 5, 7, 6, 6, 7, 3, 4, 8, 4, 7, 4, 6, 6, 4, 4, 4, 5, 3, 5, 6, 4, 6, 7, 8, 5, 6, 6, 6, 7, 5, 8, 5, 3, 5, 5, 6, 4, 7, 7, 7, 4, 6, 5, 6, 9, 4, 7, 5, 5, 5, 7, 4, 7, 8, 7, 7, 6, 5, 5, 7, 5, 9, 5, 5, 6, 4, 8, 6, 4, 5, 5, 5, 7, 6, 6, 3, 6, 7, 5, 4, 5, 5, 7, 7, 5, 4, 6, 6, 6, 5, 5, 5, 17, 7, 6, 6, 14, 5, 5, 6, 6, 4, 11, 7, 6, 4, 6, 5, 7, 6, 6, 4, 7, 4, 6, 4, 7, 7, 6, 8, 5, 6, 8, 6, 5, 8, 8, 6, 4, 5, 5, 3, 6, 4, 6, 6, 6, 5, 6, 4, 4, 6, 7, 7, 8, 4, 5, 17, 7, 5, 6, 5, 7, 8, 6, 4, 7, 5, 7, 6, 4, 4, 5, 5, 4, 4, 4, 5, 3, 5, 4, 5, 3, 4, 5, 7, 7, 4, 7, 5, 4, 5, 12, 9, 9, 5, 4, 6, 7, 6, 6, 3, 8, 4, 8, 4, 6, 8, 4, 6, 5, 9, 6, 7, 4, 4, 8, 7, 4, 8, 5, 5, 7, 6, 5, 7, 4, 8, 6, 5, 8, 6, 6, 6, 6, 4, 5, 6, 8, 5, 8, 5, 8, 5, 8, 8, 6, 8, 7, 6, 7, 6, 5, 5, 7, 9, 3, 6, 8, 8, 7, 8, 5, 4, 6, 5, 6, 9, 7, 6, 7, 8, 3, 6, 6, 4, 7, 6, 5, 3, 5, 5, 8, 6, 8, 3, 8, 6, 8, 6, 6, 5, 8, 4, 8, 8, 7, 4, 5, 6, 6, 7, 7, 7, 5, 7, 4, 8, 6, 8, 8, 6, 4, 5, 7, 6, 6, 7, 5, 7, 8, 4, 6, 7, 6, 6, 8, 7, 7, 4, 4, 8, 7, 7, 8, 11, 6, 7, 6, 10, 5, 6, 6, 6, 4, 5, 5, 8, 7, 4, 7, 6, 8, 6, 7, 6, 7, 7, 6, 6, 5, 6, 6, 6, 5, 7, 9, 4, 5, 6, 6, 5, 6, 7, 5, 10, 5, 7, 4, 5, 4, 5, 5, 5, 6, 10, 6, 6, 6, 5, 6, 7, 4, 4, 6, 6, 8, 6, 7, 8, 8, 6, 3, 5, 6, 5, 7, 5, 5, 7, 8, 6, 7, 5, 5, 6, 6, 6, 5, 6, 6, 8, 5, 8, 4, 6, 8, 4, 5, 3, 5, 5, 8, 5, 8, 7, 8, 7, 6, 10, 5, 7, 4, 4, 3, 9, 4, 6, 5, 4, 7, 7, 6, 9, 4, 6, 5, 5, 7, 5, 5, 9, 5, 6, 6, 5, 6, 5, 7, 5, 8, 8, 6, 7, 5, 7, 5, 5, 7, 7, 4, 7, 8, 6, 8, 7, 5, 7, 10, 5, 4, 7, 5, 6, 7, 7, 5, 5, 4, 6, 5, 6, 6, 7, 5, 7, 7, 6, 6, 5, 6, 4, 4, 6, 5, 6, 5, 6, 6, 5, 4, 5, 9, 9, 4, 5, 8, 6, 8, 5, 7, 7, 6, 6, 4, 9, 8, 6, 10, 5, 7, 6, 3, 6, 6, 8, 7, 5, 7, 5, 6, 8, 5, 6, 5, 5, 7, 4, 5, 4, 4, 5, 4, 5, 7, 5, 7, 6, 5, 7, 7, 6, 7, 6, 7, 7, 6, 7, 4, 4, 6, 7, 4, 6, 7, 8, 6, 8, 8, 2, 7, 4, 9, 5, 7, 7, 4, 6, 6, 4, 5, 5, 5, 9, 6, 8, 6, 9, 9, 7, 7, 5, 7, 5, 5, 5, 5, 4, 7, 6, 5, 6, 9, 6, 4, 6, 5, 6, 7, 6, 6, 4, 5, 3, 7, 4, 5, 6, 10, 4, 7, 6, 4, 6, 6, 8, 5, 6, 6, 7, 6, 6, 4, 7, 6, 7, 8, 6, 6, 7, 9, 6, 6, 9, 6, 8, 3, 5, 7, 8, 9, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 7, 3, 6, 6, 5, 9, 5, 8, 7, 5, 6, 5, 7, 7, 7, 6, 6, 8, 6, 5, 6, 7, 3, 6, 6, 8, 7, 6, 8, 5, 7, 6, 5, 6, 7, 4, 4, 7, 5, 20, 6, 5, 6, 6, 6, 7, 5, 6, 7, 5, 4, 5, 4, 9, 4, 7, 6, 8, 7, 5, 5, 5, 5, 7, 5, 6, 6, 5, 3, 5, 4, 6, 8, 6, 2, 7, 5, 5, 5, 6, 8, 7, 7, 7, 8, 7, 6, 5, 5, 6, 5, 6, 5, 7, 6, 7, 4, 6, 4, 5, 7, 5, 9, 6, 7, 7, 5, 10, 8, 5, 5, 5, 7, 6, 6, 7, 6, 7, 6, 5, 4, 5, 5, 6, 7, 10, 4, 4, 6, 6, 8, 7, 5, 6, 6, 9, 5, 5, 6, 6, 8, 5, 7, 6, 4, 9, 7, 5, 5, 6, 5, 16, 5, 4, 6, 4, 8, 8, 6, 6, 7, 7, 5, 10, 7, 6, 7, 6, 7, 5, 7, 5, 7, 6, 7, 6, 4, 8, 6, 7, 6, 6, 5, 4, 4, 5, 9, 4, 8, 8, 7, 6, 6, 11, 6, 4, 6, 6, 4, 7, 5, 6, 7, 5, 5, 3, 7, 5, 5, 3, 6, 6, 9, 5, 8, 8, 3, 6, 5, 5, 8, 5, 5, 7, 5, 9, 8, 9, 6, 6, 6, 7, 8, 8, 7, 6, 7, 9, 4, 8, 9, 9, 7, 7, 8, 6, 5, 6, 5, 5, 6, 5, 5, 8, 6, 7, 5, 7, 4, 7, 5, 10, 7, 8, 5, 6, 9, 9, 9, 10, 4, 5, 6, 9, 4, 5, 6, 6, 5, 6, 8, 9, 9, 9, 10, 7, 9, 5, 3, 6, 7, 7, 9, 7, 8, 9, 8, 7, 8, 6, 7, 6, 9, 9, 5, 8, 3, 3, 7, 5, 7, 9, 9, 8, 7, 7, 10, 6, 8, 7, 6, 8, 6, 8, 5, 5, 5, 4, 5, 7, 8, 9, 7, 3, 6, 4, 7, 7, 7, 6, 10, 6, 7, 8, 7, 10, 9, 7, 6, 8, 5, 7, 7, 6, 5, 5, 5, 7, 5, 6, 4, 5, 6, 4, 9, 7, 4, 6, 4, 9, 9, 6, 8, 5, 9, 6, 9, 6, 7, 7, 6, 5, 10, 10, 7, 7, 7, 7, 8, 6, 3, 7, 4, 5, 6, 4, 10, 9, 5, 8, 6, 9, 5, 7, 5, 7, 5, 8, 8, 9, 9, 13, 7, 7, 8, 6, 1, 7, 6, 5, 6, 7, 9, 5, 7, 6, 4, 4, 3, 7, 8, 6, 7, 8, 6, 4, 6, 9, 8, 6, 6, 6, 8, 7, 7, 5, 5, 2, 3, 7, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 6, 9, 4, 5, 5, 3, 5, 10, 5, 15, 4, 6, 9, 8, 8, 7, 5, 8, 7, 8, 9, 8, 7, 5, 6, 8, 4, 9, 6, 9, 6, 6, 6, 5, 7, 10, 11, 11, 6, 5, 6, 7, 4, 5, 11, 16, 5, 7, 9, 7, 6, 7, 7, 6, 5, 9, 6, 10, 7, 5, 7, 6, 5, 3, 8, 3, 11, 5, 5, 9, 9, 7, 6, 6, 8, 10, 6, 10, 4, 8, 7, 4, 7, 4, 5, 9, 4, 5, 9, 9, 6, 6, 8, 9, 9, 4, 7, 10, 16, 7, 11, 7, 11, 5, 8, 9, 5, 11, 7, 5, 7, 5, 5, 7, 8, 9, 5, 11, 11, 7, 8, 6, 5, 6, 5, 7, 9, 4, 7, 8, 7, 8, 4, 6, 7, 9, 9, 8, 6, 4, 7, 4, 6, 7, 6, 8, 6, 7, 4, 8, 5, 7, 7, 10, 7, 7, 6, 4, 6, 6, 13, 3, 8, 9, 6, 4, 6, 5, 6, 12, 5, 2, 6, 5, 7, 6, 7, 8, 5, 7, 4, 6, 7, 10, 3, 17, 10, 8, 6, 5, 7, 14, 1, 12, 8, 4, 4, 8, 8, 4, 8, 6, 8, 6, 5, 3, 8, 6, 7, 8, 6, 9, 8, 5, 11, 5, 3, 7, 5, 4, 5, 3, 7, 7, 6, 5, 8, 2, 5, 6, 11, 8, 6, 6, 7, 6, 7, 6, 6, 4, 4, 6, 9, 6, 9, 6, 5, 4, 3, 5, 8, 7, 10, 4, 6, 7, 4, 6, 6, 4, 6, 4, 8, 6, 10, 8, 5, 8, 6, 4, 6, 6, 5, 11, 4, 7, 8, 7, 6, 5, 5, 10, 16, 8, 5, 7, 9, 3, 7, 5, 3, 4, 8, 6, 6, 7, 6, 9, 7, 7, 6, 6, 11, 7, 7, 8, 6, 6, 7, 4, 7, 6, 3, 7, 7, 6, 8, 15, 7, 6, 4, 6, 9, 6, 6, 6, 10, 7, 8, 4, 9, 11, 6, 6, 6, 4, 11, 6, 5, 5, 5, 5, 7, 5, 5, 5, 4, 4, 8, 4, 6, 5, 7, 8, 6, 7, 5, 6, 7, 7, 6, 4, 10, 5, 6, 7, 5, 4, 7, 5, 6, 5, 7, 5, 5, 10, 6, 5, 5, 3, 7, 5, 6, 6, 6, 7, 5, 4, 7, 6, 11, 6, 8, 6, 9, 5, 14, 4, 5, 13, 8, 6, 7, 4, 7, 5, 9, 5, 8, 4, 7, 7, 8, 7, 4, 7, 5, 10, 8, 9, 5, 7, 6, 4, 7, 8, 8, 12, 7, 6, 8, 10, 5, 7, 6, 6, 6, 5, 5, 5, 6, 9, 5, 7, 4, 7, 8, 5, 7, 6, 5, 7, 7, 4, 4, 6, 7, 7, 6, 8, 6, 5, 4, 6, 5, 5, 4, 6, 6, 6, 7, 5, 4, 6, 3, 6, 7, 1, 7, 11, 4, 4, 12, 7, 6, 8, 5, 7, 8, 3, 6, 5, 5, 5, 9, 6, 9, 6, 5, 3, 7, 6, 8, 8, 9, 7, 5, 7, 6, 6, 5, 5, 10, 4, 8, 5, 8, 4, 5, 6, 5, 14, 8, 1, 5, 6, 7, 8, 10, 4, 7, 5, 9, 5, 5, 7, 4, 4, 7, 5, 8, 6, 9, 6, 8, 7, 4, 6, 4, 7, 5, 4, 6, 3, 7, 6, 6, 7, 11, 8, 6, 7, 8, 5, 7, 7, 9, 6, 5, 6, 6, 8, 6, 9, 7, 7, 7, 8, 5, 5, 5, 9, 6, 5, 4, 7, 7, 5, 7, 5, 5, 6, 8, 6, 5, 6, 4, 6, 6, 7, 7, 5, 6, 9, 6, 5, 6, 5, 8, 9, 7, 11, 7, 11, 15, 6, 7, 8, 3, 10, 8, 10, 8, 8, 4, 6, 5, 7, 6, 5, 6, 6, 7, 5, 9, 9, 9, 7, 11, 6, 6, 6, 5, 6, 6, 8, 6, 7, 5, 7, 8, 10, 10, 9, 7, 3, 9, 10, 7, 7, 6, 10, 6, 6, 7, 4, 7, 4, 5, 6, 6, 5, 5, 7, 8, 7, 4, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 8, 7, 7, 5, 8, 10, 8, 7, 7, 6, 6, 8, 7, 6, 5, 6, 6, 9, 6, 2, 7, 7, 7, 7, 6, 7, 7, 4, 9, 12, 5, 5, 5, 5, 4, 9, 5, 7, 6, 7, 6, 4, 8, 7, 7, 7, 3, 5, 6, 5, 5, 7, 8, 6, 9, 4, 5, 6, 7, 5, 4, 6, 9, 5, 6, 10, 10, 6, 8, 5, 6, 4, 7, 8, 8, 5, 4, 9, 6, 4, 6, 5, 7, 5, 7, 6, 4, 10, 5, 6, 5, 5, 4, 6, 4, 6, 16, 8, 3, 7, 6, 5, 5, 5, 7, 7, 6, 9, 7, 6, 6, 6, 8, 3, 8, 4, 5, 5, 6, 6, 8, 9, 4, 7, 8, 7, 5, 6, 6, 6, 12, 4, 5, 6, 7, 7, 4, 8, 5, 7, 6, 7, 7, 9, 8, 7, 6, 7, 7, 6, 7, 5, 6, 3, 6, 5, 3, 6, 10, 7, 6, 6, 6, 6, 13, 10, 6, 8, 4, 6, 5, 7, 8, 6, 7, 5, 8, 7, 7, 8, 13, 5, 10, 7, 6, 7, 7, 6, 7, 7, 7, 8, 4, 9, 7, 4, 5, 4, 6, 8, 8, 6, 10, 3, 5, 10, 8, 6, 6, 11, 7, 6, 6, 5, 5, 8, 4, 6, 4, 13, 4, 11, 7, 5, 7, 6, 6, 7, 5, 5, 6, 5, 10, 5, 5, 8, 10, 10, 6, 6, 7, 6, 8, 5, 5, 2, 5, 5, 11, 6, 6, 8, 13, 2, 3, 5, 4, 6, 5, 4, 5, 5, 5, 11, 5, 8, 7, 8, 7, 5, 6, 5, 6, 10, 3, 9, 5, 4, 6, 6, 9, 8, 6, 9, 6, 7, 5, 6, 3, 6, 9, 8, 7, 7, 4, 5, 8, 5, 8, 8, 7, 6, 8, 14, 6, 4, 7, 3, 9, 5, 6, 5, 5, 6, 7, 3, 9, 9, 5, 6, 6, 4, 4, 9, 7, 5, 4, 5, 15, 8, 7, 9, 6, 6, 5, 7, 6, 8, 4, 4, 5, 5, 3, 5, 3, 4, 4, 4, 7, 12, 8, 9, 9, 6, 3, 6, 4, 7, 7, 9, 4, 6, 9, 5, 7, 5, 10, 5, 10, 6, 9, 4, 6, 8, 5, 8, 12, 10, 5, 7, 6, 7, 10, 7, 9, 6, 7, 5, 6, 6, 8, 6, 6, 8, 4, 6, 6, 9, 6, 6, 7, 4, 4, 3, 8, 10, 6, 6, 25, 8, 8, 5, 5, 4, 7, 7, 5, 7, 6, 7, 7, 6, 6, 5, 8, 6, 6, 7, 6, 8, 5, 4, 5, 8, 6, 12, 6, 7, 8, 4, 4, 7, 7, 9, 9, 14, 3, 10, 6, 6, 5, 7, 14, 5, 8, 4, 8, 8, 6, 6, 4, 6, 10, 14, 8, 5, 7, 6, 9, 5, 6, 7, 7, 5, 7, 5, 6, 9, 6, 6, 8, 7, 3, 6, 5, 9, 4, 4, 6, 13, 4, 6, 4, 5, 5, 7, 6, 7, 14, 3, 5, 11, 6, 7, 7, 7, 5, 5, 6, 14, 7, 7, 7, 5, 3, 4, 8, 4, 6, 8, 2, 6, 10, 5, 12, 8, 9, 6, 5, 13, 6, 8, 5, 2, 5, 1, 5, 6, 5, 5, 4, 9, 6, 7, 3, 8, 5, 6, 7, 6, 7, 8, 7, 3, 8, 6, 7, 5, 7, 7, 6, 5, 7, 11, 9, 6, 6, 3, 4, 9, 8, 8, 8, 6, 5, 6, 5, 7, 15, 8, 10, 9, 10, 6, 5, 7, 6, 10, 6, 5, 12, 5, 5, 8, 8, 9, 4, 7, 4, 4, 8, 15, 6, 4, 12, 6, 6, 4, 6, 6, 8, 4, 7, 8, 6, 6, 8, 4, 5, 9, 7, 6, 6, 7, 7, 6, 6, 7, 9, 6, 7, 6, 8, 5, 5, 5, 10, 8, 6, 6, 7, 5, 6, 6, 6, 8, 8, 7, 6, 8, 5, 6, 7, 7, 7, 4, 7, 6, 6, 4, 11, 4, 7, 6, 5, 9, 10, 8, 6, 7, 6, 7, 6, 6, 7, 4, 7, 7, 5, 7, 7, 6, 9, 6, 6, 6, 7, 8, 4, 5, 3, 7, 5, 6, 8, 6, 6, 6, 16, 6, 5, 15, 10, 6, 7, 9, 7, 7, 8, 5, 9, 6, 5, 5, 4, 9, 11, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 8, 5, 4, 3, 8, 5, 4, 4, 5, 5, 6, 5, 3, 10, 8, 5, 9, 9, 6, 5, 3, 5, 6, 7, 7, 8, 5, 8, 6, 6, 4, 5, 4, 5, 9, 9, 4, 7, 5, 6, 9, 7, 4, 8, 8, 7, 6, 10, 7, 8, 11, 5, 7, 5, 6, 7, 9, 8, 7, 7, 8, 10, 3, 4, 6, 7, 7, 5, 7, 6, 5, 6, 9, 10, 3, 7, 5, 7, 8, 9, 5, 6, 2, 9, 7, 7, 4, 6, 6, 9, 7, 9, 6, 6, 7, 6, 7, 6, 5, 7, 8, 8, 5, 6, 6, 8, 6, 6, 6, 8, 7, 8, 6, 6, 8, 5, 7, 8, 4, 6, 5, 7, 8, 7, 7, 9, 5, 1, 8, 7, 5, 5, 7, 8, 5, 8, 6, 6, 4, 6, 6, 6, 7, 5, 5, 4, 7, 7, 7, 7, 6, 7, 5, 6, 6, 7, 6, 5, 6, 7, 9, 6, 7, 7, 4, 7, 7, 5, 6, 8, 5, 7, 7, 7, 7, 6, 7, 6, 14, 6, 9, 9, 10, 5, 6, 7, 7, 9, 4, 8, 10, 4, 8, 6, 5, 5, 6, 6, 6, 7, 7, 6, 3, 5, 10, 8, 9, 6, 8, 1, 4, 6, 7, 5, 7, 8, 7, 7, 7, 6, 5, 7, 5, 4, 10, 7, 9, 5, 8, 10, 7, 5, 9, 6, 6, 7, 8, 7, 7, 6, 7, 5, 6, 5, 8, 9, 6, 6, 6, 10, 5, 6, 7, 6, 8, 6, 7, 7, 6, 9, 5, 8, 4, 5, 7, 6, 5, 4, 4, 7, 8, 5, 5, 7, 5, 8, 7, 9, 6, 10, 6, 8, 8, 6, 7, 6, 6, 7, 9, 6, 5, 5, 8, 6, 7, 10, 6, 4, 14, 3, 6, 6, 8, 8, 7, 9, 6, 6, 10, 6, 8, 7, 6, 8, 6, 4, 5, 5, 5, 8, 4, 4, 6, 4, 6, 11, 4, 5, 6, 4, 6, 7, 5, 5, 3, 5, 6, 6, 9, 8, 9, 6, 6, 6, 6, 6, 8, 7, 7, 6, 4, 6, 5, 8, 6, 6, 5, 7, 8, 5, 7, 7, 6, 7, 3, 7, 4, 6, 5, 6, 5, 6, 6, 7, 10, 6, 6, 9, 7, 14, 6, 5, 7, 9, 8, 8, 9, 5, 4, 9, 4, 6, 5, 6, 8, 7, 6, 5, 5, 6, 5, 6, 7, 6, 6, 7, 7, 7, 6, 7, 6, 7, 6, 11, 6, 7, 7, 5, 4, 7, 6, 9, 7, 3, 6, 8, 5, 6, 3, 6, 7, 6, 3, 7, 11, 5, 7, 6, 6, 5, 6, 5, 7, 6, 8, 8, 9, 6, 6, 6, 7, 7, 10, 7, 9, 7, 6, 9, 8, 7, 6, 6, 5, 7, 8, 5, 7, 6, 6, 6, 9, 7, 4, 8, 7, 8, 7, 6, 6, 10, 5, 8, 4, 8, 5, 6, 7, 7, 7, 8, 7, 6, 9, 6, 7, 5, 4, 5, 7, 7, 5, 7, 7, 6, 6, 5, 7, 6, 5, 7, 5, 6, 8, 8, 5, 5, 5, 10, 6, 5, 6, 4, 9, 5, 6, 9, 8, 8, 7, 6, 8, 10, 6, 11, 6, 5, 10, 6, 8, 8, 7, 6, 10, 5, 7, 9, 6, 4, 8, 9, 7, 6, 6, 6, 5, 10, 6, 9, 5, 7, 7, 8, 6, 7, 5, 7, 6, 4, 7, 6, 8, 8, 5, 7, 6, 4, 7, 4, 5, 8, 5, 5, 9, 8, 5, 8, 6, 4, 7, 5, 3, 6, 6, 4, 7, 3, 5, 4, 4, 7, 8, 8, 7, 10, 8, 6, 6, 7, 5, 6, 5, 7, 8, 4, 7, 9, 5, 7, 5, 5, 7, 7, 7, 5, 5, 5, 6, 9, 6, 6, 8, 7, 5, 5, 7, 6, 8, 6, 8, 10, 9, 7, 8, 5, 6, 6, 7, 6, 8, 7, 6, 8, 5, 7, 5, 6, 7, 6, 5, 8, 5, 7, 3, 7, 6, 7, 8, 8, 4, 6, 2, 8, 6, 6, 5, 7, 5, 8, 8, 9, 7, 7, 7, 8, 7, 8, 6, 5, 8, 11, 10, 7, 7, 4, 6, 8, 5, 4, 8, 3, 5, 6, 8, 9, 7, 4, 5, 8, 8, 5, 5, 6, 6, 6, 7, 9, 6, 6, 11, 4, 7, 5, 9, 9, 6, 8, 6, 6, 5, 5, 8, 7, 7, 5, 7, 6, 12, 7, 6, 7, 5, 1, 10, 5, 3, 7, 5, 5, 6, 6, 7, 8, 8, 7, 5, 3, 5, 7, 7, 8, 9, 4, 5, 8, 8, 6, 5, 7, 7, 6, 7, 5, 8, 11, 5, 5, 4, 5, 5, 1, 9, 6, 9, 9, 5, 6, 7, 7, 9, 6, 7, 7, 7, 5, 4, 5, 6, 6, 5, 6, 4, 6, 6, 5, 5, 3, 5, 7, 4, 6, 4, 8, 6, 6, 6, 6, 6, 7, 4, 3, 4, 12, 6, 6, 6, 6, 8, 6, 6, 7, 12, 8, 5, 11, 4, 6, 6, 5, 6, 7, 6, 5, 7, 7, 10, 6, 5, 7, 6, 5, 6, 6, 6, 6, 5, 10, 19, 7, 7, 8, 5, 6, 9, 6, 6, 12, 5, 4, 5, 3, 12, 4, 6, 4, 7, 4, 9, 4, 5, 3, 4, 7, 9, 6, 5, 7, 8, 5, 6, 5, 4, 8, 7, 5, 7, 5, 4, 7, 6, 4, 7, 5, 5, 7, 6, 7, 8, 11, 5, 5, 8, 3, 5, 4, 6, 6, 3, 7, 7, 5, 6, 9, 12, 7, 5, 5, 6, 9, 5, 7, 10, 6, 9, 5, 6, 6, 6, 7, 8, 6, 5, 7, 5, 7, 5, 5, 5, 8, 6, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 6, 10, 8, 7, 7, 5, 6, 6, 5, 12, 7, 8, 6, 6, 5, 8, 5, 5, 5, 5, 6, 5, 9, 10, 6, 6, 5, 4, 5, 4, 6, 6, 5, 7, 5, 7, 7, 4, 7, 5, 9, 6, 6, 4, 6, 6, 6, 5, 6, 6, 6, 6, 4, 8, 3, 5, 9, 7, 9, 6, 4, 12, 6, 7, 6, 7, 6, 8, 16, 7, 7, 5, 10, 7, 8, 6, 6, 7, 7, 11, 6, 11, 5, 6, 9, 5, 8, 6, 7, 5, 6, 7, 6, 8, 9, 7, 2, 10, 5, 7, 6, 7, 6, 6, 6, 6, 10, 4, 4, 6, 8, 6, 6, 9, 7, 6, 2, 6, 7, 5, 7, 5, 15, 8, 6, 8, 4, 6, 7, 7, 7, 8, 8, 7, 5, 5, 6, 5, 7, 7, 7, 5, 6, 9, 8, 9, 5, 6, 4, 6, 5, 5, 7, 5, 8, 5, 4, 5, 5, 7, 7, 7, 5, 6, 8, 9, 4, 6, 6, 11, 6, 8, 9, 6, 5, 6, 6, 8, 6, 5, 7, 6, 5, 7, 4, 9, 5, 4, 6, 6, 6, 3, 7, 6, 6, 5, 13, 7, 6, 8, 5, 7, 5, 5, 8, 7, 7, 7, 7, 6, 9, 10, 8, 9, 6, 6, 7, 10, 6, 7, 7, 6, 6, 6, 6, 5, 7, 7, 5, 6, 4, 6, 7, 9, 8, 4, 9, 5, 5, 6, 4, 7, 8, 9, 7, 2, 4, 7, 7, 7, 7, 7, 11, 8, 9, 6, 4, 6, 6, 5, 8, 7, 7, 6, 7, 8, 8, 8, 6, 6, 8, 9, 8, 9, 7, 7, 5, 7, 5, 9, 6, 4, 6, 7, 4, 9, 7, 4, 4, 5, 9, 10, 8, 10, 9, 5, 6, 6, 5, 7, 7, 6, 5, 6, 9, 8, 7, 4, 6, 6, 7, 4, 5, 7, 7, 5, 9, 6, 7, 6, 9, 9, 8, 7, 8, 5, 3, 4, 11, 14, 5, 5, 4, 3, 6, 6, 5, 6, 7, 6, 8, 8, 9, 7, 6, 2, 6, 6, 5, 5, 5, 3, 8, 8, 11, 7, 6, 7, 6, 6, 7, 8, 6, 7, 7, 6, 6, 8, 8, 5, 5, 16, 8, 6, 7, 9, 5, 8, 6, 5, 4, 7, 5, 6, 8, 7, 5, 6, 7, 6, 6, 7, 10, 7, 10, 9, 8, 7, 5, 8, 5, 8, 7, 7, 9, 9, 5, 7, 6, 6, 5, 7, 5, 6, 8, 7, 6, 6, 10, 6, 5, 5, 8, 5, 7, 5, 5, 7, 6, 6, 5, 6, 0, 6, 11, 5, 5, 9, 6, 5, 9, 5, 6, 6, 6, 6, 6, 7, 7, 11, 10, 4, 7, 5, 8, 8, 8, 5, 6, 6, 7, 8, 7, 8, 5, 5, 5, 6, 7, 7, 5, 8, 5, 5, 5, 10, 8, 5, 10, 5, 4, 5, 5, 6, 5, 9, 5, 4, 5, 7, 4, 4, 6, 15, 7, 6, 8, 7, 10, 7, 7, 5, 6, 7, 6, 6, 6, 8, 5, 6, 4, 2, 6, 5, 8, 8, 5, 6, 5, 7, 10, 7, 8, 7, 6, 8, 7, 11, 7, 6, 6, 8, 7, 10, 16, 6, 7, 6, 4, 5, 4, 4, 7, 8, 8, 5, 6, 4, 6, 3, 5, 9, 8, 4, 6, 4, 6, 3, 6, 9, 7, 8, 3, 14, 6, 8, 4, 7, 6, 5, 6, 4, 8, 5, 7, 7, 7, 7, 5, 5, 7, 5, 6, 6, 7, 7, 7, 7, 4, 11, 6, 7, 4, 3, 13, 5, 8, 9, 8, 6, 8, 7, 7, 6, 7, 8, 6, 6, 9, 5, 8, 6, 7, 11, 6, 5, 8, 7, 9, 5, 7, 5, 7, 8, 4, 4, 8, 5, 7, 5, 5, 6, 7, 9, 7, 6, 11, 8, 3, 5, 7, 11, 4, 6, 7, 7, 7, 6, 5, 5, 7, 5, 6, 6, 7, 6, 5, 7, 8, 6, 5, 7, 14, 10, 6, 7, 7, 5, 6, 11, 5, 6, 5, 10, 4, 3, 6, 5, 8, 6, 6, 6, 7, 7, 8, 5, 7, 6, 5, 8, 7, 9, 6, 5, 6, 8, 5, 7, 7, 7, 6, 7, 6, 8, 5, 7, 9, 8, 5, 7, 5, 8, 8, 9, 12, 7, 4, 7, 10, 8, 7, 7, 6, 6, 7, 6, 7])\n\n\n\n\n\nRook weights are another type of contiguity weight, but consider observations as neighboring only when they share an edge. The rook neighbors of an observation may be different than its queen neighbors, depending on how the observation and its nearby polygons are configured.\nWe can construct this in the same way as the queen weights:\n\nrW = Rook.from_dataframe(df)\n\n\nrW.neighbors[100]\n\n[789, 790, 1991, 791, 3676]\n\n\n\nlen(rW.neighbors[100])\n\n5\n\n\n\ndf.iloc[rW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(rW, df)\n\n\n\n\n\n\n\n\n\npandas.Series(rW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\n\n\nIn theory, a “Bishop” weighting scheme is one that arises when only polygons that share vertexes are considered to be neighboring. But, since Queen contiguigy requires either an edge or a vertex and Rook contiguity requires only shared edges, the following relationship is true:\n\\[ \\mathcal{Q} = \\mathcal{R} \\cup \\mathcal{B} \\]\nwhere \\(\\mathcal{Q}\\) is the set of neighbor pairs via queen contiguity, \\(\\mathcal{R}\\) is the set of neighbor pairs via Rook contiguity, and \\(\\mathcal{B}\\) via Bishop contiguity. Thus:\n\\[ \\mathcal{Q} \\setminus \\mathcal{R} = \\mathcal{B}\\]\nBishop weights entail all Queen neighbor pairs that are not also Rook neighbors.\nPySAL does not have a dedicated bishop weights constructor, but you can construct very easily using the w_difference function. This function is one of a family of tools to work with weights, all defined in libpysal.weights, that conduct these types of set operations between weight objects.\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW.histogram\n\n[(0, 1624), (1, 1728), (2, 881), (3, 292), (4, 55)]\n\n\nThus, many tracts have no bishop neighbors. But, a few do. A simple way to see these observations in the dataframe is to find all elements of the dataframe that are not “islands,” the term for an observation with no neighbors:\n\nplot_spatial_weights(bW, df)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many other kinds of weighting functions in PySAL. Another separate type use a continuous measure of distance to define neighborhoods.\n\ndf.crs\n\n&lt;Projected CRS: EPSG:26911&gt;\nName: NAD83 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: North America - between 120°W and 114°W - onshore and offshore. Canada - Alberta; British Columbia; Northwest Territories; Nunavut. United States (USA) - California; Idaho; Nevada, Oregon; Washington.\n- bounds: (-120.0, 30.88, -114.0, 83.5)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nOur coordinate system (UTM 11N) measures distance in meters, so that’s how we’ll define our neighbors\n\ndist_band = DistanceBand.from_dataframe(df, threshold=2000)\n\n\nplot_spatial_weights(dist_band,df)\n\n\n\n\n\n\n\n\n\n\n\nradius_mile = libpysal.cg.sphere.RADIUS_EARTH_MILES\nradius_mile\n\n3958.755865744055\n\n\n\ndf_latlong = df.to_crs(4326)\n\n\nknn8_bad = KNN.from_dataframe(df_latlong, k=8) # ignore curvature of the earth\n\n\nknn8_bad.histogram\n\n[(8, 4580)]\n\n\n\nknn8 = KNN.from_dataframe(df_latlong, k=8, radius=radius_mile)\n\n\nknn8.histogram\n\n[(8, 4580)]\n\n\n\nknn8_bad.neighbors[1487]\n\n[501, 2296, 2960, 974, 167, 4496, 2295, 4422]\n\n\n\nknn8.neighbors[1487]\n\n[501, 2960, 2296, 974, 167, 4496, 2881, 2297]\n\n\n\nset(knn8_bad.neighbors[1487]) == set(knn8.neighbors[1487])\n\nFalse\n\n\n\nExercise:  Enumerate the tracts for which ignoring curvature results in an incorrect neighbor set for knn.\n\n\n# %load solutions/02_knn.py\n\n\n\n\nKernel Weights are continuous distance-based weights that use kernel densities to define the neighbor relationship. Typically, they estimate a bandwidth, which is a parameter governing how far out observations should be considered neighboring. Then, using this bandwidth, they evaluate a continuous kernel function to provide a weight between 0 and 1.\nMany different choices of kernel functions are supported, and bandwidths can either be fixed (constant over all units) or adaptive in function of unit density.\nFor example, if we want to use adaptive bandwidths for the map and weight according to a gaussian kernel:\n\n\nbandwidth = the distance to the kth nearest neighbor for each observation\nbandwith is changing across observations\n\nkernelWa = Kernel.from_dataframe(df, k=10, fixed=False, function='gaussian')\n\n\nplot_spatial_weights(kernelWa, df)\n\n\n\n\n\n\n\n\n\nkernelWa.bandwidth\n\narray([[1687.99751736],\n       [1997.79636883],\n       [1803.3632643 ],\n       ...,\n       [2468.39103021],\n       [3480.79114847],\n       [1749.84752448]])\n\n\n\ndf.assign(bw=kernelWa.bandwidth.flatten()).plot('bw', cmap='Reds')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw,s,e,n = df.total_bounds\n\n\nmx = (w+e)/2\nmy = (n+s)/2\n\n\nimport shapely\n\n\ncentroids = df.geometry.centroid\n\n\nlon = centroids.apply(lambda p: p.x).values\nlat = centroids.apply(lambda p: p.y).values\n\n\nnorth = lat &gt; my\nsouth = lat &lt;= my\neast = lon &gt; mx\nwest = lon &lt;= mx\n\n\nnw = west * north * 2\nne = east * north * 1\nsw = west * south * 3\nse = east * south *4\nquad = nw + ne + sw + se\n\n\nquad\n\narray([3, 2, 2, ..., 2, 4, 2])\n\n\n\ndf['quad'] = quad\ndf.plot(column=\"quad\", categorical=True)\n\n\n\n\n\n\n\n\n\nblockW = libpysal.weights.block_weights(df[\"quad\"])\n\n\nblockW.n\n\n4580\n\n\n\nblockW.pct_nonzero\n\n65.53761369920483\n\n\n\npandas.Series(blockW.cardinalities).plot.hist()\n\n\n\n\n\n\n\n\n\ndf.groupby(by='quad').count()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\nquad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n15\n15\n15\n15\n15\n15\n15\n0\n0\n0\n...\n15\n15\n15\n0\n0\n0\n0\n0\n0\n15\n\n\n2\n761\n761\n761\n761\n761\n761\n761\n0\n0\n0\n...\n761\n761\n755\n0\n0\n0\n0\n0\n0\n761\n\n\n3\n3625\n3625\n3625\n3625\n3625\n3625\n3625\n0\n0\n0\n...\n3625\n3625\n3612\n0\n0\n0\n0\n0\n0\n3625\n\n\n4\n179\n179\n179\n179\n179\n179\n179\n0\n0\n0\n...\n179\n179\n179\n0\n0\n0\n0\n0\n0\n179\n\n\n\n\n4 rows × 194 columns\n\n\n\n\n\n#plot_spatial_weights(blockW, df)\n\n\nExercise:  Which spatial weights structure would be more dense, tracts based on rook contiguity or SoCal tracts based on knn with k=4?\n\n\nExercise:  How many tracts have fewer neighbors under rook contiguity relative to knn4?\n\n\nExercise:  How many tracts have identical neighbors under queen contiguity and queen rook contiguity?\n\n\n# %load solutions/02.py\n\n\nSpatial Weights by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "week-08/spatial_weights.html#weight-types",
    "href": "week-08/spatial_weights.html#weight-types",
    "title": "Spatial Weights",
    "section": "",
    "text": "A commonly-used type of weight is a queen contigutiy weight, which reflects adjacency relationships as a binary indicator variable denoting whether or not a polygon shares an edge or a vertex with another polygon. These weights are symmetric, in that when polygon \\(A\\) neighbors polygon \\(B\\), both \\(w_{AB} = 1\\) and \\(w_{BA} = 1\\).\nTo construct queen weights from a shapefile, we will use geopandas to read the file into a GeoDataFrame, and then use libpysal to construct the weights:\n\npath = \"~/data/scag_region.parquet\"\ndf = geopandas.read_parquet(path)\ndf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\ndf = df.to_crs(26911)  #UTM zone 11N\n\n\nqW = Queen.from_dataframe(df)\n\n\nqW\n\n&lt;libpysal.weights.contiguity.Queen at 0x7d1d8341d930&gt;\n\n\nAll weights objects have a few traits that you can use to work with the weights object, as well as to get information about the weights object.\nTo get the neighbors & weights around an observation, use the observation’s index on the weights object, like a dictionary:\n\nqW[155] #neighbors & weights of the 156th observation (0-index remember)\n\n{4528: 1.0, 547: 1.0, 2133: 1.0, 2744: 1.0}\n\n\nBy default, the weights and the pandas dataframe will use the same index. So, we can view the observation and its neighbors in the dataframe by putting the observation’s index and its neighbors’ indexes together in one list:\n\nself_and_neighbors = [155]\nself_and_neighbors.extend(qW.neighbors[155])\nprint(self_and_neighbors)\n\n[155, 4528, 547, 2133, 2744]\n\n\nand grabbing those elements from the dataframe:\n\ndf.loc[self_and_neighbors]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n155\n06037552302\n26.0\n80.0\n630.0\n0.0\n0.0\n913.0\nNaN\nNaN\nNaN\n...\n2010\n1268.0\n22.060410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401577.943 3752226.320, 401578.535 3...\n\n\n4528\n06037552400\n0.0\n20.0\n670.0\n0.0\n24.0\n821.0\nNaN\nNaN\nNaN\n...\n2010\n588.0\n5.576363\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401878.957 3751827.308, 402203.594 3...\n\n\n547\n06037552301\n59.0\n103.0\n1079.0\n5.0\n0.0\n1777.0\nNaN\nNaN\nNaN\n...\n2010\n1272.0\n8.472352\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400855.726 3753114.791, 400857.855 3...\n\n\n2133\n06037552200\n38.0\n141.0\n1484.0\n0.0\n52.0\n2235.0\nNaN\nNaN\nNaN\n...\n2010\n1902.0\n6.858581\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399998.860 3752819.258, 400004.819 3...\n\n\n2744\n06037504102\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n...\n2010\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401631.531 3751829.535, 401878.957 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\nA full, dense matrix describing all of the pairwise relationships is constructed using the .full method, or when libpysal.weights.full is called on a weights object:\n\nWmatrix, ids = qW.full()\n#Wmatrix, ids = libpysal.weights.full(qW)\n\n\nWmatrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nn_neighbors = Wmatrix.sum(axis=1) # how many neighbors each region has\n\n\nn_neighbors[155]\n\n4.0\n\n\n\nqW.cardinalities[155]\n\n4\n\n\nNote that this matrix is binary, in that its elements are either zero or one, since an observation is either a neighbor or it is not a neighbor.\nHowever, many common use cases of spatial weights require that the matrix is row-standardized. This is done simply in PySAL using the .transform attribute\n\nqW.transform = 'r'\n\n('WARNING: ', 4285, ' is an island (no neighbors)')\n\n\nNow, if we build a new full matrix, its rows should sum to one:\n\nWmatrix, ids = qW.full()\n\n\nWmatrix.sum(axis=1) #numpy axes are 0:column, 1:row, 2:facet, into higher dimensions\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\nSince weight matrices are typically very sparse, there is also a sparse weights matrix constructor:\n\nqW.sparse\n\n&lt;4580x4580 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 29374 stored elements in Compressed Sparse Row format&gt;\n\n\n\nqW.pct_nonzero #Percentage of nonzero neighbor counts\n\n0.14003356152628668\n\n\nLet’s look at the neighborhoods of the 101th observation\n\ndf.iloc[100]\n\ngeoid                                                        06037910606\nn_asian_under_15                                                     0.0\nn_black_under_15                                                   210.0\nn_hispanic_under_15                                                757.0\nn_native_under_15                                                    3.0\n                                             ...                        \np_hispanic_over_60                                                   NaN\np_native_over_60                                                     NaN\np_asian_over_60                                                      NaN\np_disabled                                                           NaN\ngeometry               POLYGON ((401275.2896923868 3825401.4434467247...\nName: 100, Length: 194, dtype: object\n\n\n\nqW.neighbors[100]\n\n[789, 790, 1991, 3676, 791]\n\n\n\nlen(qW.neighbors[100])\n\n5\n\n\n\ndf.iloc[qW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(qW, df)\n\n\n\n\n\n\n\n\nBy default, PySAL assigns each observation an index according to the order in which the observation was read in. This means that, by default, all of the observations in the weights object are indexed by table order.\n\npandas.Series(qW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\nqW.cardinalities.values()\n\ndict_values([9, 9, 4, 7, 7, 5, 5, 6, 5, 8, 9, 8, 4, 3, 5, 5, 6, 6, 4, 5, 5, 6, 7, 9, 6, 4, 7, 8, 7, 5, 7, 2, 6, 6, 8, 3, 7, 7, 5, 8, 6, 5, 5, 4, 6, 6, 7, 7, 4, 6, 7, 4, 5, 6, 13, 6, 7, 6, 8, 6, 6, 6, 2, 6, 6, 8, 6, 7, 7, 6, 3, 5, 6, 6, 3, 7, 6, 5, 5, 5, 8, 8, 6, 8, 9, 7, 7, 6, 7, 5, 5, 7, 7, 6, 5, 7, 8, 8, 4, 7, 5, 4, 4, 6, 9, 6, 6, 7, 4, 8, 6, 6, 5, 6, 6, 6, 6, 7, 6, 8, 6, 6, 6, 8, 6, 6, 5, 8, 4, 5, 7, 5, 5, 5, 5, 5, 2, 4, 4, 7, 6, 8, 6, 9, 4, 6, 7, 5, 5, 6, 6, 4, 8, 9, 7, 4, 8, 4, 6, 5, 5, 4, 5, 6, 7, 8, 4, 6, 5, 6, 6, 6, 4, 6, 7, 6, 5, 6, 7, 6, 7, 7, 7, 7, 3, 10, 6, 6, 7, 7, 5, 6, 7, 8, 6, 5, 9, 7, 9, 6, 6, 4, 6, 6, 5, 7, 7, 7, 6, 4, 7, 8, 7, 5, 6, 6, 4, 6, 6, 5, 9, 7, 5, 7, 4, 7, 7, 3, 6, 7, 5, 5, 6, 6, 5, 4, 6, 5, 5, 6, 5, 10, 4, 3, 6, 1, 8, 6, 4, 5, 5, 7, 6, 4, 7, 4, 5, 6, 6, 5, 10, 3, 5, 5, 9, 5, 7, 5, 5, 7, 5, 8, 4, 6, 5, 7, 7, 7, 5, 6, 7, 5, 3, 7, 5, 4, 6, 3, 5, 6, 5, 5, 5, 4, 4, 7, 7, 5, 5, 5, 7, 9, 6, 4, 4, 5, 7, 4, 4, 7, 4, 6, 6, 4, 8, 6, 7, 5, 8, 6, 7, 6, 8, 8, 4, 5, 7, 6, 3, 5, 5, 4, 6, 6, 7, 5, 5, 5, 3, 5, 7, 6, 8, 5, 5, 5, 7, 6, 6, 7, 3, 4, 8, 4, 7, 4, 6, 6, 4, 4, 4, 5, 3, 5, 6, 4, 6, 7, 8, 5, 6, 6, 6, 7, 5, 8, 5, 3, 5, 5, 6, 4, 7, 7, 7, 4, 6, 5, 6, 9, 4, 7, 5, 5, 5, 7, 4, 7, 8, 7, 7, 6, 5, 5, 7, 5, 9, 5, 5, 6, 4, 8, 6, 4, 5, 5, 5, 7, 6, 6, 3, 6, 7, 5, 4, 5, 5, 7, 7, 5, 4, 6, 6, 6, 5, 5, 5, 17, 7, 6, 6, 14, 5, 5, 6, 6, 4, 11, 7, 6, 4, 6, 5, 7, 6, 6, 4, 7, 4, 6, 4, 7, 7, 6, 8, 5, 6, 8, 6, 5, 8, 8, 6, 4, 5, 5, 3, 6, 4, 6, 6, 6, 5, 6, 4, 4, 6, 7, 7, 8, 4, 5, 17, 7, 5, 6, 5, 7, 8, 6, 4, 7, 5, 7, 6, 4, 4, 5, 5, 4, 4, 4, 5, 3, 5, 4, 5, 3, 4, 5, 7, 7, 4, 7, 5, 4, 5, 12, 9, 9, 5, 4, 6, 7, 6, 6, 3, 8, 4, 8, 4, 6, 8, 4, 6, 5, 9, 6, 7, 4, 4, 8, 7, 4, 8, 5, 5, 7, 6, 5, 7, 4, 8, 6, 5, 8, 6, 6, 6, 6, 4, 5, 6, 8, 5, 8, 5, 8, 5, 8, 8, 6, 8, 7, 6, 7, 6, 5, 5, 7, 9, 3, 6, 8, 8, 7, 8, 5, 4, 6, 5, 6, 9, 7, 6, 7, 8, 3, 6, 6, 4, 7, 6, 5, 3, 5, 5, 8, 6, 8, 3, 8, 6, 8, 6, 6, 5, 8, 4, 8, 8, 7, 4, 5, 6, 6, 7, 7, 7, 5, 7, 4, 8, 6, 8, 8, 6, 4, 5, 7, 6, 6, 7, 5, 7, 8, 4, 6, 7, 6, 6, 8, 7, 7, 4, 4, 8, 7, 7, 8, 11, 6, 7, 6, 10, 5, 6, 6, 6, 4, 5, 5, 8, 7, 4, 7, 6, 8, 6, 7, 6, 7, 7, 6, 6, 5, 6, 6, 6, 5, 7, 9, 4, 5, 6, 6, 5, 6, 7, 5, 10, 5, 7, 4, 5, 4, 5, 5, 5, 6, 10, 6, 6, 6, 5, 6, 7, 4, 4, 6, 6, 8, 6, 7, 8, 8, 6, 3, 5, 6, 5, 7, 5, 5, 7, 8, 6, 7, 5, 5, 6, 6, 6, 5, 6, 6, 8, 5, 8, 4, 6, 8, 4, 5, 3, 5, 5, 8, 5, 8, 7, 8, 7, 6, 10, 5, 7, 4, 4, 3, 9, 4, 6, 5, 4, 7, 7, 6, 9, 4, 6, 5, 5, 7, 5, 5, 9, 5, 6, 6, 5, 6, 5, 7, 5, 8, 8, 6, 7, 5, 7, 5, 5, 7, 7, 4, 7, 8, 6, 8, 7, 5, 7, 10, 5, 4, 7, 5, 6, 7, 7, 5, 5, 4, 6, 5, 6, 6, 7, 5, 7, 7, 6, 6, 5, 6, 4, 4, 6, 5, 6, 5, 6, 6, 5, 4, 5, 9, 9, 4, 5, 8, 6, 8, 5, 7, 7, 6, 6, 4, 9, 8, 6, 10, 5, 7, 6, 3, 6, 6, 8, 7, 5, 7, 5, 6, 8, 5, 6, 5, 5, 7, 4, 5, 4, 4, 5, 4, 5, 7, 5, 7, 6, 5, 7, 7, 6, 7, 6, 7, 7, 6, 7, 4, 4, 6, 7, 4, 6, 7, 8, 6, 8, 8, 2, 7, 4, 9, 5, 7, 7, 4, 6, 6, 4, 5, 5, 5, 9, 6, 8, 6, 9, 9, 7, 7, 5, 7, 5, 5, 5, 5, 4, 7, 6, 5, 6, 9, 6, 4, 6, 5, 6, 7, 6, 6, 4, 5, 3, 7, 4, 5, 6, 10, 4, 7, 6, 4, 6, 6, 8, 5, 6, 6, 7, 6, 6, 4, 7, 6, 7, 8, 6, 6, 7, 9, 6, 6, 9, 6, 8, 3, 5, 7, 8, 9, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 7, 3, 6, 6, 5, 9, 5, 8, 7, 5, 6, 5, 7, 7, 7, 6, 6, 8, 6, 5, 6, 7, 3, 6, 6, 8, 7, 6, 8, 5, 7, 6, 5, 6, 7, 4, 4, 7, 5, 20, 6, 5, 6, 6, 6, 7, 5, 6, 7, 5, 4, 5, 4, 9, 4, 7, 6, 8, 7, 5, 5, 5, 5, 7, 5, 6, 6, 5, 3, 5, 4, 6, 8, 6, 2, 7, 5, 5, 5, 6, 8, 7, 7, 7, 8, 7, 6, 5, 5, 6, 5, 6, 5, 7, 6, 7, 4, 6, 4, 5, 7, 5, 9, 6, 7, 7, 5, 10, 8, 5, 5, 5, 7, 6, 6, 7, 6, 7, 6, 5, 4, 5, 5, 6, 7, 10, 4, 4, 6, 6, 8, 7, 5, 6, 6, 9, 5, 5, 6, 6, 8, 5, 7, 6, 4, 9, 7, 5, 5, 6, 5, 16, 5, 4, 6, 4, 8, 8, 6, 6, 7, 7, 5, 10, 7, 6, 7, 6, 7, 5, 7, 5, 7, 6, 7, 6, 4, 8, 6, 7, 6, 6, 5, 4, 4, 5, 9, 4, 8, 8, 7, 6, 6, 11, 6, 4, 6, 6, 4, 7, 5, 6, 7, 5, 5, 3, 7, 5, 5, 3, 6, 6, 9, 5, 8, 8, 3, 6, 5, 5, 8, 5, 5, 7, 5, 9, 8, 9, 6, 6, 6, 7, 8, 8, 7, 6, 7, 9, 4, 8, 9, 9, 7, 7, 8, 6, 5, 6, 5, 5, 6, 5, 5, 8, 6, 7, 5, 7, 4, 7, 5, 10, 7, 8, 5, 6, 9, 9, 9, 10, 4, 5, 6, 9, 4, 5, 6, 6, 5, 6, 8, 9, 9, 9, 10, 7, 9, 5, 3, 6, 7, 7, 9, 7, 8, 9, 8, 7, 8, 6, 7, 6, 9, 9, 5, 8, 3, 3, 7, 5, 7, 9, 9, 8, 7, 7, 10, 6, 8, 7, 6, 8, 6, 8, 5, 5, 5, 4, 5, 7, 8, 9, 7, 3, 6, 4, 7, 7, 7, 6, 10, 6, 7, 8, 7, 10, 9, 7, 6, 8, 5, 7, 7, 6, 5, 5, 5, 7, 5, 6, 4, 5, 6, 4, 9, 7, 4, 6, 4, 9, 9, 6, 8, 5, 9, 6, 9, 6, 7, 7, 6, 5, 10, 10, 7, 7, 7, 7, 8, 6, 3, 7, 4, 5, 6, 4, 10, 9, 5, 8, 6, 9, 5, 7, 5, 7, 5, 8, 8, 9, 9, 13, 7, 7, 8, 6, 1, 7, 6, 5, 6, 7, 9, 5, 7, 6, 4, 4, 3, 7, 8, 6, 7, 8, 6, 4, 6, 9, 8, 6, 6, 6, 8, 7, 7, 5, 5, 2, 3, 7, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 6, 9, 4, 5, 5, 3, 5, 10, 5, 15, 4, 6, 9, 8, 8, 7, 5, 8, 7, 8, 9, 8, 7, 5, 6, 8, 4, 9, 6, 9, 6, 6, 6, 5, 7, 10, 11, 11, 6, 5, 6, 7, 4, 5, 11, 16, 5, 7, 9, 7, 6, 7, 7, 6, 5, 9, 6, 10, 7, 5, 7, 6, 5, 3, 8, 3, 11, 5, 5, 9, 9, 7, 6, 6, 8, 10, 6, 10, 4, 8, 7, 4, 7, 4, 5, 9, 4, 5, 9, 9, 6, 6, 8, 9, 9, 4, 7, 10, 16, 7, 11, 7, 11, 5, 8, 9, 5, 11, 7, 5, 7, 5, 5, 7, 8, 9, 5, 11, 11, 7, 8, 6, 5, 6, 5, 7, 9, 4, 7, 8, 7, 8, 4, 6, 7, 9, 9, 8, 6, 4, 7, 4, 6, 7, 6, 8, 6, 7, 4, 8, 5, 7, 7, 10, 7, 7, 6, 4, 6, 6, 13, 3, 8, 9, 6, 4, 6, 5, 6, 12, 5, 2, 6, 5, 7, 6, 7, 8, 5, 7, 4, 6, 7, 10, 3, 17, 10, 8, 6, 5, 7, 14, 1, 12, 8, 4, 4, 8, 8, 4, 8, 6, 8, 6, 5, 3, 8, 6, 7, 8, 6, 9, 8, 5, 11, 5, 3, 7, 5, 4, 5, 3, 7, 7, 6, 5, 8, 2, 5, 6, 11, 8, 6, 6, 7, 6, 7, 6, 6, 4, 4, 6, 9, 6, 9, 6, 5, 4, 3, 5, 8, 7, 10, 4, 6, 7, 4, 6, 6, 4, 6, 4, 8, 6, 10, 8, 5, 8, 6, 4, 6, 6, 5, 11, 4, 7, 8, 7, 6, 5, 5, 10, 16, 8, 5, 7, 9, 3, 7, 5, 3, 4, 8, 6, 6, 7, 6, 9, 7, 7, 6, 6, 11, 7, 7, 8, 6, 6, 7, 4, 7, 6, 3, 7, 7, 6, 8, 15, 7, 6, 4, 6, 9, 6, 6, 6, 10, 7, 8, 4, 9, 11, 6, 6, 6, 4, 11, 6, 5, 5, 5, 5, 7, 5, 5, 5, 4, 4, 8, 4, 6, 5, 7, 8, 6, 7, 5, 6, 7, 7, 6, 4, 10, 5, 6, 7, 5, 4, 7, 5, 6, 5, 7, 5, 5, 10, 6, 5, 5, 3, 7, 5, 6, 6, 6, 7, 5, 4, 7, 6, 11, 6, 8, 6, 9, 5, 14, 4, 5, 13, 8, 6, 7, 4, 7, 5, 9, 5, 8, 4, 7, 7, 8, 7, 4, 7, 5, 10, 8, 9, 5, 7, 6, 4, 7, 8, 8, 12, 7, 6, 8, 10, 5, 7, 6, 6, 6, 5, 5, 5, 6, 9, 5, 7, 4, 7, 8, 5, 7, 6, 5, 7, 7, 4, 4, 6, 7, 7, 6, 8, 6, 5, 4, 6, 5, 5, 4, 6, 6, 6, 7, 5, 4, 6, 3, 6, 7, 1, 7, 11, 4, 4, 12, 7, 6, 8, 5, 7, 8, 3, 6, 5, 5, 5, 9, 6, 9, 6, 5, 3, 7, 6, 8, 8, 9, 7, 5, 7, 6, 6, 5, 5, 10, 4, 8, 5, 8, 4, 5, 6, 5, 14, 8, 1, 5, 6, 7, 8, 10, 4, 7, 5, 9, 5, 5, 7, 4, 4, 7, 5, 8, 6, 9, 6, 8, 7, 4, 6, 4, 7, 5, 4, 6, 3, 7, 6, 6, 7, 11, 8, 6, 7, 8, 5, 7, 7, 9, 6, 5, 6, 6, 8, 6, 9, 7, 7, 7, 8, 5, 5, 5, 9, 6, 5, 4, 7, 7, 5, 7, 5, 5, 6, 8, 6, 5, 6, 4, 6, 6, 7, 7, 5, 6, 9, 6, 5, 6, 5, 8, 9, 7, 11, 7, 11, 15, 6, 7, 8, 3, 10, 8, 10, 8, 8, 4, 6, 5, 7, 6, 5, 6, 6, 7, 5, 9, 9, 9, 7, 11, 6, 6, 6, 5, 6, 6, 8, 6, 7, 5, 7, 8, 10, 10, 9, 7, 3, 9, 10, 7, 7, 6, 10, 6, 6, 7, 4, 7, 4, 5, 6, 6, 5, 5, 7, 8, 7, 4, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 8, 7, 7, 5, 8, 10, 8, 7, 7, 6, 6, 8, 7, 6, 5, 6, 6, 9, 6, 2, 7, 7, 7, 7, 6, 7, 7, 4, 9, 12, 5, 5, 5, 5, 4, 9, 5, 7, 6, 7, 6, 4, 8, 7, 7, 7, 3, 5, 6, 5, 5, 7, 8, 6, 9, 4, 5, 6, 7, 5, 4, 6, 9, 5, 6, 10, 10, 6, 8, 5, 6, 4, 7, 8, 8, 5, 4, 9, 6, 4, 6, 5, 7, 5, 7, 6, 4, 10, 5, 6, 5, 5, 4, 6, 4, 6, 16, 8, 3, 7, 6, 5, 5, 5, 7, 7, 6, 9, 7, 6, 6, 6, 8, 3, 8, 4, 5, 5, 6, 6, 8, 9, 4, 7, 8, 7, 5, 6, 6, 6, 12, 4, 5, 6, 7, 7, 4, 8, 5, 7, 6, 7, 7, 9, 8, 7, 6, 7, 7, 6, 7, 5, 6, 3, 6, 5, 3, 6, 10, 7, 6, 6, 6, 6, 13, 10, 6, 8, 4, 6, 5, 7, 8, 6, 7, 5, 8, 7, 7, 8, 13, 5, 10, 7, 6, 7, 7, 6, 7, 7, 7, 8, 4, 9, 7, 4, 5, 4, 6, 8, 8, 6, 10, 3, 5, 10, 8, 6, 6, 11, 7, 6, 6, 5, 5, 8, 4, 6, 4, 13, 4, 11, 7, 5, 7, 6, 6, 7, 5, 5, 6, 5, 10, 5, 5, 8, 10, 10, 6, 6, 7, 6, 8, 5, 5, 2, 5, 5, 11, 6, 6, 8, 13, 2, 3, 5, 4, 6, 5, 4, 5, 5, 5, 11, 5, 8, 7, 8, 7, 5, 6, 5, 6, 10, 3, 9, 5, 4, 6, 6, 9, 8, 6, 9, 6, 7, 5, 6, 3, 6, 9, 8, 7, 7, 4, 5, 8, 5, 8, 8, 7, 6, 8, 14, 6, 4, 7, 3, 9, 5, 6, 5, 5, 6, 7, 3, 9, 9, 5, 6, 6, 4, 4, 9, 7, 5, 4, 5, 15, 8, 7, 9, 6, 6, 5, 7, 6, 8, 4, 4, 5, 5, 3, 5, 3, 4, 4, 4, 7, 12, 8, 9, 9, 6, 3, 6, 4, 7, 7, 9, 4, 6, 9, 5, 7, 5, 10, 5, 10, 6, 9, 4, 6, 8, 5, 8, 12, 10, 5, 7, 6, 7, 10, 7, 9, 6, 7, 5, 6, 6, 8, 6, 6, 8, 4, 6, 6, 9, 6, 6, 7, 4, 4, 3, 8, 10, 6, 6, 25, 8, 8, 5, 5, 4, 7, 7, 5, 7, 6, 7, 7, 6, 6, 5, 8, 6, 6, 7, 6, 8, 5, 4, 5, 8, 6, 12, 6, 7, 8, 4, 4, 7, 7, 9, 9, 14, 3, 10, 6, 6, 5, 7, 14, 5, 8, 4, 8, 8, 6, 6, 4, 6, 10, 14, 8, 5, 7, 6, 9, 5, 6, 7, 7, 5, 7, 5, 6, 9, 6, 6, 8, 7, 3, 6, 5, 9, 4, 4, 6, 13, 4, 6, 4, 5, 5, 7, 6, 7, 14, 3, 5, 11, 6, 7, 7, 7, 5, 5, 6, 14, 7, 7, 7, 5, 3, 4, 8, 4, 6, 8, 2, 6, 10, 5, 12, 8, 9, 6, 5, 13, 6, 8, 5, 2, 5, 1, 5, 6, 5, 5, 4, 9, 6, 7, 3, 8, 5, 6, 7, 6, 7, 8, 7, 3, 8, 6, 7, 5, 7, 7, 6, 5, 7, 11, 9, 6, 6, 3, 4, 9, 8, 8, 8, 6, 5, 6, 5, 7, 15, 8, 10, 9, 10, 6, 5, 7, 6, 10, 6, 5, 12, 5, 5, 8, 8, 9, 4, 7, 4, 4, 8, 15, 6, 4, 12, 6, 6, 4, 6, 6, 8, 4, 7, 8, 6, 6, 8, 4, 5, 9, 7, 6, 6, 7, 7, 6, 6, 7, 9, 6, 7, 6, 8, 5, 5, 5, 10, 8, 6, 6, 7, 5, 6, 6, 6, 8, 8, 7, 6, 8, 5, 6, 7, 7, 7, 4, 7, 6, 6, 4, 11, 4, 7, 6, 5, 9, 10, 8, 6, 7, 6, 7, 6, 6, 7, 4, 7, 7, 5, 7, 7, 6, 9, 6, 6, 6, 7, 8, 4, 5, 3, 7, 5, 6, 8, 6, 6, 6, 16, 6, 5, 15, 10, 6, 7, 9, 7, 7, 8, 5, 9, 6, 5, 5, 4, 9, 11, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 8, 5, 4, 3, 8, 5, 4, 4, 5, 5, 6, 5, 3, 10, 8, 5, 9, 9, 6, 5, 3, 5, 6, 7, 7, 8, 5, 8, 6, 6, 4, 5, 4, 5, 9, 9, 4, 7, 5, 6, 9, 7, 4, 8, 8, 7, 6, 10, 7, 8, 11, 5, 7, 5, 6, 7, 9, 8, 7, 7, 8, 10, 3, 4, 6, 7, 7, 5, 7, 6, 5, 6, 9, 10, 3, 7, 5, 7, 8, 9, 5, 6, 2, 9, 7, 7, 4, 6, 6, 9, 7, 9, 6, 6, 7, 6, 7, 6, 5, 7, 8, 8, 5, 6, 6, 8, 6, 6, 6, 8, 7, 8, 6, 6, 8, 5, 7, 8, 4, 6, 5, 7, 8, 7, 7, 9, 5, 1, 8, 7, 5, 5, 7, 8, 5, 8, 6, 6, 4, 6, 6, 6, 7, 5, 5, 4, 7, 7, 7, 7, 6, 7, 5, 6, 6, 7, 6, 5, 6, 7, 9, 6, 7, 7, 4, 7, 7, 5, 6, 8, 5, 7, 7, 7, 7, 6, 7, 6, 14, 6, 9, 9, 10, 5, 6, 7, 7, 9, 4, 8, 10, 4, 8, 6, 5, 5, 6, 6, 6, 7, 7, 6, 3, 5, 10, 8, 9, 6, 8, 1, 4, 6, 7, 5, 7, 8, 7, 7, 7, 6, 5, 7, 5, 4, 10, 7, 9, 5, 8, 10, 7, 5, 9, 6, 6, 7, 8, 7, 7, 6, 7, 5, 6, 5, 8, 9, 6, 6, 6, 10, 5, 6, 7, 6, 8, 6, 7, 7, 6, 9, 5, 8, 4, 5, 7, 6, 5, 4, 4, 7, 8, 5, 5, 7, 5, 8, 7, 9, 6, 10, 6, 8, 8, 6, 7, 6, 6, 7, 9, 6, 5, 5, 8, 6, 7, 10, 6, 4, 14, 3, 6, 6, 8, 8, 7, 9, 6, 6, 10, 6, 8, 7, 6, 8, 6, 4, 5, 5, 5, 8, 4, 4, 6, 4, 6, 11, 4, 5, 6, 4, 6, 7, 5, 5, 3, 5, 6, 6, 9, 8, 9, 6, 6, 6, 6, 6, 8, 7, 7, 6, 4, 6, 5, 8, 6, 6, 5, 7, 8, 5, 7, 7, 6, 7, 3, 7, 4, 6, 5, 6, 5, 6, 6, 7, 10, 6, 6, 9, 7, 14, 6, 5, 7, 9, 8, 8, 9, 5, 4, 9, 4, 6, 5, 6, 8, 7, 6, 5, 5, 6, 5, 6, 7, 6, 6, 7, 7, 7, 6, 7, 6, 7, 6, 11, 6, 7, 7, 5, 4, 7, 6, 9, 7, 3, 6, 8, 5, 6, 3, 6, 7, 6, 3, 7, 11, 5, 7, 6, 6, 5, 6, 5, 7, 6, 8, 8, 9, 6, 6, 6, 7, 7, 10, 7, 9, 7, 6, 9, 8, 7, 6, 6, 5, 7, 8, 5, 7, 6, 6, 6, 9, 7, 4, 8, 7, 8, 7, 6, 6, 10, 5, 8, 4, 8, 5, 6, 7, 7, 7, 8, 7, 6, 9, 6, 7, 5, 4, 5, 7, 7, 5, 7, 7, 6, 6, 5, 7, 6, 5, 7, 5, 6, 8, 8, 5, 5, 5, 10, 6, 5, 6, 4, 9, 5, 6, 9, 8, 8, 7, 6, 8, 10, 6, 11, 6, 5, 10, 6, 8, 8, 7, 6, 10, 5, 7, 9, 6, 4, 8, 9, 7, 6, 6, 6, 5, 10, 6, 9, 5, 7, 7, 8, 6, 7, 5, 7, 6, 4, 7, 6, 8, 8, 5, 7, 6, 4, 7, 4, 5, 8, 5, 5, 9, 8, 5, 8, 6, 4, 7, 5, 3, 6, 6, 4, 7, 3, 5, 4, 4, 7, 8, 8, 7, 10, 8, 6, 6, 7, 5, 6, 5, 7, 8, 4, 7, 9, 5, 7, 5, 5, 7, 7, 7, 5, 5, 5, 6, 9, 6, 6, 8, 7, 5, 5, 7, 6, 8, 6, 8, 10, 9, 7, 8, 5, 6, 6, 7, 6, 8, 7, 6, 8, 5, 7, 5, 6, 7, 6, 5, 8, 5, 7, 3, 7, 6, 7, 8, 8, 4, 6, 2, 8, 6, 6, 5, 7, 5, 8, 8, 9, 7, 7, 7, 8, 7, 8, 6, 5, 8, 11, 10, 7, 7, 4, 6, 8, 5, 4, 8, 3, 5, 6, 8, 9, 7, 4, 5, 8, 8, 5, 5, 6, 6, 6, 7, 9, 6, 6, 11, 4, 7, 5, 9, 9, 6, 8, 6, 6, 5, 5, 8, 7, 7, 5, 7, 6, 12, 7, 6, 7, 5, 1, 10, 5, 3, 7, 5, 5, 6, 6, 7, 8, 8, 7, 5, 3, 5, 7, 7, 8, 9, 4, 5, 8, 8, 6, 5, 7, 7, 6, 7, 5, 8, 11, 5, 5, 4, 5, 5, 1, 9, 6, 9, 9, 5, 6, 7, 7, 9, 6, 7, 7, 7, 5, 4, 5, 6, 6, 5, 6, 4, 6, 6, 5, 5, 3, 5, 7, 4, 6, 4, 8, 6, 6, 6, 6, 6, 7, 4, 3, 4, 12, 6, 6, 6, 6, 8, 6, 6, 7, 12, 8, 5, 11, 4, 6, 6, 5, 6, 7, 6, 5, 7, 7, 10, 6, 5, 7, 6, 5, 6, 6, 6, 6, 5, 10, 19, 7, 7, 8, 5, 6, 9, 6, 6, 12, 5, 4, 5, 3, 12, 4, 6, 4, 7, 4, 9, 4, 5, 3, 4, 7, 9, 6, 5, 7, 8, 5, 6, 5, 4, 8, 7, 5, 7, 5, 4, 7, 6, 4, 7, 5, 5, 7, 6, 7, 8, 11, 5, 5, 8, 3, 5, 4, 6, 6, 3, 7, 7, 5, 6, 9, 12, 7, 5, 5, 6, 9, 5, 7, 10, 6, 9, 5, 6, 6, 6, 7, 8, 6, 5, 7, 5, 7, 5, 5, 5, 8, 6, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 6, 10, 8, 7, 7, 5, 6, 6, 5, 12, 7, 8, 6, 6, 5, 8, 5, 5, 5, 5, 6, 5, 9, 10, 6, 6, 5, 4, 5, 4, 6, 6, 5, 7, 5, 7, 7, 4, 7, 5, 9, 6, 6, 4, 6, 6, 6, 5, 6, 6, 6, 6, 4, 8, 3, 5, 9, 7, 9, 6, 4, 12, 6, 7, 6, 7, 6, 8, 16, 7, 7, 5, 10, 7, 8, 6, 6, 7, 7, 11, 6, 11, 5, 6, 9, 5, 8, 6, 7, 5, 6, 7, 6, 8, 9, 7, 2, 10, 5, 7, 6, 7, 6, 6, 6, 6, 10, 4, 4, 6, 8, 6, 6, 9, 7, 6, 2, 6, 7, 5, 7, 5, 15, 8, 6, 8, 4, 6, 7, 7, 7, 8, 8, 7, 5, 5, 6, 5, 7, 7, 7, 5, 6, 9, 8, 9, 5, 6, 4, 6, 5, 5, 7, 5, 8, 5, 4, 5, 5, 7, 7, 7, 5, 6, 8, 9, 4, 6, 6, 11, 6, 8, 9, 6, 5, 6, 6, 8, 6, 5, 7, 6, 5, 7, 4, 9, 5, 4, 6, 6, 6, 3, 7, 6, 6, 5, 13, 7, 6, 8, 5, 7, 5, 5, 8, 7, 7, 7, 7, 6, 9, 10, 8, 9, 6, 6, 7, 10, 6, 7, 7, 6, 6, 6, 6, 5, 7, 7, 5, 6, 4, 6, 7, 9, 8, 4, 9, 5, 5, 6, 4, 7, 8, 9, 7, 2, 4, 7, 7, 7, 7, 7, 11, 8, 9, 6, 4, 6, 6, 5, 8, 7, 7, 6, 7, 8, 8, 8, 6, 6, 8, 9, 8, 9, 7, 7, 5, 7, 5, 9, 6, 4, 6, 7, 4, 9, 7, 4, 4, 5, 9, 10, 8, 10, 9, 5, 6, 6, 5, 7, 7, 6, 5, 6, 9, 8, 7, 4, 6, 6, 7, 4, 5, 7, 7, 5, 9, 6, 7, 6, 9, 9, 8, 7, 8, 5, 3, 4, 11, 14, 5, 5, 4, 3, 6, 6, 5, 6, 7, 6, 8, 8, 9, 7, 6, 2, 6, 6, 5, 5, 5, 3, 8, 8, 11, 7, 6, 7, 6, 6, 7, 8, 6, 7, 7, 6, 6, 8, 8, 5, 5, 16, 8, 6, 7, 9, 5, 8, 6, 5, 4, 7, 5, 6, 8, 7, 5, 6, 7, 6, 6, 7, 10, 7, 10, 9, 8, 7, 5, 8, 5, 8, 7, 7, 9, 9, 5, 7, 6, 6, 5, 7, 5, 6, 8, 7, 6, 6, 10, 6, 5, 5, 8, 5, 7, 5, 5, 7, 6, 6, 5, 6, 0, 6, 11, 5, 5, 9, 6, 5, 9, 5, 6, 6, 6, 6, 6, 7, 7, 11, 10, 4, 7, 5, 8, 8, 8, 5, 6, 6, 7, 8, 7, 8, 5, 5, 5, 6, 7, 7, 5, 8, 5, 5, 5, 10, 8, 5, 10, 5, 4, 5, 5, 6, 5, 9, 5, 4, 5, 7, 4, 4, 6, 15, 7, 6, 8, 7, 10, 7, 7, 5, 6, 7, 6, 6, 6, 8, 5, 6, 4, 2, 6, 5, 8, 8, 5, 6, 5, 7, 10, 7, 8, 7, 6, 8, 7, 11, 7, 6, 6, 8, 7, 10, 16, 6, 7, 6, 4, 5, 4, 4, 7, 8, 8, 5, 6, 4, 6, 3, 5, 9, 8, 4, 6, 4, 6, 3, 6, 9, 7, 8, 3, 14, 6, 8, 4, 7, 6, 5, 6, 4, 8, 5, 7, 7, 7, 7, 5, 5, 7, 5, 6, 6, 7, 7, 7, 7, 4, 11, 6, 7, 4, 3, 13, 5, 8, 9, 8, 6, 8, 7, 7, 6, 7, 8, 6, 6, 9, 5, 8, 6, 7, 11, 6, 5, 8, 7, 9, 5, 7, 5, 7, 8, 4, 4, 8, 5, 7, 5, 5, 6, 7, 9, 7, 6, 11, 8, 3, 5, 7, 11, 4, 6, 7, 7, 7, 6, 5, 5, 7, 5, 6, 6, 7, 6, 5, 7, 8, 6, 5, 7, 14, 10, 6, 7, 7, 5, 6, 11, 5, 6, 5, 10, 4, 3, 6, 5, 8, 6, 6, 6, 7, 7, 8, 5, 7, 6, 5, 8, 7, 9, 6, 5, 6, 8, 5, 7, 7, 7, 6, 7, 6, 8, 5, 7, 9, 8, 5, 7, 5, 8, 8, 9, 12, 7, 4, 7, 10, 8, 7, 7, 6, 6, 7, 6, 7])\n\n\n\n\n\nRook weights are another type of contiguity weight, but consider observations as neighboring only when they share an edge. The rook neighbors of an observation may be different than its queen neighbors, depending on how the observation and its nearby polygons are configured.\nWe can construct this in the same way as the queen weights:\n\nrW = Rook.from_dataframe(df)\n\n\nrW.neighbors[100]\n\n[789, 790, 1991, 791, 3676]\n\n\n\nlen(rW.neighbors[100])\n\n5\n\n\n\ndf.iloc[rW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(rW, df)\n\n\n\n\n\n\n\n\n\npandas.Series(rW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\n\n\nIn theory, a “Bishop” weighting scheme is one that arises when only polygons that share vertexes are considered to be neighboring. But, since Queen contiguigy requires either an edge or a vertex and Rook contiguity requires only shared edges, the following relationship is true:\n\\[ \\mathcal{Q} = \\mathcal{R} \\cup \\mathcal{B} \\]\nwhere \\(\\mathcal{Q}\\) is the set of neighbor pairs via queen contiguity, \\(\\mathcal{R}\\) is the set of neighbor pairs via Rook contiguity, and \\(\\mathcal{B}\\) via Bishop contiguity. Thus:\n\\[ \\mathcal{Q} \\setminus \\mathcal{R} = \\mathcal{B}\\]\nBishop weights entail all Queen neighbor pairs that are not also Rook neighbors.\nPySAL does not have a dedicated bishop weights constructor, but you can construct very easily using the w_difference function. This function is one of a family of tools to work with weights, all defined in libpysal.weights, that conduct these types of set operations between weight objects.\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW.histogram\n\n[(0, 1624), (1, 1728), (2, 881), (3, 292), (4, 55)]\n\n\nThus, many tracts have no bishop neighbors. But, a few do. A simple way to see these observations in the dataframe is to find all elements of the dataframe that are not “islands,” the term for an observation with no neighbors:\n\nplot_spatial_weights(bW, df)"
  },
  {
    "objectID": "week-08/spatial_weights.html#distance",
    "href": "week-08/spatial_weights.html#distance",
    "title": "Spatial Weights",
    "section": "",
    "text": "There are many other kinds of weighting functions in PySAL. Another separate type use a continuous measure of distance to define neighborhoods.\n\ndf.crs\n\n&lt;Projected CRS: EPSG:26911&gt;\nName: NAD83 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: North America - between 120°W and 114°W - onshore and offshore. Canada - Alberta; British Columbia; Northwest Territories; Nunavut. United States (USA) - California; Idaho; Nevada, Oregon; Washington.\n- bounds: (-120.0, 30.88, -114.0, 83.5)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nOur coordinate system (UTM 11N) measures distance in meters, so that’s how we’ll define our neighbors\n\ndist_band = DistanceBand.from_dataframe(df, threshold=2000)\n\n\nplot_spatial_weights(dist_band,df)\n\n\n\n\n\n\n\n\n\n\n\nradius_mile = libpysal.cg.sphere.RADIUS_EARTH_MILES\nradius_mile\n\n3958.755865744055\n\n\n\ndf_latlong = df.to_crs(4326)\n\n\nknn8_bad = KNN.from_dataframe(df_latlong, k=8) # ignore curvature of the earth\n\n\nknn8_bad.histogram\n\n[(8, 4580)]\n\n\n\nknn8 = KNN.from_dataframe(df_latlong, k=8, radius=radius_mile)\n\n\nknn8.histogram\n\n[(8, 4580)]\n\n\n\nknn8_bad.neighbors[1487]\n\n[501, 2296, 2960, 974, 167, 4496, 2295, 4422]\n\n\n\nknn8.neighbors[1487]\n\n[501, 2960, 2296, 974, 167, 4496, 2881, 2297]\n\n\n\nset(knn8_bad.neighbors[1487]) == set(knn8.neighbors[1487])\n\nFalse\n\n\n\nExercise:  Enumerate the tracts for which ignoring curvature results in an incorrect neighbor set for knn.\n\n\n# %load solutions/02_knn.py\n\n\n\n\nKernel Weights are continuous distance-based weights that use kernel densities to define the neighbor relationship. Typically, they estimate a bandwidth, which is a parameter governing how far out observations should be considered neighboring. Then, using this bandwidth, they evaluate a continuous kernel function to provide a weight between 0 and 1.\nMany different choices of kernel functions are supported, and bandwidths can either be fixed (constant over all units) or adaptive in function of unit density.\nFor example, if we want to use adaptive bandwidths for the map and weight according to a gaussian kernel:\n\n\nbandwidth = the distance to the kth nearest neighbor for each observation\nbandwith is changing across observations\n\nkernelWa = Kernel.from_dataframe(df, k=10, fixed=False, function='gaussian')\n\n\nplot_spatial_weights(kernelWa, df)\n\n\n\n\n\n\n\n\n\nkernelWa.bandwidth\n\narray([[1687.99751736],\n       [1997.79636883],\n       [1803.3632643 ],\n       ...,\n       [2468.39103021],\n       [3480.79114847],\n       [1749.84752448]])\n\n\n\ndf.assign(bw=kernelWa.bandwidth.flatten()).plot('bw', cmap='Reds')"
  },
  {
    "objectID": "week-08/spatial_weights.html#block-weights",
    "href": "week-08/spatial_weights.html#block-weights",
    "title": "Spatial Weights",
    "section": "",
    "text": "w,s,e,n = df.total_bounds\n\n\nmx = (w+e)/2\nmy = (n+s)/2\n\n\nimport shapely\n\n\ncentroids = df.geometry.centroid\n\n\nlon = centroids.apply(lambda p: p.x).values\nlat = centroids.apply(lambda p: p.y).values\n\n\nnorth = lat &gt; my\nsouth = lat &lt;= my\neast = lon &gt; mx\nwest = lon &lt;= mx\n\n\nnw = west * north * 2\nne = east * north * 1\nsw = west * south * 3\nse = east * south *4\nquad = nw + ne + sw + se\n\n\nquad\n\narray([3, 2, 2, ..., 2, 4, 2])\n\n\n\ndf['quad'] = quad\ndf.plot(column=\"quad\", categorical=True)\n\n\n\n\n\n\n\n\n\nblockW = libpysal.weights.block_weights(df[\"quad\"])\n\n\nblockW.n\n\n4580\n\n\n\nblockW.pct_nonzero\n\n65.53761369920483\n\n\n\npandas.Series(blockW.cardinalities).plot.hist()\n\n\n\n\n\n\n\n\n\ndf.groupby(by='quad').count()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\nquad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n15\n15\n15\n15\n15\n15\n15\n0\n0\n0\n...\n15\n15\n15\n0\n0\n0\n0\n0\n0\n15\n\n\n2\n761\n761\n761\n761\n761\n761\n761\n0\n0\n0\n...\n761\n761\n755\n0\n0\n0\n0\n0\n0\n761\n\n\n3\n3625\n3625\n3625\n3625\n3625\n3625\n3625\n0\n0\n0\n...\n3625\n3625\n3612\n0\n0\n0\n0\n0\n0\n3625\n\n\n4\n179\n179\n179\n179\n179\n179\n179\n0\n0\n0\n...\n179\n179\n179\n0\n0\n0\n0\n0\n0\n179\n\n\n\n\n4 rows × 194 columns\n\n\n\n\n\n#plot_spatial_weights(blockW, df)\n\n\nExercise:  Which spatial weights structure would be more dense, tracts based on rook contiguity or SoCal tracts based on knn with k=4?\n\n\nExercise:  How many tracts have fewer neighbors under rook contiguity relative to knn4?\n\n\nExercise:  How many tracts have identical neighbors under queen contiguity and queen rook contiguity?\n\n\n# %load solutions/02.py\n\n\nSpatial Weights by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "week-14/quadrat.html",
    "href": "week-14/quadrat.html",
    "title": "Quadrat Statistics",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#introduction",
    "href": "week-14/quadrat.html#introduction",
    "title": "Quadrat Statistics",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#example-patterns",
    "href": "week-14/quadrat.html#example-patterns",
    "title": "Quadrat Statistics",
    "section": "Example Patterns",
    "text": "Example Patterns\nTo begin we are going to create two different point patterns, one from a CSR process and one from a clustered process. We will use these two patterns to introduce the different statistical methods used to analyze the patterns. Here we are in the rare circumstance in which we actually know what process generated the pattern.\n\nCSR n=60\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(12345)\nn = 60\nxy = np.random.rand(60,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);\n\n\n\n\n\n\n\n\n\nimport pointpats as pp\n\n\ncsr = pp.PointPattern(xy)\n\n\ncsr.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.00838829794155349,0.024676210429265266), (0.9940145858999619,0.9613067360728214)]\nArea of window: 0.9231676681785911\nIntensity estimate for window: 64.99361066054225\n          x         y\n0  0.929616  0.316376\n1  0.183919  0.204560\n2  0.567725  0.595545\n3  0.964515  0.653177\n4  0.748907  0.653570\n\n\n\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, n, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')\n\n\n\n\n\n\n\n\n\nclustered = draw.realizations[0]\n\n\nclustered.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.47331760265312733,0.023178703349462502), (0.9696584457277277,0.6150208352748628)]\nArea of window: 1.0\nIntensity estimate for window: 60.0\n          x         y\n0  0.513060  0.541971\n1  0.473318  0.578385\n2  0.508373  0.536200\n3  0.881716  0.060328\n4  0.894221  0.059273",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#quadrat-statistics",
    "href": "week-14/quadrat.html#quadrat-statistics",
    "title": "Quadrat Statistics",
    "section": "Quadrat Statistics",
    "text": "Quadrat Statistics\n\nimport pointpats.quadrat_statistics as qs\n\n\ncsr_qr = qs.QStatistic(csr, shape='rectangle', nx=3, ny=3)\ncsr_qr.plot()\n\n\n\n\n\n\n\n\n\ncsr_qr.chi2\n\n10.8\n\n\n\ncsr_qr.chi2_pvalue\n\n0.21329101843394052\n\n\n\nclustered_qr = qs.QStatistic(clustered, shape='rectangle', nx=3, ny=3)\nclustered_qr.plot()\n\n\n\n\n\n\n\n\n\nclustered_qr.chi2\n\n209.99999999999994\n\n\n\nclustered_qr.chi2_pvalue\n\n4.976940117448032e-41",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#quadrat-shape",
    "href": "week-14/quadrat.html#quadrat-shape",
    "title": "Quadrat Statistics",
    "section": "Quadrat Shape",
    "text": "Quadrat Shape\n\ncsr_qr_hex = qs.QStatistic(csr, shape='hexagon', lh=0.2)\n\n\ncsr_qr_hex.plot()\n\n\n\n\n\n\n\n\n\ncsr_qr_hex.chi2\n\n20.733333333333334\n\n\n\ncsr_qr_hex.df\n\n13\n\n\n\ncsr_qr_hex.chi2_pvalue\n\n0.07837318677184964\n\n\n\nclustered_qr_hex = qs.QStatistic(clustered, shape='hexagon', lh=0.2)\n\n\nclustered_qr_hex.plot()\n\n\n\n\n\n\n\n\n\nclustered_qr_hex.chi2_pvalue\n\n2.005280432194402e-14",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#simulation-inference",
    "href": "week-14/quadrat.html#simulation-inference",
    "title": "Quadrat Statistics",
    "section": "Simulation Inference",
    "text": "Simulation Inference\n\nfrom pointpats import PoissonPointProcess as poissonpp\n\n\nclustered_sim = poissonpp(clustered.window, clustered.n, 999, asPP=True)\n\n\nclustered_sim_hex = qs.QStatistic(clustered, shape='hexagon',\n                                  lh=0.2,\n                                  realizations=clustered_sim)\n\n\nclustered_sim_hex.chi2_r_pvalue\n\n0.001\n\n\n\nclustered_sim_hex.chi2_realizations\n\narray([26.8       , 18.86666667, 20.26666667, 19.8       , 15.6       ,\n       30.53333333, 15.6       , 28.2       , 23.53333333, 17.        ,\n       35.66666667, 11.4       , 19.33333333, 21.66666667, 18.4       ,\n       15.6       , 17.46666667, 24.93333333, 17.        , 20.73333333,\n       19.33333333, 15.6       , 38.        , 38.        , 16.06666667,\n       24.46666667, 21.2       , 21.2       ,  7.66666667, 18.86666667,\n       17.46666667, 24.93333333, 15.6       , 25.4       , 14.66666667,\n       26.8       , 21.2       , 18.86666667, 14.66666667, 24.        ,\n       11.86666667, 10.        , 17.        , 17.        ,  9.06666667,\n       31.93333333, 24.93333333, 31.        , 21.2       , 17.93333333,\n       11.4       , 17.        , 21.2       , 11.86666667, 21.2       ,\n       19.33333333, 19.8       , 17.        , 38.        , 31.        ,\n       34.73333333, 16.53333333, 33.8       , 17.93333333, 18.86666667,\n       10.        ,  9.53333333, 15.6       , 25.4       , 14.66666667,\n       24.46666667, 24.93333333, 18.4       , 14.2       , 24.        ,\n       21.2       , 23.06666667, 21.2       , 19.33333333, 37.06666667,\n       15.6       , 29.6       , 31.        , 20.26666667, 23.06666667,\n       20.73333333, 32.86666667, 19.33333333, 22.6       , 15.6       ,\n       13.26666667, 46.4       , 15.6       , 20.26666667, 33.33333333,\n       11.4       , 17.        , 26.33333333, 17.93333333, 22.13333333,\n       17.        , 33.33333333, 24.93333333, 12.33333333, 28.66666667,\n       42.2       , 17.        , 22.6       , 16.53333333, 22.6       ,\n       12.33333333, 15.6       , 18.4       , 17.        , 17.46666667,\n       30.06666667, 26.8       , 17.        , 24.46666667, 21.66666667,\n       21.66666667, 20.26666667, 17.93333333, 18.4       , 28.2       ,\n       29.13333333, 14.2       , 26.33333333,  9.06666667, 20.26666667,\n       21.66666667, 27.26666667, 22.6       , 22.13333333, 21.66666667,\n       21.2       , 10.93333333, 21.66666667, 22.13333333, 33.33333333,\n       11.4       , 15.13333333, 22.6       , 17.93333333, 18.86666667,\n       22.13333333, 17.        , 10.93333333, 16.53333333, 14.2       ,\n       19.33333333, 14.66666667, 16.53333333, 16.06666667, 15.13333333,\n       36.6       , 10.        , 14.66666667, 17.46666667, 12.33333333,\n       45.        , 26.33333333, 16.06666667, 21.2       , 32.4       ,\n       22.13333333, 23.53333333, 37.06666667, 22.13333333, 28.2       ,\n       27.73333333, 12.8       , 20.73333333, 18.4       , 16.06666667,\n       24.93333333, 11.4       , 20.73333333, 21.2       , 17.        ,\n       24.        , 13.73333333, 37.53333333, 18.86666667, 15.13333333,\n       15.13333333, 27.73333333, 16.06666667, 32.4       , 20.73333333,\n       33.8       , 24.46666667, 10.46666667, 27.73333333, 18.86666667,\n       15.13333333, 21.2       , 25.86666667, 15.6       , 25.4       ,\n       11.4       , 21.2       , 27.73333333, 20.73333333, 23.53333333,\n       30.06666667, 17.        , 17.46666667, 19.8       , 19.33333333,\n       22.6       , 13.26666667, 32.4       , 29.6       , 37.53333333,\n       21.2       , 20.26666667, 17.        , 23.06666667, 26.33333333,\n       26.33333333, 17.        , 39.86666667, 16.06666667, 24.93333333,\n       20.26666667, 13.26666667, 20.73333333, 28.66666667, 23.53333333,\n       19.33333333, 19.8       , 48.26666667, 19.8       , 16.06666667,\n       26.33333333, 20.26666667, 10.46666667, 15.13333333, 16.06666667,\n       23.53333333, 26.33333333, 15.6       , 23.53333333, 19.33333333,\n        6.26666667, 27.26666667, 27.73333333, 17.        , 32.4       ,\n       22.6       , 13.73333333, 28.66666667, 20.73333333, 17.46666667,\n       20.73333333, 31.        , 22.6       , 19.33333333, 23.53333333,\n       24.        , 18.4       , 16.06666667, 21.66666667, 23.53333333,\n       12.8       , 31.        , 14.2       , 16.53333333,  8.6       ,\n       14.66666667, 27.26666667, 17.46666667, 20.73333333, 31.93333333,\n       28.2       , 24.        , 27.73333333, 24.46666667, 30.53333333,\n       17.46666667, 15.13333333, 26.8       , 36.6       , 29.6       ,\n       32.86666667, 11.86666667, 15.13333333, 14.2       , 19.8       ,\n       24.46666667, 31.93333333, 35.66666667, 18.86666667, 13.73333333,\n       14.66666667, 14.2       , 23.06666667, 21.2       , 16.53333333,\n       31.93333333, 18.4       , 10.93333333, 25.4       , 21.66666667,\n       21.66666667, 20.73333333, 29.6       , 26.33333333, 33.33333333,\n       18.86666667, 13.73333333, 23.06666667, 20.26666667, 14.2       ,\n       27.73333333, 36.6       , 16.06666667, 18.86666667, 14.2       ,\n       25.4       , 27.26666667, 21.2       , 18.86666667, 24.        ,\n       28.66666667, 30.06666667, 19.8       , 24.        , 17.46666667,\n       24.        , 14.2       , 18.86666667, 24.        , 22.13333333,\n       23.53333333, 30.53333333, 37.06666667, 22.6       , 21.66666667,\n       23.53333333, 19.8       , 20.73333333, 19.8       , 21.2       ,\n       25.86666667, 27.73333333, 14.2       , 30.06666667, 17.        ,\n       17.        , 23.06666667, 30.06666667, 29.6       , 10.93333333,\n       35.2       , 30.53333333, 17.        , 17.46666667, 41.26666667,\n       21.66666667, 15.13333333, 24.46666667, 28.2       , 31.        ,\n       17.46666667, 29.13333333, 19.8       , 17.93333333, 15.13333333,\n       17.46666667, 25.4       , 17.93333333, 30.06666667, 16.06666667,\n       27.73333333, 14.2       , 15.13333333, 20.26666667, 36.13333333,\n       24.46666667, 25.4       , 25.4       , 36.13333333, 15.6       ,\n       20.73333333, 23.53333333, 14.2       , 25.86666667, 18.86666667,\n       25.4       , 18.4       , 23.53333333, 14.66666667, 17.        ,\n       22.13333333, 23.53333333, 18.4       , 20.26666667, 28.2       ,\n       20.26666667, 40.33333333, 28.2       , 37.06666667, 17.46666667,\n       31.        , 20.26666667, 26.8       , 35.2       , 14.66666667,\n       23.06666667, 23.06666667, 13.73333333, 35.2       , 23.53333333,\n       31.        , 31.93333333, 17.46666667, 38.        , 16.53333333,\n       31.46666667, 34.26666667, 22.13333333, 22.6       , 20.73333333,\n       16.53333333, 17.93333333, 29.6       , 10.46666667, 27.26666667,\n       18.86666667, 22.6       , 36.6       , 25.4       , 15.6       ,\n       15.13333333, 18.86666667, 19.8       , 29.6       , 22.6       ,\n       13.73333333, 46.86666667, 35.2       , 14.2       , 25.4       ,\n       41.73333333, 25.4       , 13.73333333, 23.53333333, 26.8       ,\n       25.4       , 16.06666667, 21.66666667, 37.53333333, 30.06666667,\n        7.2       , 15.6       , 31.46666667, 17.        , 35.66666667,\n       23.53333333, 19.33333333, 17.46666667, 24.93333333, 19.8       ,\n       13.26666667, 25.4       , 17.46666667, 24.46666667, 20.73333333,\n       33.33333333, 20.26666667, 26.33333333, 16.06666667, 21.2       ,\n       21.2       , 56.2       , 27.26666667, 26.8       , 21.66666667,\n       22.13333333,  7.66666667, 22.6       ,  8.6       , 17.93333333,\n       25.86666667, 30.53333333, 28.66666667, 24.93333333, 13.73333333,\n        9.53333333, 21.2       , 17.93333333, 32.4       , 24.        ,\n       24.46666667, 34.73333333, 31.46666667, 23.06666667, 17.93333333,\n       30.53333333, 15.13333333, 22.13333333, 19.8       , 17.        ,\n       25.4       , 18.86666667, 26.33333333, 19.33333333, 32.4       ,\n        9.53333333, 21.66666667, 11.86666667, 31.46666667, 30.06666667,\n       24.93333333, 24.46666667, 17.46666667, 21.66666667, 19.33333333,\n       23.06666667, 23.53333333, 23.53333333, 33.33333333, 22.6       ,\n       18.4       , 24.        , 23.53333333, 16.53333333, 19.8       ,\n       17.        , 24.93333333, 10.93333333, 30.53333333, 13.73333333,\n       16.06666667, 19.33333333, 17.93333333, 26.8       , 21.2       ,\n       19.8       , 40.33333333, 18.86666667, 10.46666667, 23.53333333,\n       16.53333333, 24.        , 21.2       , 19.33333333, 31.46666667,\n       12.33333333, 10.93333333, 16.53333333, 25.4       , 33.33333333,\n       33.33333333, 20.73333333, 12.8       , 20.26666667, 34.26666667,\n       33.33333333, 29.13333333, 18.4       , 16.53333333, 20.73333333,\n       22.13333333, 22.6       , 22.6       , 17.93333333, 10.        ,\n       14.66666667, 14.66666667, 16.06666667, 16.53333333, 21.2       ,\n       10.93333333, 17.46666667, 31.46666667, 14.2       , 18.86666667,\n       24.        , 10.46666667, 30.06666667, 25.4       , 25.4       ,\n       15.13333333, 24.93333333, 22.6       , 17.93333333, 12.8       ,\n       20.73333333, 10.46666667, 12.8       , 32.86666667, 23.53333333,\n       21.2       , 23.06666667, 17.        , 13.73333333, 16.53333333,\n       24.46666667, 14.2       , 38.46666667, 26.33333333, 15.6       ,\n       20.26666667, 31.93333333, 21.66666667, 18.86666667, 15.13333333,\n       20.26666667, 31.46666667, 25.4       , 17.        , 26.8       ,\n       24.46666667, 14.2       , 42.2       , 22.6       , 17.46666667,\n       20.26666667, 35.66666667, 26.8       , 24.93333333, 31.46666667,\n       16.53333333, 24.        , 23.06666667, 17.93333333, 31.46666667,\n       30.06666667, 20.26666667, 11.2       , 23.06666667, 38.93333333,\n       13.73333333, 17.93333333, 24.        , 25.4       , 25.86666667,\n       24.        , 18.86666667, 22.13333333, 21.2       , 11.86666667,\n       18.86666667, 14.2       , 22.6       , 32.4       , 18.4       ,\n       24.93333333, 18.86666667, 24.93333333, 10.46666667, 24.93333333,\n       23.06666667, 15.6       , 21.66666667, 23.53333333, 20.73333333,\n       24.93333333, 25.86666667, 12.8       , 20.73333333, 25.4       ,\n       19.8       , 34.73333333, 30.53333333, 33.8       , 17.        ,\n       25.4       , 28.66666667, 18.4       , 30.06666667, 19.8       ,\n       52.93333333, 19.33333333, 25.86666667, 19.33333333, 10.93333333,\n       18.4       , 28.66666667,  7.66666667, 20.73333333, 43.13333333,\n       22.6       , 24.46666667, 23.53333333, 25.4       , 12.33333333,\n       26.8       , 22.6       , 22.13333333, 26.33333333, 31.        ,\n       29.13333333, 18.86666667, 20.26666667, 22.13333333, 16.53333333,\n       22.6       , 12.8       , 37.06666667, 23.06666667, 31.93333333,\n       10.        , 16.53333333, 14.2       , 26.8       , 18.4       ,\n       16.06666667, 22.6       , 13.26666667, 19.33333333, 21.66666667,\n       15.6       , 16.06666667, 19.8       , 26.33333333, 21.66666667,\n       25.86666667, 23.06666667, 16.53333333,  8.6       , 14.66666667,\n       23.06666667, 15.13333333, 15.6       , 14.66666667, 29.6       ,\n       13.73333333, 19.8       , 17.        , 19.8       , 39.86666667,\n       31.        , 12.8       , 24.93333333, 31.        , 23.06666667,\n       21.66666667, 21.66666667, 17.46666667, 20.73333333, 19.8       ,\n       25.4       , 21.2       , 19.8       , 11.86666667, 31.        ,\n       19.8       , 19.33333333, 13.26666667, 33.8       , 18.86666667,\n       46.4       , 24.        , 16.53333333, 16.53333333, 31.46666667,\n       23.53333333, 26.33333333, 25.86666667, 31.93333333, 21.2       ,\n       21.66666667, 20.73333333, 29.6       , 24.        , 38.46666667,\n        7.66666667, 17.        , 13.73333333,  6.26666667, 20.26666667,\n       36.13333333, 21.66666667, 23.53333333, 25.86666667, 18.4       ,\n       14.2       , 15.6       , 20.73333333, 34.73333333, 26.8       ,\n       21.66666667, 24.93333333, 29.13333333, 31.93333333, 25.86666667,\n       14.2       , 12.33333333, 17.46666667, 24.        , 17.        ,\n       27.26666667, 17.46666667, 17.        , 34.26666667, 12.8       ,\n       29.6       , 24.93333333, 15.13333333, 13.73333333, 34.26666667,\n       18.86666667, 30.06666667, 19.8       , 37.53333333, 26.8       ,\n       19.33333333, 17.93333333, 22.6       , 16.53333333, 12.33333333,\n       20.73333333, 17.46666667, 32.4       , 10.46666667, 33.8       ,\n       21.66666667, 18.86666667, 26.8       , 32.86666667, 41.26666667,\n       27.73333333, 16.06666667, 24.46666667, 15.6       , 19.33333333,\n       23.53333333, 19.33333333, 12.8       , 21.66666667, 12.8       ,\n       24.46666667, 36.13333333, 17.        , 18.86666667, 16.06666667,\n       17.93333333, 33.33333333, 10.93333333, 14.66666667, 11.4       ,\n       18.4       , 31.46666667, 24.46666667, 20.73333333, 40.8       ,\n       16.06666667, 20.73333333, 15.6       , 26.8       , 14.66666667,\n       31.46666667, 20.26666667, 24.46666667, 11.86666667, 17.46666667,\n       24.93333333,  6.73333333, 17.93333333, 24.46666667, 18.4       ,\n       14.2       , 26.8       , 24.        , 40.33333333, 16.53333333,\n       47.33333333, 23.53333333, 20.73333333, 14.2       , 23.53333333,\n       18.86666667, 13.73333333, 31.        , 21.66666667, 22.6       ,\n       11.4       , 24.46666667, 32.86666667, 43.13333333, 22.6       ,\n       19.8       , 36.6       , 28.2       , 23.06666667, 29.13333333,\n       15.13333333, 17.        , 28.2       , 34.26666667, 25.86666667,\n       13.26666667, 32.4       ,  8.6       , 29.13333333, 26.33333333,\n       28.66666667, 15.6       , 19.8       , 17.        , 12.8       ,\n       23.06666667, 16.06666667, 30.06666667, 20.26666667, 19.8       ,\n       22.6       , 21.66666667, 25.86666667, 36.6       , 15.6       ,\n       24.46666667, 15.13333333, 19.33333333, 10.46666667, 26.33333333,\n       18.86666667, 23.06666667, 24.46666667, 22.13333333, 29.6       ,\n       17.46666667, 23.06666667, 16.        , 23.06666667, 28.66666667,\n       20.26666667, 17.93333333, 17.        , 26.33333333, 21.66666667,\n       13.73333333, 17.93333333, 16.06666667, 21.66666667, 26.8       ,\n       18.86666667, 24.93333333, 21.66666667, 34.26666667, 23.06666667,\n       23.06666667, 12.8       , 15.6       , 13.73333333, 35.2       ,\n       26.8       , 25.4       , 19.33333333, 25.4       , 22.6       ,\n       18.86666667, 23.06666667, 29.6       , 13.73333333, 13.26666667,\n       30.06666667, 31.93333333, 12.8       , 33.8       , 11.4       ,\n       15.13333333, 15.6       , 36.13333333, 16.06666667, 13.73333333,\n       42.2       , 14.2       , 30.06666667, 24.46666667, 45.93333333,\n       29.6       , 17.93333333, 29.13333333, 29.6       , 38.46666667,\n       17.93333333, 19.33333333, 12.8       , 22.13333333, 17.93333333,\n       19.33333333, 14.2       , 24.46666667, 14.66666667, 21.2       ,\n       29.6       , 18.4       , 20.73333333, 20.26666667, 22.6       ,\n       17.        , 33.33333333, 29.6       , 31.46666667])",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-14/quadrat.html#influence-of-bounding-extent",
    "href": "week-14/quadrat.html#influence-of-bounding-extent",
    "title": "Quadrat Statistics",
    "section": "Influence of Bounding Extent",
    "text": "Influence of Bounding Extent\n\nimport libpysal\nfrom libpysal.cg import shapely_ext\nimport geopandas\nimport numpy as np\n\n\nfrom pointpats import Window\n\n\nva = libpysal.io.open(libpysal.examples.get_path(\"vautm17n.shp\"))\n\n\npolys = [shp for shp in va]\nstate = shapely_ext.cascaded_union(polys)\n\n\nwindow = Window(state.parts)\n\n\nnp.random.seed(5)\n\n\nsamples = poissonpp(window, 200, 1, conditioning=False, asPP=False)\n\n\nsamples.realizations[0]\n\narray([[ 414659.62831296, 4098843.62306827],\n       [ 944877.32665295, 4064524.51881247],\n       [ 624464.14012849, 4135522.50140932],\n       [ 716322.44071544, 4158399.44765818],\n       [ 831143.93926953, 4237580.13537067],\n       [ 400509.95328193, 4096900.41002516],\n       [ 810686.07255281, 4283013.52195582],\n       [ 589382.5482221 , 4207114.63468694],\n       [ 464834.86653763, 4099680.23066161],\n       [ 729016.86342122, 4295967.88710233],\n       [ 692558.78034025, 4104007.16031022],\n       [ 707522.86530594, 4083889.38215422],\n       [ 368068.7091872 , 4091422.04350303],\n       [ 781939.22465452, 4097373.59625561],\n       [ 690938.87384332, 4295945.27613892],\n       [ 737215.08721821, 4168650.77104844],\n       [ 610370.4068341 , 4121670.2849925 ],\n       [ 557829.48604931, 4085935.58930553],\n       [ 866763.76550655, 4111032.57384731],\n       [ 834827.89687458, 4166256.04344958],\n       [ 621872.14784085, 4116134.29168132],\n       [ 731234.98798312, 4131270.48035054],\n       [ 682313.43606906, 4223967.01711812],\n       [ 733908.93745755, 4275494.4664081 ],\n       [ 529604.21979282, 4140192.20505708],\n       [ 667623.24339624, 4231046.01435233],\n       [ 852876.94714772, 4144452.31221088],\n       [ 736531.73670527, 4298186.25041488],\n       [ 628191.02243473, 4102712.26100424],\n       [ 695286.19214258, 4089811.49187367],\n       [ 335791.24699685, 4071456.16300708],\n       [ 976525.54090453, 4190170.9356442 ],\n       [ 269907.7423385 , 4053819.55125621],\n       [ 901005.55219154, 4135790.38279535],\n       [ 343450.39084153, 4069350.54999132],\n       [ 616962.39647439, 4060407.85366255],\n       [ 856890.64094909, 4088171.88687401],\n       [ 784246.96302866, 4158561.00287435],\n       [ 506667.94859104, 4057344.67213736],\n       [ 652994.7682644 , 4222285.369847  ],\n       [ 752892.56374042, 4051208.83003112],\n       [ 925790.59890195, 4067279.1569195 ],\n       [ 979545.31057954, 4216138.61394589],\n       [ 834141.62958798, 4078931.40609538],\n       [ 826072.78416898, 4141338.71410948],\n       [ 789540.24165872, 4134439.70394346],\n       [ 832204.18186126, 4197953.71447929],\n       [ 425655.57604877, 4089467.13633862],\n       [ 864217.85281509, 4188693.47802216],\n       [ 862328.44401525, 4076592.74052587],\n       [ 867441.18732928, 4116652.71820767],\n       [ 699661.2534777 , 4307801.55307259],\n       [ 751829.65031639, 4319968.92400562],\n       [ 693363.69978698, 4164890.1129782 ],\n       [ 746506.9340749 , 4128752.07645258],\n       [ 732083.67983923, 4248326.90577783],\n       [ 653338.32075661, 4156598.67265619],\n       [ 976606.02276802, 4197597.40929845],\n       [ 848350.52448247, 4064866.02614159],\n       [ 630634.31029898, 4214285.994748  ],\n       [ 710542.99205501, 4142311.94640686],\n       [ 818235.01391031, 4101202.27810456],\n       [ 823663.1391133 , 4155899.63963564],\n       [ 664444.47776103, 4236228.98363075],\n       [ 360769.23263894, 4085354.33832084],\n       [ 766550.94064044, 4292018.87615081],\n       [ 708574.49400057, 4316489.95362822],\n       [ 678464.74663622, 4157026.01869247],\n       [ 669386.8193962 , 4168943.73102981],\n       [ 739117.58013062, 4238904.58741115],\n       [ 802008.45139164, 4058821.42801494],\n       [ 719353.5399807 , 4310434.15086711],\n       [ 699010.43647616, 4235885.22615002],\n       [ 711302.81748723, 4189425.14715171],\n       [ 711004.15283455, 4307013.13265628],\n       [ 443641.20854179, 4112502.92242971],\n       [ 549017.70273327, 4122109.33477415],\n       [ 633880.98332554, 4180721.53522785],\n       [ 761505.3618409 , 4337608.44051833],\n       [ 628113.51741034, 4248999.17523924],\n       [ 612085.42234894, 4193531.31241006],\n       [ 773003.9526705 , 4191729.03945972],\n       [ 869260.59923287, 4095817.64598321],\n       [ 429695.78427436, 4083168.63837101],\n       [ 411407.60297276, 4095291.64838574],\n       [ 774204.90698235, 4114220.83806725],\n       [ 592778.43642512, 4048923.19755613],\n       [ 384376.48425827, 4088331.25454645],\n       [ 947440.38984378, 4151004.37886308],\n       [ 767483.69263252, 4178399.29510034],\n       [ 690935.63144369, 4235036.15530389],\n       [ 576980.01519074, 4070580.86201211],\n       [ 712493.43738419, 4089094.16426798],\n       [ 519079.03725716, 4062389.09073365],\n       [ 594894.13977775, 4126627.3577412 ],\n       [ 802962.14790058, 4255158.21686279],\n       [ 641802.18696761, 4198983.48922481],\n       [ 726954.63054571, 4344401.63706861],\n       [ 821460.13878318, 4132750.73296464],\n       [ 837220.98889414, 4222136.55345996],\n       [ 563543.98825404, 4153613.9629347 ],\n       [ 814038.74000601, 4093096.42208192],\n       [ 697273.58284353, 4163467.0010079 ],\n       [ 629072.6884094 , 4213295.94973238],\n       [ 819817.84709794, 4102394.60580934],\n       [ 526160.27728639, 4104037.23601285],\n       [ 836763.95559331, 4129516.28447547],\n       [ 736374.59850277, 4352644.80670343],\n       [ 841168.99204432, 4138747.2216095 ],\n       [ 751403.55221945, 4199922.61169574],\n       [ 557689.70136621, 4058695.77127365],\n       [ 878737.13093459, 4069134.69802166],\n       [ 645585.19035458, 4215659.45070671],\n       [ 848301.94947934, 4240704.72062012],\n       [ 811185.54323789, 4246760.16720487],\n       [ 802385.39914521, 4152499.83940512],\n       [ 967083.70536616, 4187628.68689018],\n       [ 566173.41485163, 4171863.51737594],\n       [ 695607.47904146, 4288180.87390924],\n       [ 677616.25847172, 4050830.91542181],\n       [ 861869.68527996, 4210372.43779312],\n       [ 654780.64776662, 4111656.98283347],\n       [ 585233.58031201, 4074992.10261737],\n       [ 781434.81304468, 4058413.05158421],\n       [ 822682.84769376, 4213417.77129642],\n       [ 768900.63289125, 4088226.44858065],\n       [ 791899.72825801, 4245563.76587488],\n       [ 831576.99118498, 4312162.65433444],\n       [ 878388.72674408, 4062578.7934681 ],\n       [ 823191.58958438, 4196021.45946538],\n       [ 378156.77607172, 4068466.52709272],\n       [ 862070.43546301, 4193252.59919724],\n       [ 817515.77267352, 4251166.78629974],\n       [ 690730.22340288, 4134039.36745731],\n       [ 582428.37056692, 4142295.17500245],\n       [ 701803.27707921, 4257761.15121192],\n       [ 794450.97333767, 4082458.53574423],\n       [ 683956.9175291 , 4263206.01583531],\n       [ 810304.08829204, 4189035.31606198],\n       [ 626831.94796862, 4183736.79813135],\n       [ 447925.51227984, 4053619.31963374],\n       [ 720176.25306412, 4080228.49511975],\n       [ 671706.30020031, 4057219.61544144],\n       [ 737091.10397116, 4186517.38432557],\n       [ 621943.7767598 , 4230899.1177717 ],\n       [ 781727.00521532, 4188771.47311895],\n       [ 533270.55041969, 4101228.87129633],\n       [ 422275.11061901, 4061412.90318906],\n       [ 623274.94414034, 4137159.04830485],\n       [ 613396.83788472, 4098031.58804004],\n       [ 918485.80482483, 4066117.04895843],\n       [ 835277.63162857, 4216987.74937433],\n       [ 840905.06186863, 4119589.82676569],\n       [ 776734.80434232, 4130379.65381659],\n       [ 822926.36295514, 4212704.28173775],\n       [ 577183.29908505, 4173970.73377266],\n       [ 844670.36825708, 4169964.16925377],\n       [ 726945.33864164, 4247843.41173161],\n       [ 522700.79684074, 4133457.24989219],\n       [ 781400.95255701, 4075836.20334854],\n       [ 807851.53178381, 4155393.2584169 ],\n       [ 528527.53915051, 4128711.54607999],\n       [ 733715.04344501, 4324209.51951477],\n       [ 761959.58353421, 4278337.10568371],\n       [ 462416.02583551, 4055426.72279291],\n       [ 783784.3588985 , 4219065.00137999],\n       [ 656981.17357128, 4081038.36317257],\n       [ 752519.5579292 , 4122287.87050136],\n       [ 785998.97275375, 4180386.72063339],\n       [ 515218.38308205, 4058619.31300199],\n       [ 665460.74168329, 4220305.65172169],\n       [ 635096.17082896, 4088479.03653079],\n       [ 525676.09274115, 4094679.99674041],\n       [ 652994.1969737 , 4196544.93157394],\n       [ 684221.48776759, 4279076.50535586],\n       [ 841203.32727666, 4091321.7151704 ],\n       [ 658966.29762168, 4152432.0546778 ],\n       [ 746057.91004075, 4159150.20681748],\n       [ 713023.65348106, 4313845.48665105],\n       [ 389367.77238998, 4131872.44048721],\n       [ 964384.90536381, 4177830.49754845],\n       [ 861956.74790132, 4163047.64826283],\n       [ 652335.83396067, 4066521.74757694],\n       [ 970927.39251655, 4191911.74445852],\n       [ 723112.44351288, 4066723.92870727],\n       [ 617299.64641549, 4215686.04441991],\n       [ 733460.51785091, 4209610.5745427 ],\n       [ 774366.73041553, 4083740.25237318],\n       [ 773520.40251126, 4336421.17625672],\n       [ 410898.72636993, 4079437.47953348],\n       [ 673646.58115341, 4145523.97371361],\n       [ 817877.62652755, 4319656.3622249 ],\n       [ 592810.59894755, 4136791.36885295],\n       [ 552979.14353052, 4059233.18378104],\n       [ 812842.56890396, 4121182.53606018],\n       [ 767558.36215433, 4227759.98274924],\n       [ 844106.62271791, 4069246.81629651],\n       [ 801080.39465864, 4189486.98256327],\n       [ 781265.07995516, 4324246.416531  ],\n       [ 628867.42811353, 4070282.90463193]])\n\n\n\nfrom pointpats import PointPattern\npp_csr = PointPattern(samples.realizations[0])\n\n\npp_csr.plot()\n\n\n\n\n\n\n\n\n\npp_csr.plot(window=True, hull=True, title='Random Point Pattern')\n\n\n\n\n\n\n\n\n\ncsr_qr = qs.QStatistic(pp_csr, shape='rectangle', nx=3, ny=3)\ncsr_qr.plot()\n\n\n\n\n\n\n\n\n\ncsr_qr.chi2_pvalue\n\n7.295335713511762e-19\n\n\nThe low p-value is an artifact of the shape of the window, since we will never see points in the top two cells in the first column. We did infact generate a CSR sample, so our test is incorrect here because of the shape of the window.\nTo correct for this, we can simulate other CSR samples in the same window and compare the value of our original statistic to this distribution to develop an alternative approach to inference.\n\nnp.random.seed(12345)\ncsr_samples = poissonpp(window, 200, 99, conditioning=False, asPP=True)\n\n\ncsr_samples.realizations[0]\n\n&lt;pointpats.pointpattern.PointPattern at 0x7f32030e84a0&gt;\n\n\n\nstats = []\nfor key in csr_samples.realizations:\n    realization = csr_samples.realizations[key]\n    test = qs.QStatistic(realization, shape='rectangle', nx=3, ny=3)\n    stats.append(test.chi2)\n\n\nstats = np.array(stats)\nnum = 1 + (stats &gt;= csr_qr.chi2).sum()\nden = 99 +1\np_value = num / den\np_value\n\n0.31\n\n\n\nimport seaborn as sns\n\n\nax =sns.distplot(stats)\nax.axvline(x = csr_qr.chi2, ymin=0, ymax=.25, color='r')\n\n\n\n\n\n\n\n\n\ncsr_qr.chi2\n\n103.75000000000001",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Quadrat Statistics"
    ]
  },
  {
    "objectID": "week-04/import_demo.html",
    "href": "week-04/import_demo.html",
    "title": "Importing",
    "section": "",
    "text": "import distances\n\n\np1 = [0, 0]\np2 = [5, 0]\np3 = [5, 5]\n\n\ndistances.euclidean(p1, p2)\n\n5.0\n\n\n\ndistances.manhattan(p1, p2)\n\n5\n\n\n\ndistances.distance_composite(p2, p3)\n\n5.0\n\n\n\nfrom distances import distance_composite\n\n\ndistance_composite(p1, p3)\n\n7.0710678118654755",
    "crumbs": [
      "Week 4 2/06, 2/08",
      "Importing"
    ]
  },
  {
    "objectID": "week-04/distances.html",
    "href": "week-04/distances.html",
    "title": "distances.py",
    "section": "",
    "text": "import numpy\n\ndef euclidean(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    return (dx**2 + dy**2) ** (1 / 2)\n\n\ndef manhattan(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    adx = numpy.abs(dx)\n    ady = numpy.abs(dy)\n    return adx + ady\n\n\ndef distance_composite(pnt1, pnt2, metric=\"euclidean\"):\n    if metric == \"euclidean\":\n        return euclidean(pnt1, pnt2)\n    else:\n        return manhattan(pnt1, pnt2)",
    "crumbs": [
      "Week 4 2/06, 2/08",
      "distances.py"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html",
    "href": "week-03/python_concepts.html",
    "title": "Python Programming Concepts",
    "section": "",
    "text": "first_list = [ 1, 2, 4]\n\n\ntype(first_list)\n\nlist\n\n\n\ncounties = [ 'San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\ntype(counties)\n\nlist\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[1]\n\n'Orange'\n\n\n\ncounties[2]\n\n'Imperial'\n\n\n\ncounties[3]\n\n'Riverside'\n\n\n\ncounties[-1]\n\n'Riverside'\n\n\n\ncounties[-2]\n\n'Imperial'\n\n\n\ncounties\n\n['San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\n\nprint(counties)\n\n['San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[-2:]\n\n['Imperial', 'Riverside']\n\n\n\ncounties[:2]\n\n['San Diego', 'Orange']\n\n\n\nlen(counties)\n\n4\n\n\n\ntype(counties)\n\nlist\n\n\n\ntype(counties[0])\n\nstr\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[0] = 'SD'\n\n\ncounties\n\n['SD', 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties[0] = 10\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties.append(5000)\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nnorth=['LA', 'Ventura']\nnorth\n\n['LA', 'Ventura']\n\n\n\nc = counties.copy()\n\n\nc\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nc.append(north)\n\n\nc\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, ['LA', 'Ventura']]\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\ncounties.extend(north)\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, 'LA', 'Ventura']\n\n\n\ncounties.count('Orange')\n\n1\n\n\n\ncounties.count('LAX')\n\n0\n\n\n\ncounties.index('Orange')\n\n1\n\n\n\ncounties.index('LAX')\n\nValueError: 'LAX' is not in list\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, 'LA', 'Ventura']\n\n\n\ncounties.reverse()\n\n\ncounties\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\ncounties.sort()\n\nTypeError: '&lt;' not supported between instances of 'int' and 'str'\n\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\nnorth.sort()\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\nnorth = [ 'ventura', 'la']\n\n\nnorth\n\n['ventura', 'la']\n\n\n\nnorth.sort()\n\n\nnorth\n\n['la', 'ventura']\n\n\n\n\n\ncounties\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\n[ element for element in counties]\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\n[ i for i in counties ]\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\ntype(counties[0])\n\nstr\n\n\n\n[ type(i) for i in counties ]\n\n[str, str, int, str, str, str, int]\n\n\n\n[ (i,type(i)) for i in counties ]\n\n[('Ventura', str),\n ('LA', str),\n (5000, int),\n ('Riverside', str),\n ('Imperial', str),\n ('Orange', str),\n (10, int)]\n\n\n\ncounties = ['San Diego', 'Orange', 'Los Angeles', 'Ventura']\n\n\ncounties\n\n['San Diego', 'Orange', 'Los Angeles', 'Ventura']\n\n\n\nsd = counties[0]\nsd\n\n'San Diego'\n\n\n\ntype(sd)\n\nstr\n\n\n\nsd.split()\n\n['San', 'Diego']\n\n\n\n[ i.split() for i in counties ]\n\n[['San', 'Diego'], ['Orange'], ['Los', 'Angeles'], ['Ventura']]",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html#lists",
    "href": "week-03/python_concepts.html#lists",
    "title": "Python Programming Concepts",
    "section": "",
    "text": "first_list = [ 1, 2, 4]\n\n\ntype(first_list)\n\nlist\n\n\n\ncounties = [ 'San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\ntype(counties)\n\nlist\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[1]\n\n'Orange'\n\n\n\ncounties[2]\n\n'Imperial'\n\n\n\ncounties[3]\n\n'Riverside'\n\n\n\ncounties[-1]\n\n'Riverside'\n\n\n\ncounties[-2]\n\n'Imperial'\n\n\n\ncounties\n\n['San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\n\nprint(counties)\n\n['San Diego', 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[-2:]\n\n['Imperial', 'Riverside']\n\n\n\ncounties[:2]\n\n['San Diego', 'Orange']\n\n\n\nlen(counties)\n\n4\n\n\n\ntype(counties)\n\nlist\n\n\n\ntype(counties[0])\n\nstr\n\n\n\ncounties[0]\n\n'San Diego'\n\n\n\ncounties[0] = 'SD'\n\n\ncounties\n\n['SD', 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties[0] = 10\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside']\n\n\n\ncounties.append(5000)\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nnorth=['LA', 'Ventura']\nnorth\n\n['LA', 'Ventura']\n\n\n\nc = counties.copy()\n\n\nc\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nc.append(north)\n\n\nc\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, ['LA', 'Ventura']]\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000]\n\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\ncounties.extend(north)\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, 'LA', 'Ventura']\n\n\n\ncounties.count('Orange')\n\n1\n\n\n\ncounties.count('LAX')\n\n0\n\n\n\ncounties.index('Orange')\n\n1\n\n\n\ncounties.index('LAX')\n\nValueError: 'LAX' is not in list\n\n\n\ncounties\n\n[10, 'Orange', 'Imperial', 'Riverside', 5000, 'LA', 'Ventura']\n\n\n\ncounties.reverse()\n\n\ncounties\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\ncounties.sort()\n\nTypeError: '&lt;' not supported between instances of 'int' and 'str'\n\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\nnorth.sort()\n\n\nnorth\n\n['LA', 'Ventura']\n\n\n\nnorth = [ 'ventura', 'la']\n\n\nnorth\n\n['ventura', 'la']\n\n\n\nnorth.sort()\n\n\nnorth\n\n['la', 'ventura']\n\n\n\n\n\ncounties\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\n[ element for element in counties]\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\n[ i for i in counties ]\n\n['Ventura', 'LA', 5000, 'Riverside', 'Imperial', 'Orange', 10]\n\n\n\ntype(counties[0])\n\nstr\n\n\n\n[ type(i) for i in counties ]\n\n[str, str, int, str, str, str, int]\n\n\n\n[ (i,type(i)) for i in counties ]\n\n[('Ventura', str),\n ('LA', str),\n (5000, int),\n ('Riverside', str),\n ('Imperial', str),\n ('Orange', str),\n (10, int)]\n\n\n\ncounties = ['San Diego', 'Orange', 'Los Angeles', 'Ventura']\n\n\ncounties\n\n['San Diego', 'Orange', 'Los Angeles', 'Ventura']\n\n\n\nsd = counties[0]\nsd\n\n'San Diego'\n\n\n\ntype(sd)\n\nstr\n\n\n\nsd.split()\n\n['San', 'Diego']\n\n\n\n[ i.split() for i in counties ]\n\n[['San', 'Diego'], ['Orange'], ['Los', 'Angeles'], ['Ventura']]",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html#f-strings",
    "href": "week-03/python_concepts.html#f-strings",
    "title": "Python Programming Concepts",
    "section": "f-strings",
    "text": "f-strings\n\nsd\n\n'San Diego'\n\n\n\nf\"The name of my favorite county is {sd}\"\n\n'The name of my favorite county is San Diego'\n\n\n\n[ f\"The county is {c}\" for c in counties]\n\n['The county is San Diego',\n 'The county is Orange',\n 'The county is Los Angeles',\n 'The county is Ventura']\n\n\n\n[ f\"The county is {c} and it is {len(c)} characters\" for c in counties]\n\n['The county is San Diego and it is 9 characters',\n 'The county is Orange and it is 6 characters',\n 'The county is Los Angeles and it is 11 characters',\n 'The county is Ventura and it is 7 characters']\n\n\n\npci = 18001.23\n\n\npci\n\n18001.23\n\n\n\nf\"The pci in this city is {pci}\"\n\n'The pci in this city is 18001.23'\n\n\n\nf\"The pci in this city is {pci:.0f}\"\n\n'The pci in this city is 18001'\n\n\n\nf\"The pci in this city is {pci:.1f}\"\n\n'The pci in this city is 18001.2'",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html#iteration",
    "href": "week-03/python_concepts.html#iteration",
    "title": "Python Programming Concepts",
    "section": "Iteration",
    "text": "Iteration\n\ncounties\n\n['San Diego', 'Orange', 'Los Angeles', 'Ventura']\n\n\n\nfor county in counties:\n    print(county)\n\nSan Diego\nOrange\nLos Angeles\nVentura\n\n\n\nc = 0\nfor county in counties:\n    print(c, county)\n    c += 1\n\n0 San Diego\n1 Orange\n2 Los Angeles\n3 Ventura\n\n\n\nfor c, county in enumerate(counties):\n    print(c, county)\n\n0 San Diego\n1 Orange\n2 Los Angeles\n3 Ventura\n\n\n\nfor c, county in enumerate(counties):\n    print(c, county)\n    for char in county:\n        print(char)\n\n0 San Diego\nS\na\nn\n \nD\ni\ne\ng\no\n1 Orange\nO\nr\na\nn\ng\ne\n2 Los Angeles\nL\no\ns\n \nA\nn\ng\ne\nl\ne\ns\n3 Ventura\nV\ne\nn\nt\nu\nr\na",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html#conditional-execution",
    "href": "week-03/python_concepts.html#conditional-execution",
    "title": "Python Programming Concepts",
    "section": "Conditional Execution",
    "text": "Conditional Execution\n\n10 &gt; 5\n\nTrue\n\n\n\nif 10&gt;5:\n    print('10 is greater than 5')\n\n10 is greater than 5\n\n\n\na = 5\nb = 10\nif a&gt;b:\n    print(f'{a} is greater than {b}')\n\n\na = 5\nb = 10\nif a&lt;b:\n    print(f'{a} is less than {b}')\n\n5 is less than 10\n\n\n\ncounties\n\n['San Diego', 'Orange', 'Los Angeles', 'Ventura']",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-03/python_concepts.html#use-a-for-loop-with-an-if-statement-to-print-out-the-counties-with-multiple-word-names",
    "href": "week-03/python_concepts.html#use-a-for-loop-with-an-if-statement-to-print-out-the-counties-with-multiple-word-names",
    "title": "Python Programming Concepts",
    "section": "use a for loop with an if statement to print out the counties with multiple word names",
    "text": "use a for loop with an if statement to print out the counties with multiple word names\n\nfor county in counties:\n    print(county.split())\n\n['San', 'Diego']\n['Orange']\n['Los', 'Angeles']\n['Ventura']\n\n\n\nfor county in counties:\n    if len(county.split()) == 2:\n        print(county)\n\nSan Diego\nLos Angeles\n\n\n\nfor county in counties:\n    if \" \" in county:\n        print(county)\n\nSan Diego\nLos Angeles",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Python Programming Concepts"
    ]
  },
  {
    "objectID": "week-06/geoprocessing.html",
    "href": "week-06/geoprocessing.html",
    "title": "Geoprocessing",
    "section": "",
    "text": "import geopandas as gpd\nimport pandas as pd\ngdf = gpd.read_parquet(\"~/data/scag_region.parquet\")\ngdf.shape\n\n(4580, 194)\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame\ngdf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\ngdf.columns.values\n\narray(['geoid', 'n_asian_under_15', 'n_black_under_15',\n       'n_hispanic_under_15', 'n_native_under_15', 'n_white_under_15',\n       'n_persons_under_18', 'n_asian_over_60', 'n_black_over_60',\n       'n_hispanic_over_60', 'n_native_over_60', 'n_persons_over_60',\n       'n_white_over_60', 'n_asian_over_65', 'n_black_over_65',\n       'n_hispanic_over_65', 'n_native_over_65', 'n_white_over_65',\n       'n_persons_over_75', 'n_persons_over_15', 'n_civilians_over_16',\n       'n_civilians_over_18', 'n_persons_over_25', 'n_age_5_older',\n       'n_asian_age_distribution', 'n_black_age_distribution',\n       'n_hispanic_age_distribution', 'n_native_age_distribution',\n       'n_white_age_distribution', 'n_asian_persons', 'n_black_persons',\n       'n_chinese_persons', 'n_labor_force', 'n_civilians_16_64',\n       'n_edu_college_greater', 'n_cuban_pop',\n       'n_poverty_determined_asian', 'n_poverty_determined_black',\n       'n_total_pop_sample', 'n_female_over_16',\n       'n_poverty_determined_families', 'n_poverty_determined_hispanic',\n       'n_disabled', 'n_housing_units_multiunit_structures_denom',\n       'n_poverty_determined_native', 'n_poverty_determined_persons',\n       'n_poverty_determined_white', 'n_employed_over_16',\n       'n_total_families', 'n_foreign_born_pop',\n       'n_female_headed_families', 'n_filipino_persons',\n       'n_female_labor_force', 'n_german_pop', 'n_german_born_pop',\n       'n_household_recent_move', 'n_structures_30_old',\n       'n_hawaiian_persons', 'n_total_households', 'n_asian_households',\n       'n_black_households', 'n_hispanic_households',\n       'n_white_households', 'median_household_income',\n       'median_income_asianhh', 'median_income_blackhh',\n       'median_income_hispanichh', 'median_income_whitehh',\n       'n_hispanic_persons', 'n_edu_hs_less', 'n_total_housing_units',\n       'per_capita_income', 'n_asian_indian_persons', 'n_irish_pop',\n       'n_irish_born_pop', 'n_italian_pop', 'n_italian_born_pop',\n       'n_japanese_persons', 'n_korean_persons', 'n_limited_english',\n       'n_employed_manufacturing', 'n_married', 'n_mexican_pop',\n       'median_home_value', 'median_contract_rent',\n       'n_housing_units_multiunit_structures', 'n_recent_immigrant_pop',\n       'n_poverty_over_65', 'n_poverty_asian', 'n_naturalized_pop',\n       'n_poverty_black', 'n_poverty_families_children',\n       'n_nonhisp_black_persons', 'n_poverty_hispanic',\n       'n_nonhisp_white_persons', 'n_poverty_native', 'n_poverty_persons',\n       'n_native_persons', 'n_poverty_white', 'n_occupied_housing_units',\n       'n_other_language', 'n_owner_occupied_housing_units',\n       'p_recent_immigrant_pop', 'p_household_recent_move',\n       'p_asian_under_15', 'p_black_under_15', 'p_hispanic_under_15',\n       'p_native_under_15', 'p_white_under_15', 'p_persons_under_18',\n       'p_structures_30_old', 'p_persons_over_60', 'p_asian_over_65',\n       'p_black_over_65', 'p_hispanic_over_65', 'p_native_over_65',\n       'p_poverty_rate_over_65', 'p_white_over_65', 'p_persons_over_75',\n       'p_poverty_rate_asian', 'p_asian_persons', 'p_poverty_rate_black',\n       'p_chinese_persons', 'p_edu_college_greater', 'p_cuban_pop',\n       'p_foreign_born_pop', 'p_female_headed_families',\n       'p_filipino_persons', 'p_female_labor_force',\n       'p_poverty_rate_children', 'p_german_pop', 'p_german_born_pop',\n       'p_hawaiian_persons', 'p_hispanic_persons',\n       'p_poverty_rate_hispanic', 'p_edu_hs_less',\n       'p_asian_indian_persons', 'p_irish_pop', 'p_irish_born_pop',\n       'p_italian_pop', 'p_italian_born_pop', 'p_japanese_persons',\n       'p_korean_persons', 'p_limited_english',\n       'p_employed_manufacturing', 'p_married', 'p_mexican_pop',\n       'p_housing_units_multiunit_structures', 'p_poverty_rate_native',\n       'p_naturalized_pop', 'p_nonhisp_black_persons', 'p_black_persons',\n       'p_native_persons', 'p_other_language', 'n_total_pop',\n       'p_owner_occupied_units', 'p_poverty_rate', 'p_puerto_rican_pop',\n       'p_employed_professional', 'n_puerto_rican_pop',\n       'n_employed_professional', 'p_russian_pop', 'p_russian_born_pop',\n       'p_scandanavian_pop', 'p_scandanavian_born_pop',\n       'p_employed_self_employed', 'p_unemployment_rate',\n       'p_vacant_housing_units', 'p_veterans', 'p_vietnamese_persons',\n       'p_widowed_divorced', 'p_poverty_rate_white',\n       'n_renter_occupied_housing_units', 'n_russian_pop',\n       'n_russian_born_pop', 'n_scandaniavian_pop',\n       'n_scandaniavian__born_pop', 'n_employed_self_employed',\n       'n_unemployed_persons', 'n_vacant_housing_units', 'n_veterans',\n       'n_vietnamese_persons', 'n_widowed_divorced', 'n_white_persons',\n       'year', 'n_total_housing_units_sample', 'p_nonhisp_white_persons',\n       'p_white_over_60', 'p_black_over_60', 'p_hispanic_over_60',\n       'p_native_over_60', 'p_asian_over_60', 'p_disabled', 'geometry'],\n      dtype=object)\ngdf.n_total_pop\n\n0       5497.0\n1       5659.0\n2       4486.0\n3       2924.0\n4       3415.0\n         ...  \n4575    3672.0\n4576    5257.0\n4577    6765.0\n4578    2981.0\n4579    3994.0\nName: n_total_pop, Length: 4580, dtype: float64\ngdf.geometry\n\n0       POLYGON ((-118.44870 34.16485, -118.43997 34.1...\n1       POLYGON ((-118.56229 34.22033, -118.55792 34.2...\n2       POLYGON ((-118.57976 34.21558, -118.57539 34.2...\n3       POLYGON ((-118.61472 34.21952, -118.61039 34.2...\n4       POLYGON ((-118.25416 33.93882, -118.25413 33.9...\n                              ...                        \n4575    POLYGON ((-118.50373 34.42607, -118.50050 34.4...\n4576    POLYGON ((-118.20731 33.90754, -118.20641 33.9...\n4577    POLYGON ((-119.22134 34.18130, -119.21727 34.1...\n4578    POLYGON ((-116.51068 33.80502, -116.51069 33.8...\n4579    POLYGON ((-118.41379 34.17940, -118.41160 34.1...\nName: geometry, Length: 4580, dtype: geometry\ngdf.plot()",
    "crumbs": [
      "Week 6 2/20, 2/22",
      "Geoprocessing"
    ]
  },
  {
    "objectID": "week-06/geoprocessing.html#projections",
    "href": "week-06/geoprocessing.html#projections",
    "title": "Geoprocessing",
    "section": "Projections",
    "text": "Projections\n\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf1 = gdf.to_crs(gdf.estimate_utm_crs())\n\n\ngdf1.crs\n\n&lt;Projected CRS: EPSG:32611&gt;\nName: WGS 84 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 120°W and 114°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Alberta; British Columbia (BC); Northwest Territories (NWT); Nunavut. Mexico. United States (USA).\n- bounds: (-120.0, 0.0, -114.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf1.plot()\n\n\n\n\n\n\n\n\n\ncounty = gdf1.geoid.str[0:5]\n\n\ncounty\n\n0       06037\n1       06037\n2       06037\n3       06037\n4       06037\n        ...  \n4575    06037\n4576    06037\n4577    06111\n4578    06065\n4579    06037\nName: geoid, Length: 4580, dtype: object\n\n\n\ngdf1['county'] = county\n\n\ngdf1.plot(column='county', categorical=True, legend=True)\n\n\n\n\n\n\n\n\n\nnames = {\"06025\": \"Imperial\",\n         \"06037\": \"Los Angeles\",\n         \"06059\": \"Orange\",\n         \"06065\": \"Riverside\",\n         \"06071\": \"San Bernardino\",\n         \"06073\": \"San Diego\",\n         \"06111\": \"Ventura\"}\n\n\ngdf1.county.map(names)\n\n0       Los Angeles\n1       Los Angeles\n2       Los Angeles\n3       Los Angeles\n4       Los Angeles\n           ...     \n4575    Los Angeles\n4576    Los Angeles\n4577        Ventura\n4578      Riverside\n4579    Los Angeles\nName: county, Length: 4580, dtype: object\n\n\n\ngdf1['county_name'] = gdf1.county.map(names)\n\n\ngdf1.plot(column='county_name', categorical=True, legend=True)",
    "crumbs": [
      "Week 6 2/20, 2/22",
      "Geoprocessing"
    ]
  },
  {
    "objectID": "week-06/geoprocessing.html#dissolve",
    "href": "week-06/geoprocessing.html#dissolve",
    "title": "Geoprocessing",
    "section": "Dissolve",
    "text": "Dissolve\n\ncounties = gdf1.dissolve(by='county')\n\n\ncounties.plot()\n\n\n\n\n\n\n\n\n\ncounties.shape\n\n(7, 195)\n\n\n\ngdf1.shape\n\n(4580, 196)\n\n\n\ncounty_centroids = counties.centroid\n\n\nbase = counties.plot()\ncounty_centroids.plot(color='r', ax=base)\n\n\n\n\n\n\n\n\n\ncounties.crs = gdf1.crs\n\n\ncounties.crs\n\n&lt;Projected CRS: EPSG:32611&gt;\nName: WGS 84 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 120°W and 114°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Alberta; British Columbia (BC); Northwest Territories (NWT); Nunavut. Mexico. United States (USA).\n- bounds: (-120.0, 0.0, -114.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ncounties.area\n\ncounty\n06025    1.160447e+10\n06037    1.061663e+10\n06059    2.068050e+09\n06065    1.890743e+10\n06071    5.204763e+10\n06073    1.102391e+10\n06111    4.811482e+09\ndtype: float64\n\n\n\nbuffer = county_centroids.buffer(16093.4) # 10 mile buffer\n\n\nbase = counties.plot()\nbuffer.plot(color='green', ax=base)\ncounty_centroids.plot(color='r', ax=base)\n\n\n\n\n\n\n\n\n\ngdf1.sindex.query(buffer, predicate='intersects')\n\narray([[   0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n           2,    2,    2,    2,    2,    2,    2,    3,    3,    3,    3,\n           3,    4,    5,    5,    5,    5,    5,    5,    5,    5,    5,\n           5,    5,    5,    6,    6,    6,    6,    6,    6,    6,    6,\n           6,    6,    6,    6,    6,    6,    6],\n       [1431, 1017, 1512,  679, 4303, 4341, 3220, 4445, 4338, 3280, 2904,\n        2078, 2864, 1167, 1389, 3288, 3870, 3286, 3285, 3494,   19, 3284,\n         734, 2040, 3483, 4349, 3763, 3728, 3475, 2039, 1053, 3013,  897,\n        4450, 3238, 4111, 1417, 1596, 3158,  137, 1300, 2388, 3159,  855,\n        4213,  138, 4486, 4561, 3271, 3871, 3459, 1388, 3287, 1407, 4256,\n        4094,  854, 3817,  853, 1341, 4488, 3598, 3491, 1055, 3177, 1765,\n        1767, 4274, 1683, 3429, 3476, 3263, 2100, 3260, 1160, 1076, 3245,\n        3234, 1073, 4125, 4207, 3915, 3235,  608, 4539,  540, 1257, 4221,\n        1025, 1721, 3252, 3427, 3485, 3282,  303, 3816, 1074, 1831, 4259,\n        2118, 4127, 3325, 3864,  283, 1780,  609, 2101, 1779, 1868, 1139,\n        4099,  152, 3903, 3264,  562, 3251, 4220, 1869, 1024, 3323,  153,\n        3267, 3913, 1834, 1836, 1833, 1832,  611, 1680, 3289, 3579, 3270,\n        1870,  563,  281, 2074, 3266, 1564, 3326, 2098, 2099, 3324,  607,\n         302,  606,  301, 2076, 1141,  305,  612, 1250, 1026, 4164, 2060,\n        1837, 3495, 1254, 4367, 4126, 3366,  605,  604, 2095, 3364, 3314,\n         286, 1265, 3262, 1023,  542, 3365, 3580, 4540, 4541, 3283,  285,\n        4366, 3279, 3278,  613,  307, 1027, 3914,  867, 1707, 2062, 3316,\n        3265, 2061, 1266, 1086, 2091, 3432, 2097, 3433, 3315, 3435, 4251,\n        2063, 4135, 3434, 3062, 3061, 3791, 1271, 3800, 2075,  308, 4267,\n        4046, 2064,  860, 3436, 2096, 1087, 3023,  615, 3438, 3115,  616,\n         309,  614, 3437, 3700,  311, 1177, 3912, 1781,  304, 1830, 4100,\n        1829, 3274, 3440,  310, 3439, 3802, 3030,  252, 2065, 3595, 2814,\n        2066, 3024, 4252, 3594, 2090, 4102, 4025, 1028, 4103,  868, 3441,\n        4165,  859, 3035,  861, 3321, 2089, 3233, 2092, 1968, 3272, 3815,\n        3430, 2073, 3034, 3862, 1867, 3249, 3250,  151, 1022, 3792, 3040,\n        1072, 2280, 1935, 3036,  863, 3039, 4104, 1172, 4105,  312, 4273,\n        3111, 3294, 1937,  864, 1178, 3701, 1936,  862, 3037, 3347, 3038,\n        1934, 3042, 1804, 3292,  655,  617, 2067,  872, 2172, 2171, 3442,\n        4043,  321, 3798, 4026, 3041, 1916, 1803, 4474, 3959, 1720,  437,\n        3976,  489, 1637,  963, 4128, 3600, 1757, 2487,  481, 2486, 3571,\n        3969, 1084, 3636, 2675, 4247, 2025,  924, 3715, 2023, 1571, 4337,\n        2022, 2379, 4248, 1374, 1572, 1931, 1209]])\n\n\n\nres = gdf1.sindex.query(buffer, predicate='intersects')\n\n\ngdf1.iloc[res[1]].plot()\n\n\n\n\n\n\n\n\n\nbase = gdf1.plot(alpha=0.5)\ngdf1.iloc[res[1]].plot(ax=base, color='yellow')\nbuffer.plot(ax=base, color='green')\n\n\n\n\n\n\n\n\n\ncounties.boundary\n\ncounty\n06025    LINESTRING (622670.817 3613246.064, 608496.116...\n06037    MULTILINESTRING ((334535.659 3765052.961, 3344...\n06059    LINESTRING (405551.296 3725825.754, 404548.958...\n06065    LINESTRING (450519.329 3730169.444, 450507.821...\n06071    LINESTRING (437464.649 3750033.907, 437589.410...\n06073    LINESTRING (476144.129 3623310.643, 476302.004...\n06111    MULTILINESTRING ((262292.345 3684931.770, 2623...\ndtype: geometry\n\n\n\ncounties.boundary.plot()\n\n\n\n\n\n\n\n\n\nbbuffer = counties.boundary.buffer(10000)\n\n\nbbuffer.plot()\n\n\n\n\n\n\n\n\n\nints = gdf1.sindex.query(bbuffer, predicate='intersects')\n\n\nints.shape\n\n(2, 3843)\n\n\n\ngdf1.iloc[ints[1]].plot()\n\n\n\n\n\n\n\n\n\ngdf1.iloc[~ints[1]].plot()\n\n\n\n\n\n\n\n\n\nbase = gdf1.plot()\ngdf1.iloc[ints[1]].plot(color='y', ax=base)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nboundary = np.array(['False'] * gdf1.shape[0])\nboundary[ints[1]]=True\ngdf1['Boundary'] = boundary\n\n\ngdf1.plot(column='Boundary', categorical=True, legend=True)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(1, figsize=(10, 6))\ngdf1.plot(column='Boundary', categorical=True, legend=True, ax=ax)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, figsize=(10, 6))\ngdf1.plot(column='Boundary', categorical=True, legend=True, ax=ax)\nax.axis('off');\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, figsize=(10, 6))\ngdf1.plot(column='Boundary', categorical=True, legend=True, ax=ax)\nax.set_title('Tracts within 6.2 miles of county border')\nax.axis('off');\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, figsize=(10, 6))\ngdf1.plot(column='Boundary', categorical=True, legend=True, ax=ax)\nax.set_title('Tracts within 6.2 miles of county border')\n\nax.annotate(\"Source: Author's calculation\", xy=gdf1.total_bounds[:2])\nax.axis('off');",
    "crumbs": [
      "Week 6 2/20, 2/22",
      "Geoprocessing"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#introduction",
    "href": "week-01/01-introduction.html#introduction",
    "title": "Course Introduction",
    "section": "Introduction",
    "text": "Introduction\n\nThis course introduces the fundamental concepts of spatial data analysis. Key fundamentals include spatial sampling, descriptive statistics for areal data, inferential statistics, use of maps in data analysis.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#approach",
    "href": "week-01/01-introduction.html#approach",
    "title": "Course Introduction",
    "section": "Approach",
    "text": "Approach\n\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#scope",
    "href": "week-01/01-introduction.html#scope",
    "title": "Course Introduction",
    "section": "Scope",
    "text": "Scope\n\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#prerequisites",
    "href": "week-01/01-introduction.html#prerequisites",
    "title": "Course Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#schedule-reading-and-content",
    "href": "week-01/01-introduction.html#schedule-reading-and-content",
    "title": "Course Introduction",
    "section": "Schedule, Reading, and Content",
    "text": "Schedule, Reading, and Content\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore coming into class prepared means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#readings",
    "href": "week-01/01-introduction.html#readings",
    "title": "Course Introduction",
    "section": "Readings",
    "text": "Readings\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#schedule-planned",
    "href": "week-01/01-introduction.html#schedule-planned",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#schedule-planned-1",
    "href": "week-01/01-introduction.html#schedule-planned-1",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#grading",
    "href": "week-01/01-introduction.html#grading",
    "title": "Course Introduction",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#specification-grading",
    "href": "week-01/01-introduction.html#specification-grading",
    "title": "Course Introduction",
    "section": "Specification Grading",
    "text": "Specification Grading\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes, and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#base-grade",
    "href": "week-01/01-introduction.html#base-grade",
    "title": "Course Introduction",
    "section": "Base Grade",
    "text": "Base Grade\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 12 of 14 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 10 of 14 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 8 of 14 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 6 of 14 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#final-grade",
    "href": "week-01/01-introduction.html#final-grade",
    "title": "Course Introduction",
    "section": "Final Grade",
    "text": "Final Grade\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#quizzes",
    "href": "week-01/01-introduction.html#quizzes",
    "title": "Course Introduction",
    "section": "Quizzes",
    "text": "Quizzes\nStarting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class. Quizzes are graded on a pass/fail basis.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#exercises",
    "href": "week-01/01-introduction.html#exercises",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#exercises-1",
    "href": "week-01/01-introduction.html#exercises-1",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#exercises-2",
    "href": "week-01/01-introduction.html#exercises-2",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#final-exam",
    "href": "week-01/01-introduction.html#final-exam",
    "title": "Course Introduction",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on May 7 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#tokens",
    "href": "week-01/01-introduction.html#tokens",
    "title": "Course Introduction",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#using-tokens",
    "href": "week-01/01-introduction.html#using-tokens",
    "title": "Course Introduction",
    "section": "Using Tokens",
    "text": "Using Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam. (Requests required by 2024-04-15 17:00.)",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#remaining-tokens",
    "href": "week-01/01-introduction.html#remaining-tokens",
    "title": "Course Introduction",
    "section": "Remaining Tokens",
    "text": "Remaining Tokens\nEach token that remains unused after 2024-05-02 will be counted as a passed quiz. Tokens cannot be exchanged with other students.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#administration",
    "href": "week-01/01-introduction.html#administration",
    "title": "Course Introduction",
    "section": "Administration",
    "text": "Administration",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#accommodations",
    "href": "week-01/01-introduction.html#accommodations",
    "title": "Course Introduction",
    "section": "Accommodations",
    "text": "Accommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#privacy-and-intellectual-property",
    "href": "week-01/01-introduction.html#privacy-and-intellectual-property",
    "title": "Course Introduction",
    "section": "Privacy and Intellectual Property",
    "text": "Privacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use [Canvas / Blackboard] to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#academic-integrity",
    "href": "week-01/01-introduction.html#academic-integrity",
    "title": "Course Introduction",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: http://www.sa.sdsu.edu/srr/index.html.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#code-of-conduct",
    "href": "week-01/01-introduction.html#code-of-conduct",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#code-of-conduct-1",
    "href": "week-01/01-introduction.html#code-of-conduct-1",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct.",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#computational-learning",
    "href": "week-01/01-introduction.html#computational-learning",
    "title": "Course Introduction",
    "section": "Computational Learning",
    "text": "Computational Learning\n\n\nShow me the code\nimport libpysal.examples\nimport geopandas \n\n# get path to built-in dataset for Mexico\npth = libpysal.examples.get_path(\"mexicojoin.shp\")\n# load the file with geopandas to create a GeoDataframe\ngdf = geopandas.read_file(pth)\n# call the plot method of the GeoDataFrame\ngdf.plot(edgecolor='white');",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#open-source",
    "href": "week-01/01-introduction.html#open-source",
    "title": "Course Introduction",
    "section": "Open Source",
    "text": "Open Source",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#server-or-laptop",
    "href": "week-01/01-introduction.html#server-or-laptop",
    "title": "Course Introduction",
    "section": "Server or Laptop",
    "text": "Server or Laptop\nYou can choose to either use an account on our course JupyterHub or install the packages on your own laptop.\nEither way, you will be using Jupyter Notebooks for all computation:",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#my-program",
    "href": "week-01/01-introduction.html#my-program",
    "title": "Course Introduction",
    "section": "My Program",
    "text": "My Program\n\n\n\nurl",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#why-am-i-here",
    "href": "week-01/01-introduction.html#why-am-i-here",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#why-am-i-here-1",
    "href": "week-01/01-introduction.html#why-am-i-here-1",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "href": "week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "title": "Course Introduction",
    "section": "‘Trump turned this place into a ghost town’",
    "text": "‘Trump turned this place into a ghost town’",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton-and-atlantic-city",
    "href": "week-01/01-introduction.html#stockton-and-atlantic-city",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "href": "week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City\n\nSource",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton",
    "href": "week-01/01-introduction.html#stockton",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton-1",
    "href": "week-01/01-introduction.html#stockton-1",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton-2",
    "href": "week-01/01-introduction.html#stockton-2",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#stockton-3",
    "href": "week-01/01-introduction.html#stockton-3",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-01/01-introduction.html#you",
    "href": "week-01/01-introduction.html#you",
    "title": "Course Introduction",
    "section": "You",
    "text": "You\nTake a few minutes and let us know a bit about yourself\n\nName\nProgram/Concentration\nWhy you are here",
    "crumbs": [
      "Week 1 1/18",
      "Course Introduction"
    ]
  },
  {
    "objectID": "week-06/geopandas.html",
    "href": "week-06/geopandas.html",
    "title": "GeoPandas",
    "section": "",
    "text": "import geopandas as gpd\nimport pandas as pd\ngdf = gpd.read_parquet(\"~/data/scag_region.parquet\")\ngdf.shape\n\n(4580, 194)\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame\ngdf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\ngdf.columns.values\n\narray(['geoid', 'n_asian_under_15', 'n_black_under_15',\n       'n_hispanic_under_15', 'n_native_under_15', 'n_white_under_15',\n       'n_persons_under_18', 'n_asian_over_60', 'n_black_over_60',\n       'n_hispanic_over_60', 'n_native_over_60', 'n_persons_over_60',\n       'n_white_over_60', 'n_asian_over_65', 'n_black_over_65',\n       'n_hispanic_over_65', 'n_native_over_65', 'n_white_over_65',\n       'n_persons_over_75', 'n_persons_over_15', 'n_civilians_over_16',\n       'n_civilians_over_18', 'n_persons_over_25', 'n_age_5_older',\n       'n_asian_age_distribution', 'n_black_age_distribution',\n       'n_hispanic_age_distribution', 'n_native_age_distribution',\n       'n_white_age_distribution', 'n_asian_persons', 'n_black_persons',\n       'n_chinese_persons', 'n_labor_force', 'n_civilians_16_64',\n       'n_edu_college_greater', 'n_cuban_pop',\n       'n_poverty_determined_asian', 'n_poverty_determined_black',\n       'n_total_pop_sample', 'n_female_over_16',\n       'n_poverty_determined_families', 'n_poverty_determined_hispanic',\n       'n_disabled', 'n_housing_units_multiunit_structures_denom',\n       'n_poverty_determined_native', 'n_poverty_determined_persons',\n       'n_poverty_determined_white', 'n_employed_over_16',\n       'n_total_families', 'n_foreign_born_pop',\n       'n_female_headed_families', 'n_filipino_persons',\n       'n_female_labor_force', 'n_german_pop', 'n_german_born_pop',\n       'n_household_recent_move', 'n_structures_30_old',\n       'n_hawaiian_persons', 'n_total_households', 'n_asian_households',\n       'n_black_households', 'n_hispanic_households',\n       'n_white_households', 'median_household_income',\n       'median_income_asianhh', 'median_income_blackhh',\n       'median_income_hispanichh', 'median_income_whitehh',\n       'n_hispanic_persons', 'n_edu_hs_less', 'n_total_housing_units',\n       'per_capita_income', 'n_asian_indian_persons', 'n_irish_pop',\n       'n_irish_born_pop', 'n_italian_pop', 'n_italian_born_pop',\n       'n_japanese_persons', 'n_korean_persons', 'n_limited_english',\n       'n_employed_manufacturing', 'n_married', 'n_mexican_pop',\n       'median_home_value', 'median_contract_rent',\n       'n_housing_units_multiunit_structures', 'n_recent_immigrant_pop',\n       'n_poverty_over_65', 'n_poverty_asian', 'n_naturalized_pop',\n       'n_poverty_black', 'n_poverty_families_children',\n       'n_nonhisp_black_persons', 'n_poverty_hispanic',\n       'n_nonhisp_white_persons', 'n_poverty_native', 'n_poverty_persons',\n       'n_native_persons', 'n_poverty_white', 'n_occupied_housing_units',\n       'n_other_language', 'n_owner_occupied_housing_units',\n       'p_recent_immigrant_pop', 'p_household_recent_move',\n       'p_asian_under_15', 'p_black_under_15', 'p_hispanic_under_15',\n       'p_native_under_15', 'p_white_under_15', 'p_persons_under_18',\n       'p_structures_30_old', 'p_persons_over_60', 'p_asian_over_65',\n       'p_black_over_65', 'p_hispanic_over_65', 'p_native_over_65',\n       'p_poverty_rate_over_65', 'p_white_over_65', 'p_persons_over_75',\n       'p_poverty_rate_asian', 'p_asian_persons', 'p_poverty_rate_black',\n       'p_chinese_persons', 'p_edu_college_greater', 'p_cuban_pop',\n       'p_foreign_born_pop', 'p_female_headed_families',\n       'p_filipino_persons', 'p_female_labor_force',\n       'p_poverty_rate_children', 'p_german_pop', 'p_german_born_pop',\n       'p_hawaiian_persons', 'p_hispanic_persons',\n       'p_poverty_rate_hispanic', 'p_edu_hs_less',\n       'p_asian_indian_persons', 'p_irish_pop', 'p_irish_born_pop',\n       'p_italian_pop', 'p_italian_born_pop', 'p_japanese_persons',\n       'p_korean_persons', 'p_limited_english',\n       'p_employed_manufacturing', 'p_married', 'p_mexican_pop',\n       'p_housing_units_multiunit_structures', 'p_poverty_rate_native',\n       'p_naturalized_pop', 'p_nonhisp_black_persons', 'p_black_persons',\n       'p_native_persons', 'p_other_language', 'n_total_pop',\n       'p_owner_occupied_units', 'p_poverty_rate', 'p_puerto_rican_pop',\n       'p_employed_professional', 'n_puerto_rican_pop',\n       'n_employed_professional', 'p_russian_pop', 'p_russian_born_pop',\n       'p_scandanavian_pop', 'p_scandanavian_born_pop',\n       'p_employed_self_employed', 'p_unemployment_rate',\n       'p_vacant_housing_units', 'p_veterans', 'p_vietnamese_persons',\n       'p_widowed_divorced', 'p_poverty_rate_white',\n       'n_renter_occupied_housing_units', 'n_russian_pop',\n       'n_russian_born_pop', 'n_scandaniavian_pop',\n       'n_scandaniavian__born_pop', 'n_employed_self_employed',\n       'n_unemployed_persons', 'n_vacant_housing_units', 'n_veterans',\n       'n_vietnamese_persons', 'n_widowed_divorced', 'n_white_persons',\n       'year', 'n_total_housing_units_sample', 'p_nonhisp_white_persons',\n       'p_white_over_60', 'p_black_over_60', 'p_hispanic_over_60',\n       'p_native_over_60', 'p_asian_over_60', 'p_disabled', 'geometry'],\n      dtype=object)\ngdf.n_total_pop\n\n0       5497.0\n1       5659.0\n2       4486.0\n3       2924.0\n4       3415.0\n         ...  \n4575    3672.0\n4576    5257.0\n4577    6765.0\n4578    2981.0\n4579    3994.0\nName: n_total_pop, Length: 4580, dtype: float64\ngdf.geometry\n\n0       POLYGON ((-118.44870 34.16485, -118.43997 34.1...\n1       POLYGON ((-118.56229 34.22033, -118.55792 34.2...\n2       POLYGON ((-118.57976 34.21558, -118.57539 34.2...\n3       POLYGON ((-118.61472 34.21952, -118.61039 34.2...\n4       POLYGON ((-118.25416 33.93882, -118.25413 33.9...\n                              ...                        \n4575    POLYGON ((-118.50373 34.42607, -118.50050 34.4...\n4576    POLYGON ((-118.20731 33.90754, -118.20641 33.9...\n4577    POLYGON ((-119.22134 34.18130, -119.21727 34.1...\n4578    POLYGON ((-116.51068 33.80502, -116.51069 33.8...\n4579    POLYGON ((-118.41379 34.17940, -118.41160 34.1...\nName: geometry, Length: 4580, dtype: geometry\ngdf.plot()",
    "crumbs": [
      "Week 6 2/20, 2/22",
      "GeoPandas"
    ]
  },
  {
    "objectID": "week-06/geopandas.html#projections",
    "href": "week-06/geopandas.html#projections",
    "title": "GeoPandas",
    "section": "Projections",
    "text": "Projections\n\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf1 = gdf.to_crs(3857)\n\n\ngdf1.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf1.plot()\n\n\n\n\n\n\n\n\n\ngdf.median_home_value\n\n0       647272.659176\n1       400842.977528\n2       416741.666667\n3       406178.838951\n4       251438.857678\n            ...      \n4575    291838.951311\n4576    273871.254682\n4577    293254.588015\n4578    255794.662921\n4579    581717.790262\nName: median_home_value, Length: 4580, dtype: float64\n\n\n\ngdf.plot(column='median_home_value')\n\n\n\n\n\n\n\n\n\ngdf.plot(column='median_home_value', legend=True)\n\n\n\n\n\n\n\n\n\ngdf.plot(column='median_home_value', legend=True,\n        scheme='quantiles', k=10)\n\n\n\n\n\n\n\n\n\ngdf.plot(column='p_hispanic_persons', legend=True,\n        scheme='quantiles', k=10)\n\n\n\n\n\n\n\n\n\ngdf.explore(column='p_hispanic_persons', tooltip=['geoid', 'p_hispanic_persons'])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Week 6 2/20, 2/22",
      "GeoPandas"
    ]
  },
  {
    "objectID": "week-03/python_intro.html",
    "href": "week-03/python_intro.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "its “free” (as in beer and as in speech)\nwidely used (QGIS, ArcGIS)\napproachable\nfun\n\n\n\n\n\ninput (keyboard, file, network, sensor)\noutput: display data on screen, write to a file, ship over network\nmath: basic mathematical operations (add, subtract, multiply, divide)\nconditional execution: if something is true do something else\nrepetition: Perform some action repeatedly\n\n\n1 + 1\n\n2\n\n\n\nx = 1 + 1\n\n\n\n\n\n40 + 2\n\n42\n\n\n\n40 * 2\n\n80\n\n\n\n40 / 2\n\n20.0\n\n\n\n40 ** 2\n\n1600\n\n\n\n40**3\n\n64000\n\n\n\n\n\nWhat the program works with\n\n2\n\n2\n\n\n\ntype(2)\n\nint\n\n\n\ntype('42')\n\nstr\n\n\n\n'42'\n\n'42'\n\n\n\n2 * 3\n\n6\n\n\n\n2 * 'my name is serge'\n\n'my name is sergemy name is serge'\n\n\n\nname = 'serge'\n\n\nname\n\n'serge'\n\n\n\ntype(name)\n\nstr\n\n\n\nname = 10\n\n\nname\n\n10\n\n\n\nx = 10**2 + 50\n\n\nx\n\n150\n\n\n\nx = 10**(2 + 50)\n\n\nx\n\n10000000000000000000000000000000000000000000000000000\n\n\n\nf = 3.14\ntype(f)\n\nfloat\n\n\n\nprint('hi world')\n\nhi world\n\n\n\ntempF = 9 / 5 * 40 + 32\n\n\ntempF\n\n104.0\n\n\n\ndef tempF(c):\n    return 9/5 * c + 32\n\n\ntempF(32)\n\n89.6\n\n\n\n\n\n\nint: 7\nfloat: 7.23\nstr: ‘7.23’\nbool: True/False\n\n\nx = 5 &gt; 7\nx\n\nFalse\n\n\n\ntype(x)\n\nbool\n\n\n\ny = 10\n\n\ny\n\n10\n\n\n\nx * y\n\n0\n\n\n\nz = 10 &gt; 1\n\n\nz\n\nTrue\n\n\n\nz * y\n\n10\n\n\n\n\n\n\nx = 'Today is my birthday'\n\n\nx\n\n'Today is my birthday'\n\n\n\ntype(x)\n\nstr\n\n\n\nlen(x)\n\n20\n\n\n\nx.lower()\n\n'today is my birthday'\n\n\n\nx.upper()\n\n'TODAY IS MY BIRTHDAY'\n\n\n\nx.title()\n\n'Today Is My Birthday'\n\n\n\nx.center(50)\n\n'               Today is my birthday               '\n\n\n\nx.title().center(100)\n\n'                                        Today Is My Birthday                                        '\n\n\n\ndir(x)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\nx\n\n'Today is my birthday'\n\n\n\nx.zfill(30)\n\n'0000000000Today is my birthday'\n\n\n\nx.zfill?\n\n\nSignature: x.zfill(width, /)\nDocstring:\nPad a numeric string with zeros on the left, to fill a field of the given width.\nThe string is never truncated.\nType:      builtin_function_or_method\n\n\n\n\nx.strip?\n\n\nSignature: x.strip(chars=None, /)\nDocstring:\nReturn a copy of the string with leading and trailing whitespace removed.\nIf chars is given and not None, remove characters in chars instead.\nType:      builtin_function_or_method\n\n\n\n\nx\n\n'Today is my birthday'\n\n\n\nx.split()\n\n['Today', 'is', 'my', 'birthday']\n\n\n\nx.split?\n\n\nSignature: x.split(sep=None, maxsplit=-1)\nDocstring:\nReturn a list of the substrings in the string, using sep as the separator string.\n  sep\n    The separator used to split the string.\n    When set to None (the default value), will split on any whitespace\n    character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n    empty strings from the result.\n  maxsplit\n    Maximum number of splits (starting from the left).\n    -1 (the default value) means no limit.\nNote, str.split() is mainly useful for data that has been intentionally\ndelimited.  With natural text that includes punctuation, consider using\nthe regular expression module.\nType:      builtin_function_or_method",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#why-python",
    "href": "week-03/python_intro.html#why-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "its “free” (as in beer and as in speech)\nwidely used (QGIS, ArcGIS)\napproachable\nfun",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#what-is-a-program",
    "href": "week-03/python_intro.html#what-is-a-program",
    "title": "Introduction to Python",
    "section": "",
    "text": "input (keyboard, file, network, sensor)\noutput: display data on screen, write to a file, ship over network\nmath: basic mathematical operations (add, subtract, multiply, divide)\nconditional execution: if something is true do something else\nrepetition: Perform some action repeatedly\n\n\n1 + 1\n\n2\n\n\n\nx = 1 + 1",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#operators",
    "href": "week-03/python_intro.html#operators",
    "title": "Introduction to Python",
    "section": "",
    "text": "40 + 2\n\n42\n\n\n\n40 * 2\n\n80\n\n\n\n40 / 2\n\n20.0\n\n\n\n40 ** 2\n\n1600\n\n\n\n40**3\n\n64000",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#values",
    "href": "week-03/python_intro.html#values",
    "title": "Introduction to Python",
    "section": "",
    "text": "What the program works with\n\n2\n\n2\n\n\n\ntype(2)\n\nint\n\n\n\ntype('42')\n\nstr\n\n\n\n'42'\n\n'42'\n\n\n\n2 * 3\n\n6\n\n\n\n2 * 'my name is serge'\n\n'my name is sergemy name is serge'\n\n\n\nname = 'serge'\n\n\nname\n\n'serge'\n\n\n\ntype(name)\n\nstr\n\n\n\nname = 10\n\n\nname\n\n10\n\n\n\nx = 10**2 + 50\n\n\nx\n\n150\n\n\n\nx = 10**(2 + 50)\n\n\nx\n\n10000000000000000000000000000000000000000000000000000\n\n\n\nf = 3.14\ntype(f)\n\nfloat\n\n\n\nprint('hi world')\n\nhi world\n\n\n\ntempF = 9 / 5 * 40 + 32\n\n\ntempF\n\n104.0\n\n\n\ndef tempF(c):\n    return 9/5 * c + 32\n\n\ntempF(32)\n\n89.6",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#data-types",
    "href": "week-03/python_intro.html#data-types",
    "title": "Introduction to Python",
    "section": "",
    "text": "int: 7\nfloat: 7.23\nstr: ‘7.23’\nbool: True/False\n\n\nx = 5 &gt; 7\nx\n\nFalse\n\n\n\ntype(x)\n\nbool\n\n\n\ny = 10\n\n\ny\n\n10\n\n\n\nx * y\n\n0\n\n\n\nz = 10 &gt; 1\n\n\nz\n\nTrue\n\n\n\nz * y\n\n10",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "week-03/python_intro.html#strings-str",
    "href": "week-03/python_intro.html#strings-str",
    "title": "Introduction to Python",
    "section": "",
    "text": "x = 'Today is my birthday'\n\n\nx\n\n'Today is my birthday'\n\n\n\ntype(x)\n\nstr\n\n\n\nlen(x)\n\n20\n\n\n\nx.lower()\n\n'today is my birthday'\n\n\n\nx.upper()\n\n'TODAY IS MY BIRTHDAY'\n\n\n\nx.title()\n\n'Today Is My Birthday'\n\n\n\nx.center(50)\n\n'               Today is my birthday               '\n\n\n\nx.title().center(100)\n\n'                                        Today Is My Birthday                                        '\n\n\n\ndir(x)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\nx\n\n'Today is my birthday'\n\n\n\nx.zfill(30)\n\n'0000000000Today is my birthday'\n\n\n\nx.zfill?\n\n\nSignature: x.zfill(width, /)\nDocstring:\nPad a numeric string with zeros on the left, to fill a field of the given width.\nThe string is never truncated.\nType:      builtin_function_or_method\n\n\n\n\nx.strip?\n\n\nSignature: x.strip(chars=None, /)\nDocstring:\nReturn a copy of the string with leading and trailing whitespace removed.\nIf chars is given and not None, remove characters in chars instead.\nType:      builtin_function_or_method\n\n\n\n\nx\n\n'Today is my birthday'\n\n\n\nx.split()\n\n['Today', 'is', 'my', 'birthday']\n\n\n\nx.split?\n\n\nSignature: x.split(sep=None, maxsplit=-1)\nDocstring:\nReturn a list of the substrings in the string, using sep as the separator string.\n  sep\n    The separator used to split the string.\n    When set to None (the default value), will split on any whitespace\n    character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n    empty strings from the result.\n  maxsplit\n    Maximum number of splits (starting from the left).\n    -1 (the default value) means no limit.\nNote, str.split() is mainly useful for data that has been intentionally\ndelimited.  With natural text that includes punctuation, consider using\nthe regular expression module.\nType:      builtin_function_or_method",
    "crumbs": [
      "Week 3 1/30, 2/01",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "syllabus.html#class-meetings",
    "href": "syllabus.html#class-meetings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Class Meetings",
    "text": "Class Meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nGMCS 307\nTue & Thu 2:00 - 3:15pm",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nSergio Rey\nTue 9:00 - 10:00 (by appointment)\nPSFA 361G\n\n\nJin Huang\nFri 10:30am (virtual)\nPSFA 361F",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#introduction",
    "href": "syllabus.html#introduction",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nWelcome to 385: Spatial Data Analysis!\nThe purpose of this course is to introduce you to methods of spatial data analysis. The focus is on both the conceptual and applied aspects of spatial statistical methods. We will place particular emphasis on the computational aspects of Exploratory Spatial Data Analysis (ESDA) methods for diﬀerent types of spatial data including point processes, lattice data, geostatistical data, network data, and spatial interaction. Throughout the course you will gain valuable hands-on experience with several specialized software packages for spatial data analysis. The overriding goal of the course is for you to acquire familiarity with the fundamental methodological and operational issues in the statistical analysis of geographic information and the ability to extend these methods in your own research.\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts. Put differently, students will learn to code. But this is a means to the end goal: students will code to learn spatial data analysis.\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nGEOG 101 or GEOG 102\nSTAT 250 or comparable course in statistics.\n\nAll students are required to complete the prerequisite assessment quiz before 2024-01-30 2:00pm.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#computational-learning",
    "href": "syllabus.html#computational-learning",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Computational Learning",
    "text": "Computational Learning\nWe will be using open source geospatial software throughout the course together with Jupyter Notebooks, and Python as our scripting language.\nAll software for the course will be made available through JupyterHub a web-based framework. Students wishing to install these materials on their own machines will be given instructions to do so, but this is not required.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Readings",
    "text": "Readings\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore being prepared for class means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings.\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#schedule-planned",
    "href": "syllabus.html#schedule-planned",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)\n\n\n\nWeek\nDates\nTopic\nReading\nDue\n\n\n\n\n1\nJan-18\nCourse Introduction\n\n\n\n\n2\nJan-23\nComputational Environment I\n\n\n\n\n\nJan-25\nComputational Environment II\n\n\n\n\n3\nJan-30\nPython: Introduction\nGDA 1 GDS 2\nQuiz 1\n\n\n\nFeb-01\nPython: Programming Concepts\nGDA 2\n\n\n\n4\nFeb-06\nPython: Scripting\nGDA 2\nQuiz 2\n\n\n\nFeb-08\nPython: Functions\nGDA 2\n\n\n\n5\nFeb-13\nExericse 1 Collaboration\n\nExercise 1\n\n\n\nFeb-15\nPython: Data Analysis/Visualization\nGDA 3,4\nQuiz 3\n\n\n6\nFeb-20\nGeopandas\nGDA 5\nQuiz 4\n\n\n\nFeb-22\nGeoprocessing\nGDA 6\n\n\n\n7\nFeb-27\nGeoSNAP\n\nQuiz 5\n\n\n\nFeb-29\nPySAL\nGDS 3\n\n\n\n8\nMar-05\nGeoVisualization\nGDS 5\nQuiz 6\n\n\n\nMar-07\nSpatial Weights\nGDS 4\nExercise 2\n\n\n9\nMar-12\nSpatial Dependence\nGDS 6\nQuiz 7\n\n\n\nMar-14\nGlobal Autocorrelation\nGDS 6\n\n\n\n10\nMar-19\nGlobal Autocorrelation Tests\nGDS 6\nQuiz 8\n\n\n\nMar-21\nLocal Autocorrelation\nGDS 7\n\n\n\n11\nMar-26\nLocal Autocorrelation Tests\nGDS 7\nQuiz 9\n\n\n\nMar-28\nPoint Pattern Data\nGDS 8\nExercise 3\n\n\n\nApr-02\nSpring Break\n\n\n\n\n\nApr-04\nSpring Break\n\n\n\n\n12\nApr-09\nCentrography\nGDS 8\nQuiz 10\n\n\n\nApr-11\nPoint Processes\nGDS 8\n\n\n\n13\nApr-16\nQuadrat Statistics\nGDS 8\nQuiz 11\n\n\n\nApr-18\nNearest Neighbor Statistics\nGDS 8\n\n\n\n14\nApr-23\nDistance Based Statistics\nGDS 8\nQuiz 12\n\n\n\nApr-25\nGeostatistical Data\nDS 6.1-6.5\n\n\n\n15\nApr-30\nSpatial Interpolation\nDS 6.5\nQuiz 13\n\n\n\nMay-02\nKriging\nDS 6.7\nExercise 4\n\n\n16\nMay-07\nFinal Exam (1-3pm)",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course.\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade.\n\nFor Step 1, the base grade is determined using the following specification:\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 12 of 14 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 10 of 14 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 8 of 14 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 6 of 14 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles\n\n\n\nFor Step 2, your final course grade is determined as follows:\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Quizzes",
    "text": "Quizzes\nStarting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class. Quizzes are graded on a pass/fail basis.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#exercises",
    "href": "syllabus.html#exercises",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise.\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#final-exam",
    "href": "syllabus.html#final-exam",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on May 7 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#tokens",
    "href": "syllabus.html#tokens",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester.\nUsing Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam. (Requests required by 2024-04-15 17:00.)\n\nRemaining Tokens\nEach token that remains unused after 2024-05-02 will be counted as a passed quiz. Tokens cannot be exchanged with other students.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Policies",
    "text": "Policies\n\nAccommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center.\n\n\nPrivacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use Canvas to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise.\n\n\nAcademic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: https://sacd.sdsu.edu/student-rights/academic-dishonesty.\n\n\nCode of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form.\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct.",
    "crumbs": [
      "Syllabus",
      "Geography 385 Spatial Data Analysis"
    ]
  },
  {
    "objectID": "week-04/scripts_and_functions.html",
    "href": "week-04/scripts_and_functions.html",
    "title": "Functions",
    "section": "",
    "text": "A function is a body of code that does something we need done, typically repeatedly\ndef hello():\n    print('hello world')\nhello()\n\nhello world",
    "crumbs": [
      "Week 4 2/06, 2/08",
      "Functions"
    ]
  },
  {
    "objectID": "week-04/scripts_and_functions.html#returns",
    "href": "week-04/scripts_and_functions.html#returns",
    "title": "Functions",
    "section": "Returns",
    "text": "Returns\n\ndef hello(message):\n    print(message)\n    return len(message)\n\n\nhello('serge')\n\nserge\n\n\n5\n\n\n\nlen('serge')\n\n5\n\n\n\nhello(42)\n\n42\n\n\nTypeError: object of type 'int' has no len()\n\n\n\nlen(10)\n\nTypeError: object of type 'int' has no len()\n\n\n\nhello('42')\n\n42\n\n\n2",
    "crumbs": [
      "Week 4 2/06, 2/08",
      "Functions"
    ]
  },
  {
    "objectID": "week-04/scripts_and_functions.html#real-functions",
    "href": "week-04/scripts_and_functions.html#real-functions",
    "title": "Functions",
    "section": "“Real functions”",
    "text": "“Real functions”\n\n\n\nimage.png\n\n\n\np3 = [0, 0]\np1 = [0, 5]\np2 = [10, 0]\n\n\n# distance between p1 and p2\nb = p2[0] - p3[0]\nb\n\n10\n\n\n\na = p1[1] - p3[1]\na\n\n5\n\n\n\n# c = sqrt(a**2 + b**2)\n\n\na**2\n\n25\n\n\n\nb**2\n\n100\n\n\n\n(a**2 + b**2)\n\n125\n\n\n\n(a**2 + b**2)**(1/2)\n\n11.180339887498949\n\n\n\ndef distance(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    return (dx**2 + dy**2)**(1/2)\n\n\np3 = [0, 0]\np1 = [0, 5]\np2 = [10, 0]\n\n\ndistance(p1, p3)\n\n5.0\n\n\n\ndistance(p2, p3)\n\n10.0\n\n\n\ndistance(p2, p1)\n\n11.180339887498949\n\n\n\ndistance(p1, p1)\n\n0.0\n\n\n\n\n\nimage.png\n\n\n\nimport numpy\ndef manhattan(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    adx = numpy.abs(dx)\n    ady = numpy.abs(dy)\n    return adx + ady\n    \n\n\np3 = [0, 0]\np1 = [0, 5]\np2 = [10, 0]\n\n\nmanhattan(p3, p2)\n\n10\n\n\n\nmanhattan(p3, p1)\n\n5\n\n\n\nmanhattan(p2, p1)\n\n15\n\n\n\ndistance(p2, p1)\n\n11.180339887498949\n\n\n\ndef euclidean(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    return (dx**2 + dy**2)**(1/2)\n\n\nimport numpy\ndef manhattan(pnt1, pnt2):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    adx = numpy.abs(dx)\n    ady = numpy.abs(dy)\n    return adx + ady\n    \n\n\neuclidean(p2, p1)\n\n11.180339887498949\n\n\n\nmanhattan(p2, p1)\n\n15\n\n\n\nmanhattan(p1, p2)\n\n15\n\n\n\neuclidean(p1, p2)\n\n11.180339887498949\n\n\n\nComposition\n\ndef distance_composite(pnt1, pnt2, metric='euclidean'):\n    if metric == 'euclidean':\n        return euclidean(pnt1, pnt2)\n    else:\n        return manhattan(pnt1, pnt2)\n\n\ndistance_composite(p1, p2, metric='euclidean')\n\n11.180339887498949\n\n\n\ndistance_composite(p1, p2, metric='manhattan')\n\n15\n\n\n\n\nAll-in-one function\n\ndef distance(pnt1, pnt2, metric='euclidean'):\n    dx = pnt1[0] - pnt2[0]\n    dy = pnt1[1] - pnt2[1]\n    if metric == 'euclidean':\n        return (dx**2 + dy**2)**(1/2)\n    else:\n        adx = numpy.abs(dx)\n        ady = numpy.abs(dy)\n        return adx + ady\n        \n\n\ndistance(p1, p2, metric='manhattan')\n\n15\n\n\n\ndistance(p1, p2, metric='euclidean')\n\n11.180339887498949\n\n\n\ndistance(p1, p2)\n\n11.180339887498949",
    "crumbs": [
      "Week 4 2/06, 2/08",
      "Functions"
    ]
  },
  {
    "objectID": "week-14/point-processes.html",
    "href": "week-14/point-processes.html",
    "title": "Point Processes",
    "section": "",
    "text": "Thus far we have been looking at a collection of points as a point pattern.\nNow we want to take a different view of that pattern, one that sees the pattern as the outcome of a process.\nA point process is a statistical model that will generate point patterns with particular characteristics.\nFrom a scientific point of view we are interested in making inferences about the process that may have generated our point pattern.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#introduction",
    "href": "week-14/point-processes.html#introduction",
    "title": "Point Processes",
    "section": "",
    "text": "Thus far we have been looking at a collection of points as a point pattern.\nNow we want to take a different view of that pattern, one that sees the pattern as the outcome of a process.\nA point process is a statistical model that will generate point patterns with particular characteristics.\nFrom a scientific point of view we are interested in making inferences about the process that may have generated our point pattern.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#first-order-properties",
    "href": "week-14/point-processes.html#first-order-properties",
    "title": "Point Processes",
    "section": "First Order Properties",
    "text": "First Order Properties\n\nFirst Order Properties: Spatial Analysis\nMean value of the process in space\n\nVariation in mean value of the process in space\nGlobal, large scale spatial trend\n\nFirst Order Property of Point Patterns, Intensity: \\(\\lambda\\)\n\nIntensity: \\(\\lambda\\) = number of events expected per unit area\nEstimation of \\(\\lambda\\)\nSpatial variation of \\(\\lambda\\), \\(\\lambda(s)\\), \\(s\\) is a location\n\n\\[\\lambda(s) = \\lim_{ds\\rightarrow 0}\\left\\{ \\frac{E(Y(ds))}{ds} \\right\\}\\]",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#second-order-property",
    "href": "week-14/point-processes.html#second-order-property",
    "title": "Point Processes",
    "section": "Second Order Property",
    "text": "Second Order Property\n\nSecond Order Properties: Spatial Analysis\nSpatial Correlation Structure\n\nDeviations in values from process mean\nLocal or small scale effects\n\nSecond Order Property of Point Patterns\n\nRelationship between number of events in pairs of areas\nSecond order intensity \\(\\gamma(s_i,s_j)\\)\n\n\\[\\gamma(s_i,s_j) = \\lim_{ds_i\\rightarrow 0,ds_j\\rightarrow 0}\\left\\{\n       \\frac{E(Y(ds_i)Y(ds_j))}{ds_ids_j} \\right\\}\\]",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#spatial-stationarity",
    "href": "week-14/point-processes.html#spatial-stationarity",
    "title": "Point Processes",
    "section": "Spatial Stationarity",
    "text": "Spatial Stationarity\nFirst Order Stationarity \\[\\lambda(s) = \\lambda \\forall s \\in A\\] \\[E(Y(A)) = \\lambda \\times A\\]\nSecond Order Stationarity \\[\\gamma(s_i,s_j) = \\gamma(s_i - s_j) = \\gamma(h)\\]\n\n\\(h\\) is the vector difference between locations \\(s_i\\) and \\(s_j\\)\n\\(h\\) encompasses direction and distance (relative location)\nSecond order intensity only depends on \\(h\\) for second order stationarity",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#spatial-isotropy-and-stationarity",
    "href": "week-14/point-processes.html#spatial-isotropy-and-stationarity",
    "title": "Point Processes",
    "section": "Spatial Isotropy and Stationarity",
    "text": "Spatial Isotropy and Stationarity\nIsotropic Process\n\nWhen a stationary process is invariant to rotation about the origin.\nRelationship between two events depend only on the distance separating their locations and not on their orientation to each other.\nDepends only on distance, not direction\n\nUsefulness\n\nTwo pairs of events from a stationary process separated by same distance and relative direction should have same “relatedness”\nTwo pairs of events from a stationary and isotropic process separated by the same distance (irrespective of direction) should have the same “relatedness”\nBoth allow for replication and the ability to carry out estimation of the underlying DGP.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#invariance",
    "href": "week-14/point-processes.html#invariance",
    "title": "Point Processes",
    "section": "Invariance",
    "text": "Invariance\n\n\n\n\n\n\n\n\nUnder Translation\n\n\n\n\n\n\n\nUnder Rotation\n\n\n\n\n\n\nFigure 1: Invariance",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#complete-spatial-randomness",
    "href": "week-14/point-processes.html#complete-spatial-randomness",
    "title": "Point Processes",
    "section": "Complete Spatial Randomness",
    "text": "Complete Spatial Randomness\n\nCSR\n\nStandard of Reference\nUniform: each location has equal probability\nIndependent: location of points independent\nHomogeneous Planar Poisson Point Process\n\n\n\nPoisson Point Process\n\nIntensity\n\nnumber of points in region \\(A: N(A)\\)\nintensity: \\(\\lambda = N/|A|\\)\nimplies: \\(\\lambda |A|\\) points randomly scattered in a region with area \\(|A|\\)\ne.g., \\(10\\times 1\\) (points per \\(km^2\\))\n\n\n\nPoisson Distribution \\(N(A) \\sim Poi(\\lambda |A|)\\)",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#poisson-distribution",
    "href": "week-14/point-processes.html#poisson-distribution",
    "title": "Point Processes",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nSingle Parameter Distribution: \\(\\lambda |A|\\)\n\nGenerally, \\(\\lambda\\) is the number of events in some well defined interval\n\nTime: phone calls to operator in one hour\nTime: accidents at an intersection per week\nSpace: trees in a quadrat\n\nLet \\(x\\) be a Poisson random variable\n\n\\(E[x] = V[x]= \\lambda |A|\\)\n\n\n\n\nPoisson Distribution \\[P(x) =  \\frac{e^{-\\lambda |A|} (\\lambda |A|)^x}{x!}\\]",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#spatial-example",
    "href": "week-14/point-processes.html#spatial-example",
    "title": "Point Processes",
    "section": "Spatial Example",
    "text": "Spatial Example\n\nCSR with \\(\\lambda = 5/km^2\\)\n\nRegion = Circle\n\narea = \\(|A| = \\pi r^2\\)\n\\(r=0.1\\ km\\) then area \\(\\approx 0.03 \\ km^2\\)\n\nProbability of Zero Points in Circle \\[\\begin{aligned}\n         P[N(A) = 0] &= &  e^{-\\lambda |A|} (\\lambda |A|)^x /x!\\\\\n                     &\\approx&e^{-5 \\times 0.03} (5 \\times 0.03)^0 /0!\\\\\n                     &\\approx&e^{-5 \\times 0.03} \\\\\n                     &\\approx&0.86\n       \\end{aligned}\\]",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#complete-spatial-randomness-csr",
    "href": "week-14/point-processes.html#complete-spatial-randomness-csr",
    "title": "Point Processes",
    "section": "Complete Spatial Randomness (CSR)",
    "text": "Complete Spatial Randomness (CSR)\n\nHomogeneous spatial Poisson point process\n\nThe number of events occurring within a finite region \\(A\\) is a random variable following a Poisson distribution with mean \\(\\lambda|A|\\), with \\(|A|\\) denoting area of \\(A\\).\nGiven the total number of events \\(N\\) occurring within an area \\(A\\), the locations of the \\(N\\) events represent an independent random sample of \\(N\\) locations where each location is equally likely to be chosen as an event.\n\n\n\n\nCriterion 2 is the general concept of CSR (uniform (random)) distribution in \\(A\\).\nCriterion 1 pertains to the intensity \\(\\lambda\\).",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#homogeneous-poisson-process",
    "href": "week-14/point-processes.html#homogeneous-poisson-process",
    "title": "Point Processes",
    "section": "Homogeneous Poisson Process",
    "text": "Homogeneous Poisson Process\n\nImplications\n\nThe number of events in nonoverlapping regions in \\(A\\) are statistically independent.\nFor any region \\(R \\subset A\\): \\[\\lim_{|R| \\rightarrow 0} \\frac{Pr[exactly\\ one\\ event\\ in\\ R]}{|R|}\n      = \\lambda &gt; 0\\]\n\\[\\lim_{|R| \\rightarrow 0} \\frac{Pr[more\\ than\\ one\\ event\\ in\\\n       R]}{|R|} = 0\\]\n\n\n:::",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#homogeneous-poisson-process-1",
    "href": "week-14/point-processes.html#homogeneous-poisson-process-1",
    "title": "Point Processes",
    "section": "Homogeneous Poisson process",
    "text": "Homogeneous Poisson process\n\nImplications\n\n\\(\\lambda\\) is the intensity of the spatial point pattern.\nFor a Poisson random variable, \\(Y\\): \\[E[Y] = \\lambda = V[Y]\\]\nProvides the motivation for some quadrat tests of CSR hypothesis.\n\nIf \\(Y_R\\) is the count in quadrat \\(R\\)\nIf \\(\\widehat{E[Y]}&lt; \\widehat{V[Y]}\\): overdispersion = spatial clustering\nIf \\(\\widehat{E[Y]}&gt; \\widehat{V[Y]}\\): underdispersion = spatial uniformity",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#poisson-distribution-lambda20",
    "href": "week-14/point-processes.html#poisson-distribution-lambda20",
    "title": "Point Processes",
    "section": "Poisson Distribution \\(\\lambda=20\\)",
    "text": "Poisson Distribution \\(\\lambda=20\\)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nnp.random.seed(12345)\nxy = np.random.rand(20,2)\ndf = pd.DataFrame(data=xy, columns=['x','y'])\nsns.scatterplot(x='x', y='y', data=df);\ndf.shape\n\n\n\n\n\n\n\n\nThe example we just did is known as \\(n-conditioning\\) where we will always get \\(n\\) points for the CSR process.\nA slightly different approach to generating a random point process is to use \\(\\lambda-conditioning\\)\n\nfrom scipy.stats import poisson\nlam=20\nn = poisson.rvs(lam, 1)\nxy = np.random.rand(n,2)\ndf = pd.DataFrame(data=xy, columns=['x','y'])\nsns.scatterplot(x='x', y='y', data=df);\ndf.shape\n\n\n\n\n\n\n\n\nThe difference is the number of points in the pattern will always be \\(n\\) with \\(n-conditioning\\) but may not be \\(n\\) with \\(\\lambda-conditioning\\). The latter allows the intensity to be drawn from a Poisson distribution, then that becomes the parameter for the draw of the point pattern.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#limitations-of-csr",
    "href": "week-14/point-processes.html#limitations-of-csr",
    "title": "Point Processes",
    "section": "Limitations of CSR",
    "text": "Limitations of CSR\n\nStationary Poisson Process\n\nhomogeneous\ntranslation invaratiant\n\n\n\nRare in practice - very few actual processes are CSR\n\n\nStrawman\n\npurely a benchmark\nnull hypothesis",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#inhomogeneous-poisson-process-ipp",
    "href": "week-14/point-processes.html#inhomogeneous-poisson-process-ipp",
    "title": "Point Processes",
    "section": "Inhomogeneous Poisson Process (IPP)",
    "text": "Inhomogeneous Poisson Process (IPP)\n\nCriteria\n\nThe number of events occurring within a finite region \\(A\\) is a random variable following a Poisson Distribution with mean \\(\\int_{A}\\lambda(s) ds\\).\nGiven the total number of events \\(N\\) occurring within \\(A\\), the \\(N\\) events represent an independent sample of \\(N\\) locations, with the probability of sampling a particular point \\(s\\) proportional to \\(\\lambda(s)\\).\n\n\n\nSpatially Variable Intensity \\(\\lambda(s)\\)\n\nUseful for constant risk hypothesis\nUnderlying population at risk is spatially clustered\nWant to control for that since with individual constant risk apparent clusters would be generated.\nCompare pattern against constant risk, not CSR.",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#inhomogeneous-poisson-process",
    "href": "week-14/point-processes.html#inhomogeneous-poisson-process",
    "title": "Point Processes",
    "section": "Inhomogeneous Poisson Process",
    "text": "Inhomogeneous Poisson Process\n\nImplications\n\nApparent clusters can occur solely due to heterogeneities in the intensity function \\(\\lambda(s)\\).\nIndividual event locations still remain independent of one another.\nProcess is not stationary due to intensity heterogeneity\n\n\n\nHPP vs. IPP HPP is a special case of IPP with a constant intensity",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#csr-vs.-constant-risk-hypotheses",
    "href": "week-14/point-processes.html#csr-vs.-constant-risk-hypotheses",
    "title": "Point Processes",
    "section": "CSR vs. Constant Risk Hypotheses",
    "text": "CSR vs. Constant Risk Hypotheses\n\nCSR\n\nIntensity is spatially constant\nPopulation at risk assumed spatially uniform\nUseful null hypothesis if these conditions are met\n\n\n\nConstant Risk Hypothesis\n\nPopulation density variable\nIndividual risk constant\nExpected number of events should vary with population density\nClusters due to deviation from CSR\nClusters due to deviation from CSR and Constant Risk",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#contagion-process-of-size-20-with-10-parents-and-2-children",
    "href": "week-14/point-processes.html#contagion-process-of-size-20-with-10-parents-and-2-children",
    "title": "Point Processes",
    "section": "Contagion process of size 20 with 10 parents and 2 children",
    "text": "Contagion process of size 20 with 10 parents and 2 children\n\nimport pointpats as pp\nnp.random.seed(12345)\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, 20, 10, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (10 parents)')",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#contagion-process-of-size-20-with-2-parents-and-10-children",
    "href": "week-14/point-processes.html#contagion-process-of-size-20-with-2-parents-and-10-children",
    "title": "Point Processes",
    "section": "Contagion process of size 20 with 2 parents and 10 children",
    "text": "Contagion process of size 20 with 2 parents and 10 children\n\nimport pointpats as pp\nnp.random.seed(12345)\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, 20, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#inhomogenous-poisson-process",
    "href": "week-14/point-processes.html#inhomogenous-poisson-process",
    "title": "Point Processes",
    "section": "Inhomogenous Poisson Process",
    "text": "Inhomogenous Poisson Process\n\nIntensity varies with a covariate\n\ntrend surface\n\\(\\lambda(s) = exp(\\alpha + \\beta s)\\)\n\n\n\nIntensity varies with distance to a focus\n\n\\(\\lambda(s) = \\lambda 0(s). f( || s-s_0||, \\theta)\\)",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#simulating-an-inhomogeneous-poisson-point-process",
    "href": "week-14/point-processes.html#simulating-an-inhomogeneous-poisson-point-process",
    "title": "Point Processes",
    "section": "Simulating An Inhomogeneous Poisson Point Process",
    "text": "Simulating An Inhomogeneous Poisson Point Process\nIntensity function:\n\\(\\lambda(s) = 100 e^{-(x^2 + y^2) / \\sigma}\\)\n\\(\\sigma\\) is a scale parameter here, equal to 0.5\n\n\nCode\nimport numpy as np;  # NumPy package for arrays, random number generation, etc\nimport matplotlib.pyplot as plt  # For plotting\nfrom scipy.optimize import minimize  # For optimizing\nfrom scipy import integrate  # For integrating\n\nplt.close('all');  # close all figures\n\n# Simulation window parameters\nxMin = 0;\nxMax = 1;\nyMin = 0;\nyMax = 1;\nxDelta = xMax - xMin;\nyDelta = yMax - yMin;  # rectangle dimensions\nareaTotal = xDelta * yDelta;\n\nnumbSim = 10 ** 3;  # number of simulations\ns = 0.5;  # scale parameter\n# Point process parameters\ndef fun_lambda(x, y):\n    return 100 * np.exp(-(x ** 2 + y ** 2) / s ** 2);  # intensity function\n#fun_lambda = lambda x,y: 100 * np.exp(-(x ** 2 + y ** 2) / s ** 2);\n\n###START -- find maximum lambda -- START ###\n# For an intensity function lambda, given by function fun_lambda,\n# finds the maximum of lambda in a rectangular region given by\n# [xMin,xMax,yMin,yMax].\ndef fun_Neg(x):\n    return -fun_lambda(x[0], x[1]);  # negative of lambda\n#fun_Neg = lambda x: -fun_lambda(x[0], x[1]);  # negative of lambda\n\nxy0 = [(xMin + xMax) / 2, (yMin + yMax) / 2];  # initial value(ie centre)\n# Find largest lambda value\nresultsOpt = minimize(fun_Neg, xy0, bounds=((xMin, xMax), (yMin, yMax)));\nlambdaNegMin = resultsOpt.fun;  # retrieve minimum value found by minimize\nlambdaMax = -lambdaNegMin;\n\n\n###END -- find maximum lambda -- END ###\n\n# define thinning probability function\ndef fun_p(x, y):\n    return fun_lambda(x, y) / lambdaMax;\n#fun_p = lambda x, y: fun_lambda(x, y) / lambdaMax;\n\n# for collecting statistics -- set numbSim=1 for one simulation\nnumbPointsRetained = np.zeros(numbSim);  # vector to record number of points\nfor ii in range(numbSim):\n    # Simulate a Poisson point process\n    numbPoints = np.random.poisson(areaTotal * lambdaMax);  # Poisson number of points\n    xx = np.random.uniform(0, xDelta, ((numbPoints, 1))) + xMin;  # x coordinates of Poisson points\n    yy = np.random.uniform(0, yDelta, ((numbPoints, 1))) + yMin;  # y coordinates of Poisson points\n\n    # calculate spatially-dependent thinning probabilities\n    p = fun_p(xx, yy);\n\n    # Generate Bernoulli variables (ie coin flips) for thinning\n    booleRetained = np.random.uniform(0, 1, ((numbPoints, 1))) &lt; p;  # points to be retained\n\n    # x/y locations of retained points\n    xxRetained = xx[booleRetained];\n    yyRetained = yy[booleRetained];\n    numbPointsRetained[ii] = xxRetained.size;\n\n# Plotting\nplt.scatter(xxRetained, yyRetained, edgecolor='b', facecolor='none', alpha=0.5);\nplt.xlabel('x');\nplt.ylabel('y');\nplt.xlim([xMin, xMax]);\nplt.ylim([xMin, xMax]);\n\n\n\n\n\n\n\n\n\nsource\nThat pattern comes from a spatially-explicit thinning of a CSR pattern:\n\n\nCode\n# Plotting\nplt.scatter(xx, yy, edgecolor='b', facecolor='none', alpha=0.5);\nplt.xlabel('x');\nplt.ylabel('y');",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#regular-processes",
    "href": "week-14/point-processes.html#regular-processes",
    "title": "Point Processes",
    "section": "Regular Processes",
    "text": "Regular Processes\n\nLess grouped than CSR\n\nfewer high densities\ndispersed\nrepulsion, competition\n\n\n\nUnderdispersion\n\nvariance &lt; mean\nless variation in densities than CSR",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#inhibition-process",
    "href": "week-14/point-processes.html#inhibition-process",
    "title": "Point Processes",
    "section": "Inhibition Process",
    "text": "Inhibition Process\n\nMinimum Permissible Distance\n\nno two points closer than \\(\\delta\\)\npacking intensity \\(\\tau = \\lambda \\pi \\delta^2 / 4\\)\n\n\n\nMatern Process\n\nthinned Poisson process using \\(\\delta\\)\nsequential inhibition process, generate points conditional on previous points and distance (denser than the thinned approach)",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#matern-thinning",
    "href": "week-14/point-processes.html#matern-thinning",
    "title": "Point Processes",
    "section": "Matern (Thinning)",
    "text": "Matern (Thinning)\n\nnp.random.seed(12345)\ndelta = 0.1\nn = 20\nxy = np.random.random((n,2))\nxy\nfrom scipy.spatial import distance_matrix\n\nd = distance_matrix(xy, xy) # 20 x 20 distance matrix\nd[0] # first row\n\narray([0.        , 0.75403388, 0.4570564 , 0.33860475, 0.38256491,\n       0.67009276, 0.94484483, 0.716711  , 0.56856568, 0.40881349,\n       0.49326812, 0.46210886, 0.64101492, 0.36620498, 0.20105384,\n       1.02432357, 0.29284635, 0.4855703 , 0.42540863, 0.41333518])\n\n\nDetermine which observations to thin\n\nijs = np.where(d&lt;delta)\ni,j = ijs\npairs = list(zip(i[i!=j], j[i!=j]))\nprint(\"The pairs within delta of one another:\")\nprint(pairs)\ndrop = []\n\nfor left, right in pairs:\n    if left in drop or right in drop:\n        continue\n    else:\n        drop.append(left)\n        \nprint(\"Observations to drop:\")\nprint(drop)\n\nThe pairs within delta of one another:\n[(3, 9), (3, 13), (9, 3), (9, 13), (9, 19), (13, 3), (13, 9), (19, 9)]\nObservations to drop:\n[3, 9]\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\ndf['thin'] = False\ndf.iloc[drop, df.columns.get_loc('thin')] = True\ndf.head()\n\n\n\n\n\n\n\n\n\nx\ny\nthin\n\n\n\n\n0\n0.929616\n0.316376\nFalse\n\n\n1\n0.183919\n0.204560\nFalse\n\n\n2\n0.567725\n0.595545\nFalse\n\n\n3\n0.964515\n0.653177\nTrue\n\n\n4\n0.748907\n0.653570\nFalse\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.scatterplot(x='x', y='y', hue='thin', data=df);",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#matern-sequential",
    "href": "week-14/point-processes.html#matern-sequential",
    "title": "Point Processes",
    "section": "Matern (Sequential)",
    "text": "Matern (Sequential)\n\ndelta = 0.1\nN = 20\nn = 1\nxy = np.zeros((N,2))\nxy[0,:] = np.random.rand(1,2)\nwhile n &lt; N:\n    candidate = np.random.rand(1,2)\n    d = distance_matrix(xy[:n,:], candidate)\n    if d.min() &gt; delta:\n        xy[n,:] = candidate\n        n += 1\n\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-14/point-processes.html#csr-n20",
    "href": "week-14/point-processes.html#csr-n20",
    "title": "Point Processes",
    "section": "CSR n=20",
    "text": "CSR n=20\n\ndelta = 0.1\nxy = np.random.rand(20,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);",
    "crumbs": [
      "Week 14 4/23 4/25",
      "Point Processes"
    ]
  },
  {
    "objectID": "week-08/02_spatial_weights.html",
    "href": "week-08/02_spatial_weights.html",
    "title": "Spatial Weights",
    "section": "",
    "text": "Spatial weights are mathematical structures used to represent spatial relationships. Many spatial analytics, such as spatial autocorrelation statistics and regionalization algorithms rely on spatial weights. Generally speaking, a spatial weight \\(w_{i,j}\\) expresses the notion of a geographical relationship between locations \\(i\\) and \\(j\\). These relationships can be based on a number of criteria including contiguity, geospatial distance and general distances.\nlibpysal offers functionality for the construction, manipulation, analysis, and conversion of a wide array of spatial weights.\nWe begin with construction of weights from common spatial data formats.\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nimport libpysal \nfrom libpysal.weights import Queen, Rook, KNN, Kernel, DistanceBand\nimport numpy as np\nimport geopandas\nimport pandas\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nfrom splot.libpysal import plot_spatial_weights\n\nThere are functions to construct weights directly from a file path.\n\n\n\n\n\n\nA commonly-used type of weight is a queen contigutiy weight, which reflects adjacency relationships as a binary indicator variable denoting whether or not a polygon shares an edge or a vertex with another polygon. These weights are symmetric, in that when polygon \\(A\\) neighbors polygon \\(B\\), both \\(w_{AB} = 1\\) and \\(w_{BA} = 1\\).\nTo construct queen weights from a shapefile, we will use geopandas to read the file into a GeoDataFrame, and then use libpysal to construct the weights:\n\npath = \"~/data/scag_region.parquet\"\ndf = geopandas.read_parquet(path)\ndf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\ndf = df.to_crs(26911)  #UTM zone 11N\n\n\nqW = Queen.from_dataframe(df)\n\n\nqW\n\n&lt;libpysal.weights.contiguity.Queen at 0x7d1d8341d930&gt;\n\n\nAll weights objects have a few traits that you can use to work with the weights object, as well as to get information about the weights object.\nTo get the neighbors & weights around an observation, use the observation’s index on the weights object, like a dictionary:\n\nqW[155] #neighbors & weights of the 156th observation (0-index remember)\n\n{4528: 1.0, 547: 1.0, 2133: 1.0, 2744: 1.0}\n\n\nBy default, the weights and the pandas dataframe will use the same index. So, we can view the observation and its neighbors in the dataframe by putting the observation’s index and its neighbors’ indexes together in one list:\n\nself_and_neighbors = [155]\nself_and_neighbors.extend(qW.neighbors[155])\nprint(self_and_neighbors)\n\n[155, 4528, 547, 2133, 2744]\n\n\nand grabbing those elements from the dataframe:\n\ndf.loc[self_and_neighbors]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n155\n06037552302\n26.0\n80.0\n630.0\n0.0\n0.0\n913.0\nNaN\nNaN\nNaN\n...\n2010\n1268.0\n22.060410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401577.943 3752226.320, 401578.535 3...\n\n\n4528\n06037552400\n0.0\n20.0\n670.0\n0.0\n24.0\n821.0\nNaN\nNaN\nNaN\n...\n2010\n588.0\n5.576363\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401878.957 3751827.308, 402203.594 3...\n\n\n547\n06037552301\n59.0\n103.0\n1079.0\n5.0\n0.0\n1777.0\nNaN\nNaN\nNaN\n...\n2010\n1272.0\n8.472352\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400855.726 3753114.791, 400857.855 3...\n\n\n2133\n06037552200\n38.0\n141.0\n1484.0\n0.0\n52.0\n2235.0\nNaN\nNaN\nNaN\n...\n2010\n1902.0\n6.858581\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399998.860 3752819.258, 400004.819 3...\n\n\n2744\n06037504102\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n...\n2010\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401631.531 3751829.535, 401878.957 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\nA full, dense matrix describing all of the pairwise relationships is constructed using the .full method, or when libpysal.weights.full is called on a weights object:\n\nWmatrix, ids = qW.full()\n#Wmatrix, ids = libpysal.weights.full(qW)\n\n\nWmatrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nn_neighbors = Wmatrix.sum(axis=1) # how many neighbors each region has\n\n\nn_neighbors[155]\n\n4.0\n\n\n\nqW.cardinalities[155]\n\n4\n\n\nNote that this matrix is binary, in that its elements are either zero or one, since an observation is either a neighbor or it is not a neighbor.\nHowever, many common use cases of spatial weights require that the matrix is row-standardized. This is done simply in PySAL using the .transform attribute\n\nqW.transform = 'r'\n\n('WARNING: ', 4285, ' is an island (no neighbors)')\n\n\nNow, if we build a new full matrix, its rows should sum to one:\n\nWmatrix, ids = qW.full()\n\n\nWmatrix.sum(axis=1) #numpy axes are 0:column, 1:row, 2:facet, into higher dimensions\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\nSince weight matrices are typically very sparse, there is also a sparse weights matrix constructor:\n\nqW.sparse\n\n&lt;4580x4580 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 29374 stored elements in Compressed Sparse Row format&gt;\n\n\n\nqW.pct_nonzero #Percentage of nonzero neighbor counts\n\n0.14003356152628668\n\n\nLet’s look at the neighborhoods of the 101th observation\n\ndf.iloc[100]\n\ngeoid                                                        06037910606\nn_asian_under_15                                                     0.0\nn_black_under_15                                                   210.0\nn_hispanic_under_15                                                757.0\nn_native_under_15                                                    3.0\n                                             ...                        \np_hispanic_over_60                                                   NaN\np_native_over_60                                                     NaN\np_asian_over_60                                                      NaN\np_disabled                                                           NaN\ngeometry               POLYGON ((401275.2896923868 3825401.4434467247...\nName: 100, Length: 194, dtype: object\n\n\n\nqW.neighbors[100]\n\n[789, 790, 1991, 3676, 791]\n\n\n\nlen(qW.neighbors[100])\n\n5\n\n\n\ndf.iloc[qW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(qW, df)\n\n\n\n\n\n\n\n\nBy default, PySAL assigns each observation an index according to the order in which the observation was read in. This means that, by default, all of the observations in the weights object are indexed by table order.\n\npandas.Series(qW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\nqW.cardinalities.values()\n\ndict_values([9, 9, 4, 7, 7, 5, 5, 6, 5, 8, 9, 8, 4, 3, 5, 5, 6, 6, 4, 5, 5, 6, 7, 9, 6, 4, 7, 8, 7, 5, 7, 2, 6, 6, 8, 3, 7, 7, 5, 8, 6, 5, 5, 4, 6, 6, 7, 7, 4, 6, 7, 4, 5, 6, 13, 6, 7, 6, 8, 6, 6, 6, 2, 6, 6, 8, 6, 7, 7, 6, 3, 5, 6, 6, 3, 7, 6, 5, 5, 5, 8, 8, 6, 8, 9, 7, 7, 6, 7, 5, 5, 7, 7, 6, 5, 7, 8, 8, 4, 7, 5, 4, 4, 6, 9, 6, 6, 7, 4, 8, 6, 6, 5, 6, 6, 6, 6, 7, 6, 8, 6, 6, 6, 8, 6, 6, 5, 8, 4, 5, 7, 5, 5, 5, 5, 5, 2, 4, 4, 7, 6, 8, 6, 9, 4, 6, 7, 5, 5, 6, 6, 4, 8, 9, 7, 4, 8, 4, 6, 5, 5, 4, 5, 6, 7, 8, 4, 6, 5, 6, 6, 6, 4, 6, 7, 6, 5, 6, 7, 6, 7, 7, 7, 7, 3, 10, 6, 6, 7, 7, 5, 6, 7, 8, 6, 5, 9, 7, 9, 6, 6, 4, 6, 6, 5, 7, 7, 7, 6, 4, 7, 8, 7, 5, 6, 6, 4, 6, 6, 5, 9, 7, 5, 7, 4, 7, 7, 3, 6, 7, 5, 5, 6, 6, 5, 4, 6, 5, 5, 6, 5, 10, 4, 3, 6, 1, 8, 6, 4, 5, 5, 7, 6, 4, 7, 4, 5, 6, 6, 5, 10, 3, 5, 5, 9, 5, 7, 5, 5, 7, 5, 8, 4, 6, 5, 7, 7, 7, 5, 6, 7, 5, 3, 7, 5, 4, 6, 3, 5, 6, 5, 5, 5, 4, 4, 7, 7, 5, 5, 5, 7, 9, 6, 4, 4, 5, 7, 4, 4, 7, 4, 6, 6, 4, 8, 6, 7, 5, 8, 6, 7, 6, 8, 8, 4, 5, 7, 6, 3, 5, 5, 4, 6, 6, 7, 5, 5, 5, 3, 5, 7, 6, 8, 5, 5, 5, 7, 6, 6, 7, 3, 4, 8, 4, 7, 4, 6, 6, 4, 4, 4, 5, 3, 5, 6, 4, 6, 7, 8, 5, 6, 6, 6, 7, 5, 8, 5, 3, 5, 5, 6, 4, 7, 7, 7, 4, 6, 5, 6, 9, 4, 7, 5, 5, 5, 7, 4, 7, 8, 7, 7, 6, 5, 5, 7, 5, 9, 5, 5, 6, 4, 8, 6, 4, 5, 5, 5, 7, 6, 6, 3, 6, 7, 5, 4, 5, 5, 7, 7, 5, 4, 6, 6, 6, 5, 5, 5, 17, 7, 6, 6, 14, 5, 5, 6, 6, 4, 11, 7, 6, 4, 6, 5, 7, 6, 6, 4, 7, 4, 6, 4, 7, 7, 6, 8, 5, 6, 8, 6, 5, 8, 8, 6, 4, 5, 5, 3, 6, 4, 6, 6, 6, 5, 6, 4, 4, 6, 7, 7, 8, 4, 5, 17, 7, 5, 6, 5, 7, 8, 6, 4, 7, 5, 7, 6, 4, 4, 5, 5, 4, 4, 4, 5, 3, 5, 4, 5, 3, 4, 5, 7, 7, 4, 7, 5, 4, 5, 12, 9, 9, 5, 4, 6, 7, 6, 6, 3, 8, 4, 8, 4, 6, 8, 4, 6, 5, 9, 6, 7, 4, 4, 8, 7, 4, 8, 5, 5, 7, 6, 5, 7, 4, 8, 6, 5, 8, 6, 6, 6, 6, 4, 5, 6, 8, 5, 8, 5, 8, 5, 8, 8, 6, 8, 7, 6, 7, 6, 5, 5, 7, 9, 3, 6, 8, 8, 7, 8, 5, 4, 6, 5, 6, 9, 7, 6, 7, 8, 3, 6, 6, 4, 7, 6, 5, 3, 5, 5, 8, 6, 8, 3, 8, 6, 8, 6, 6, 5, 8, 4, 8, 8, 7, 4, 5, 6, 6, 7, 7, 7, 5, 7, 4, 8, 6, 8, 8, 6, 4, 5, 7, 6, 6, 7, 5, 7, 8, 4, 6, 7, 6, 6, 8, 7, 7, 4, 4, 8, 7, 7, 8, 11, 6, 7, 6, 10, 5, 6, 6, 6, 4, 5, 5, 8, 7, 4, 7, 6, 8, 6, 7, 6, 7, 7, 6, 6, 5, 6, 6, 6, 5, 7, 9, 4, 5, 6, 6, 5, 6, 7, 5, 10, 5, 7, 4, 5, 4, 5, 5, 5, 6, 10, 6, 6, 6, 5, 6, 7, 4, 4, 6, 6, 8, 6, 7, 8, 8, 6, 3, 5, 6, 5, 7, 5, 5, 7, 8, 6, 7, 5, 5, 6, 6, 6, 5, 6, 6, 8, 5, 8, 4, 6, 8, 4, 5, 3, 5, 5, 8, 5, 8, 7, 8, 7, 6, 10, 5, 7, 4, 4, 3, 9, 4, 6, 5, 4, 7, 7, 6, 9, 4, 6, 5, 5, 7, 5, 5, 9, 5, 6, 6, 5, 6, 5, 7, 5, 8, 8, 6, 7, 5, 7, 5, 5, 7, 7, 4, 7, 8, 6, 8, 7, 5, 7, 10, 5, 4, 7, 5, 6, 7, 7, 5, 5, 4, 6, 5, 6, 6, 7, 5, 7, 7, 6, 6, 5, 6, 4, 4, 6, 5, 6, 5, 6, 6, 5, 4, 5, 9, 9, 4, 5, 8, 6, 8, 5, 7, 7, 6, 6, 4, 9, 8, 6, 10, 5, 7, 6, 3, 6, 6, 8, 7, 5, 7, 5, 6, 8, 5, 6, 5, 5, 7, 4, 5, 4, 4, 5, 4, 5, 7, 5, 7, 6, 5, 7, 7, 6, 7, 6, 7, 7, 6, 7, 4, 4, 6, 7, 4, 6, 7, 8, 6, 8, 8, 2, 7, 4, 9, 5, 7, 7, 4, 6, 6, 4, 5, 5, 5, 9, 6, 8, 6, 9, 9, 7, 7, 5, 7, 5, 5, 5, 5, 4, 7, 6, 5, 6, 9, 6, 4, 6, 5, 6, 7, 6, 6, 4, 5, 3, 7, 4, 5, 6, 10, 4, 7, 6, 4, 6, 6, 8, 5, 6, 6, 7, 6, 6, 4, 7, 6, 7, 8, 6, 6, 7, 9, 6, 6, 9, 6, 8, 3, 5, 7, 8, 9, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 7, 3, 6, 6, 5, 9, 5, 8, 7, 5, 6, 5, 7, 7, 7, 6, 6, 8, 6, 5, 6, 7, 3, 6, 6, 8, 7, 6, 8, 5, 7, 6, 5, 6, 7, 4, 4, 7, 5, 20, 6, 5, 6, 6, 6, 7, 5, 6, 7, 5, 4, 5, 4, 9, 4, 7, 6, 8, 7, 5, 5, 5, 5, 7, 5, 6, 6, 5, 3, 5, 4, 6, 8, 6, 2, 7, 5, 5, 5, 6, 8, 7, 7, 7, 8, 7, 6, 5, 5, 6, 5, 6, 5, 7, 6, 7, 4, 6, 4, 5, 7, 5, 9, 6, 7, 7, 5, 10, 8, 5, 5, 5, 7, 6, 6, 7, 6, 7, 6, 5, 4, 5, 5, 6, 7, 10, 4, 4, 6, 6, 8, 7, 5, 6, 6, 9, 5, 5, 6, 6, 8, 5, 7, 6, 4, 9, 7, 5, 5, 6, 5, 16, 5, 4, 6, 4, 8, 8, 6, 6, 7, 7, 5, 10, 7, 6, 7, 6, 7, 5, 7, 5, 7, 6, 7, 6, 4, 8, 6, 7, 6, 6, 5, 4, 4, 5, 9, 4, 8, 8, 7, 6, 6, 11, 6, 4, 6, 6, 4, 7, 5, 6, 7, 5, 5, 3, 7, 5, 5, 3, 6, 6, 9, 5, 8, 8, 3, 6, 5, 5, 8, 5, 5, 7, 5, 9, 8, 9, 6, 6, 6, 7, 8, 8, 7, 6, 7, 9, 4, 8, 9, 9, 7, 7, 8, 6, 5, 6, 5, 5, 6, 5, 5, 8, 6, 7, 5, 7, 4, 7, 5, 10, 7, 8, 5, 6, 9, 9, 9, 10, 4, 5, 6, 9, 4, 5, 6, 6, 5, 6, 8, 9, 9, 9, 10, 7, 9, 5, 3, 6, 7, 7, 9, 7, 8, 9, 8, 7, 8, 6, 7, 6, 9, 9, 5, 8, 3, 3, 7, 5, 7, 9, 9, 8, 7, 7, 10, 6, 8, 7, 6, 8, 6, 8, 5, 5, 5, 4, 5, 7, 8, 9, 7, 3, 6, 4, 7, 7, 7, 6, 10, 6, 7, 8, 7, 10, 9, 7, 6, 8, 5, 7, 7, 6, 5, 5, 5, 7, 5, 6, 4, 5, 6, 4, 9, 7, 4, 6, 4, 9, 9, 6, 8, 5, 9, 6, 9, 6, 7, 7, 6, 5, 10, 10, 7, 7, 7, 7, 8, 6, 3, 7, 4, 5, 6, 4, 10, 9, 5, 8, 6, 9, 5, 7, 5, 7, 5, 8, 8, 9, 9, 13, 7, 7, 8, 6, 1, 7, 6, 5, 6, 7, 9, 5, 7, 6, 4, 4, 3, 7, 8, 6, 7, 8, 6, 4, 6, 9, 8, 6, 6, 6, 8, 7, 7, 5, 5, 2, 3, 7, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 6, 9, 4, 5, 5, 3, 5, 10, 5, 15, 4, 6, 9, 8, 8, 7, 5, 8, 7, 8, 9, 8, 7, 5, 6, 8, 4, 9, 6, 9, 6, 6, 6, 5, 7, 10, 11, 11, 6, 5, 6, 7, 4, 5, 11, 16, 5, 7, 9, 7, 6, 7, 7, 6, 5, 9, 6, 10, 7, 5, 7, 6, 5, 3, 8, 3, 11, 5, 5, 9, 9, 7, 6, 6, 8, 10, 6, 10, 4, 8, 7, 4, 7, 4, 5, 9, 4, 5, 9, 9, 6, 6, 8, 9, 9, 4, 7, 10, 16, 7, 11, 7, 11, 5, 8, 9, 5, 11, 7, 5, 7, 5, 5, 7, 8, 9, 5, 11, 11, 7, 8, 6, 5, 6, 5, 7, 9, 4, 7, 8, 7, 8, 4, 6, 7, 9, 9, 8, 6, 4, 7, 4, 6, 7, 6, 8, 6, 7, 4, 8, 5, 7, 7, 10, 7, 7, 6, 4, 6, 6, 13, 3, 8, 9, 6, 4, 6, 5, 6, 12, 5, 2, 6, 5, 7, 6, 7, 8, 5, 7, 4, 6, 7, 10, 3, 17, 10, 8, 6, 5, 7, 14, 1, 12, 8, 4, 4, 8, 8, 4, 8, 6, 8, 6, 5, 3, 8, 6, 7, 8, 6, 9, 8, 5, 11, 5, 3, 7, 5, 4, 5, 3, 7, 7, 6, 5, 8, 2, 5, 6, 11, 8, 6, 6, 7, 6, 7, 6, 6, 4, 4, 6, 9, 6, 9, 6, 5, 4, 3, 5, 8, 7, 10, 4, 6, 7, 4, 6, 6, 4, 6, 4, 8, 6, 10, 8, 5, 8, 6, 4, 6, 6, 5, 11, 4, 7, 8, 7, 6, 5, 5, 10, 16, 8, 5, 7, 9, 3, 7, 5, 3, 4, 8, 6, 6, 7, 6, 9, 7, 7, 6, 6, 11, 7, 7, 8, 6, 6, 7, 4, 7, 6, 3, 7, 7, 6, 8, 15, 7, 6, 4, 6, 9, 6, 6, 6, 10, 7, 8, 4, 9, 11, 6, 6, 6, 4, 11, 6, 5, 5, 5, 5, 7, 5, 5, 5, 4, 4, 8, 4, 6, 5, 7, 8, 6, 7, 5, 6, 7, 7, 6, 4, 10, 5, 6, 7, 5, 4, 7, 5, 6, 5, 7, 5, 5, 10, 6, 5, 5, 3, 7, 5, 6, 6, 6, 7, 5, 4, 7, 6, 11, 6, 8, 6, 9, 5, 14, 4, 5, 13, 8, 6, 7, 4, 7, 5, 9, 5, 8, 4, 7, 7, 8, 7, 4, 7, 5, 10, 8, 9, 5, 7, 6, 4, 7, 8, 8, 12, 7, 6, 8, 10, 5, 7, 6, 6, 6, 5, 5, 5, 6, 9, 5, 7, 4, 7, 8, 5, 7, 6, 5, 7, 7, 4, 4, 6, 7, 7, 6, 8, 6, 5, 4, 6, 5, 5, 4, 6, 6, 6, 7, 5, 4, 6, 3, 6, 7, 1, 7, 11, 4, 4, 12, 7, 6, 8, 5, 7, 8, 3, 6, 5, 5, 5, 9, 6, 9, 6, 5, 3, 7, 6, 8, 8, 9, 7, 5, 7, 6, 6, 5, 5, 10, 4, 8, 5, 8, 4, 5, 6, 5, 14, 8, 1, 5, 6, 7, 8, 10, 4, 7, 5, 9, 5, 5, 7, 4, 4, 7, 5, 8, 6, 9, 6, 8, 7, 4, 6, 4, 7, 5, 4, 6, 3, 7, 6, 6, 7, 11, 8, 6, 7, 8, 5, 7, 7, 9, 6, 5, 6, 6, 8, 6, 9, 7, 7, 7, 8, 5, 5, 5, 9, 6, 5, 4, 7, 7, 5, 7, 5, 5, 6, 8, 6, 5, 6, 4, 6, 6, 7, 7, 5, 6, 9, 6, 5, 6, 5, 8, 9, 7, 11, 7, 11, 15, 6, 7, 8, 3, 10, 8, 10, 8, 8, 4, 6, 5, 7, 6, 5, 6, 6, 7, 5, 9, 9, 9, 7, 11, 6, 6, 6, 5, 6, 6, 8, 6, 7, 5, 7, 8, 10, 10, 9, 7, 3, 9, 10, 7, 7, 6, 10, 6, 6, 7, 4, 7, 4, 5, 6, 6, 5, 5, 7, 8, 7, 4, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 8, 7, 7, 5, 8, 10, 8, 7, 7, 6, 6, 8, 7, 6, 5, 6, 6, 9, 6, 2, 7, 7, 7, 7, 6, 7, 7, 4, 9, 12, 5, 5, 5, 5, 4, 9, 5, 7, 6, 7, 6, 4, 8, 7, 7, 7, 3, 5, 6, 5, 5, 7, 8, 6, 9, 4, 5, 6, 7, 5, 4, 6, 9, 5, 6, 10, 10, 6, 8, 5, 6, 4, 7, 8, 8, 5, 4, 9, 6, 4, 6, 5, 7, 5, 7, 6, 4, 10, 5, 6, 5, 5, 4, 6, 4, 6, 16, 8, 3, 7, 6, 5, 5, 5, 7, 7, 6, 9, 7, 6, 6, 6, 8, 3, 8, 4, 5, 5, 6, 6, 8, 9, 4, 7, 8, 7, 5, 6, 6, 6, 12, 4, 5, 6, 7, 7, 4, 8, 5, 7, 6, 7, 7, 9, 8, 7, 6, 7, 7, 6, 7, 5, 6, 3, 6, 5, 3, 6, 10, 7, 6, 6, 6, 6, 13, 10, 6, 8, 4, 6, 5, 7, 8, 6, 7, 5, 8, 7, 7, 8, 13, 5, 10, 7, 6, 7, 7, 6, 7, 7, 7, 8, 4, 9, 7, 4, 5, 4, 6, 8, 8, 6, 10, 3, 5, 10, 8, 6, 6, 11, 7, 6, 6, 5, 5, 8, 4, 6, 4, 13, 4, 11, 7, 5, 7, 6, 6, 7, 5, 5, 6, 5, 10, 5, 5, 8, 10, 10, 6, 6, 7, 6, 8, 5, 5, 2, 5, 5, 11, 6, 6, 8, 13, 2, 3, 5, 4, 6, 5, 4, 5, 5, 5, 11, 5, 8, 7, 8, 7, 5, 6, 5, 6, 10, 3, 9, 5, 4, 6, 6, 9, 8, 6, 9, 6, 7, 5, 6, 3, 6, 9, 8, 7, 7, 4, 5, 8, 5, 8, 8, 7, 6, 8, 14, 6, 4, 7, 3, 9, 5, 6, 5, 5, 6, 7, 3, 9, 9, 5, 6, 6, 4, 4, 9, 7, 5, 4, 5, 15, 8, 7, 9, 6, 6, 5, 7, 6, 8, 4, 4, 5, 5, 3, 5, 3, 4, 4, 4, 7, 12, 8, 9, 9, 6, 3, 6, 4, 7, 7, 9, 4, 6, 9, 5, 7, 5, 10, 5, 10, 6, 9, 4, 6, 8, 5, 8, 12, 10, 5, 7, 6, 7, 10, 7, 9, 6, 7, 5, 6, 6, 8, 6, 6, 8, 4, 6, 6, 9, 6, 6, 7, 4, 4, 3, 8, 10, 6, 6, 25, 8, 8, 5, 5, 4, 7, 7, 5, 7, 6, 7, 7, 6, 6, 5, 8, 6, 6, 7, 6, 8, 5, 4, 5, 8, 6, 12, 6, 7, 8, 4, 4, 7, 7, 9, 9, 14, 3, 10, 6, 6, 5, 7, 14, 5, 8, 4, 8, 8, 6, 6, 4, 6, 10, 14, 8, 5, 7, 6, 9, 5, 6, 7, 7, 5, 7, 5, 6, 9, 6, 6, 8, 7, 3, 6, 5, 9, 4, 4, 6, 13, 4, 6, 4, 5, 5, 7, 6, 7, 14, 3, 5, 11, 6, 7, 7, 7, 5, 5, 6, 14, 7, 7, 7, 5, 3, 4, 8, 4, 6, 8, 2, 6, 10, 5, 12, 8, 9, 6, 5, 13, 6, 8, 5, 2, 5, 1, 5, 6, 5, 5, 4, 9, 6, 7, 3, 8, 5, 6, 7, 6, 7, 8, 7, 3, 8, 6, 7, 5, 7, 7, 6, 5, 7, 11, 9, 6, 6, 3, 4, 9, 8, 8, 8, 6, 5, 6, 5, 7, 15, 8, 10, 9, 10, 6, 5, 7, 6, 10, 6, 5, 12, 5, 5, 8, 8, 9, 4, 7, 4, 4, 8, 15, 6, 4, 12, 6, 6, 4, 6, 6, 8, 4, 7, 8, 6, 6, 8, 4, 5, 9, 7, 6, 6, 7, 7, 6, 6, 7, 9, 6, 7, 6, 8, 5, 5, 5, 10, 8, 6, 6, 7, 5, 6, 6, 6, 8, 8, 7, 6, 8, 5, 6, 7, 7, 7, 4, 7, 6, 6, 4, 11, 4, 7, 6, 5, 9, 10, 8, 6, 7, 6, 7, 6, 6, 7, 4, 7, 7, 5, 7, 7, 6, 9, 6, 6, 6, 7, 8, 4, 5, 3, 7, 5, 6, 8, 6, 6, 6, 16, 6, 5, 15, 10, 6, 7, 9, 7, 7, 8, 5, 9, 6, 5, 5, 4, 9, 11, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 8, 5, 4, 3, 8, 5, 4, 4, 5, 5, 6, 5, 3, 10, 8, 5, 9, 9, 6, 5, 3, 5, 6, 7, 7, 8, 5, 8, 6, 6, 4, 5, 4, 5, 9, 9, 4, 7, 5, 6, 9, 7, 4, 8, 8, 7, 6, 10, 7, 8, 11, 5, 7, 5, 6, 7, 9, 8, 7, 7, 8, 10, 3, 4, 6, 7, 7, 5, 7, 6, 5, 6, 9, 10, 3, 7, 5, 7, 8, 9, 5, 6, 2, 9, 7, 7, 4, 6, 6, 9, 7, 9, 6, 6, 7, 6, 7, 6, 5, 7, 8, 8, 5, 6, 6, 8, 6, 6, 6, 8, 7, 8, 6, 6, 8, 5, 7, 8, 4, 6, 5, 7, 8, 7, 7, 9, 5, 1, 8, 7, 5, 5, 7, 8, 5, 8, 6, 6, 4, 6, 6, 6, 7, 5, 5, 4, 7, 7, 7, 7, 6, 7, 5, 6, 6, 7, 6, 5, 6, 7, 9, 6, 7, 7, 4, 7, 7, 5, 6, 8, 5, 7, 7, 7, 7, 6, 7, 6, 14, 6, 9, 9, 10, 5, 6, 7, 7, 9, 4, 8, 10, 4, 8, 6, 5, 5, 6, 6, 6, 7, 7, 6, 3, 5, 10, 8, 9, 6, 8, 1, 4, 6, 7, 5, 7, 8, 7, 7, 7, 6, 5, 7, 5, 4, 10, 7, 9, 5, 8, 10, 7, 5, 9, 6, 6, 7, 8, 7, 7, 6, 7, 5, 6, 5, 8, 9, 6, 6, 6, 10, 5, 6, 7, 6, 8, 6, 7, 7, 6, 9, 5, 8, 4, 5, 7, 6, 5, 4, 4, 7, 8, 5, 5, 7, 5, 8, 7, 9, 6, 10, 6, 8, 8, 6, 7, 6, 6, 7, 9, 6, 5, 5, 8, 6, 7, 10, 6, 4, 14, 3, 6, 6, 8, 8, 7, 9, 6, 6, 10, 6, 8, 7, 6, 8, 6, 4, 5, 5, 5, 8, 4, 4, 6, 4, 6, 11, 4, 5, 6, 4, 6, 7, 5, 5, 3, 5, 6, 6, 9, 8, 9, 6, 6, 6, 6, 6, 8, 7, 7, 6, 4, 6, 5, 8, 6, 6, 5, 7, 8, 5, 7, 7, 6, 7, 3, 7, 4, 6, 5, 6, 5, 6, 6, 7, 10, 6, 6, 9, 7, 14, 6, 5, 7, 9, 8, 8, 9, 5, 4, 9, 4, 6, 5, 6, 8, 7, 6, 5, 5, 6, 5, 6, 7, 6, 6, 7, 7, 7, 6, 7, 6, 7, 6, 11, 6, 7, 7, 5, 4, 7, 6, 9, 7, 3, 6, 8, 5, 6, 3, 6, 7, 6, 3, 7, 11, 5, 7, 6, 6, 5, 6, 5, 7, 6, 8, 8, 9, 6, 6, 6, 7, 7, 10, 7, 9, 7, 6, 9, 8, 7, 6, 6, 5, 7, 8, 5, 7, 6, 6, 6, 9, 7, 4, 8, 7, 8, 7, 6, 6, 10, 5, 8, 4, 8, 5, 6, 7, 7, 7, 8, 7, 6, 9, 6, 7, 5, 4, 5, 7, 7, 5, 7, 7, 6, 6, 5, 7, 6, 5, 7, 5, 6, 8, 8, 5, 5, 5, 10, 6, 5, 6, 4, 9, 5, 6, 9, 8, 8, 7, 6, 8, 10, 6, 11, 6, 5, 10, 6, 8, 8, 7, 6, 10, 5, 7, 9, 6, 4, 8, 9, 7, 6, 6, 6, 5, 10, 6, 9, 5, 7, 7, 8, 6, 7, 5, 7, 6, 4, 7, 6, 8, 8, 5, 7, 6, 4, 7, 4, 5, 8, 5, 5, 9, 8, 5, 8, 6, 4, 7, 5, 3, 6, 6, 4, 7, 3, 5, 4, 4, 7, 8, 8, 7, 10, 8, 6, 6, 7, 5, 6, 5, 7, 8, 4, 7, 9, 5, 7, 5, 5, 7, 7, 7, 5, 5, 5, 6, 9, 6, 6, 8, 7, 5, 5, 7, 6, 8, 6, 8, 10, 9, 7, 8, 5, 6, 6, 7, 6, 8, 7, 6, 8, 5, 7, 5, 6, 7, 6, 5, 8, 5, 7, 3, 7, 6, 7, 8, 8, 4, 6, 2, 8, 6, 6, 5, 7, 5, 8, 8, 9, 7, 7, 7, 8, 7, 8, 6, 5, 8, 11, 10, 7, 7, 4, 6, 8, 5, 4, 8, 3, 5, 6, 8, 9, 7, 4, 5, 8, 8, 5, 5, 6, 6, 6, 7, 9, 6, 6, 11, 4, 7, 5, 9, 9, 6, 8, 6, 6, 5, 5, 8, 7, 7, 5, 7, 6, 12, 7, 6, 7, 5, 1, 10, 5, 3, 7, 5, 5, 6, 6, 7, 8, 8, 7, 5, 3, 5, 7, 7, 8, 9, 4, 5, 8, 8, 6, 5, 7, 7, 6, 7, 5, 8, 11, 5, 5, 4, 5, 5, 1, 9, 6, 9, 9, 5, 6, 7, 7, 9, 6, 7, 7, 7, 5, 4, 5, 6, 6, 5, 6, 4, 6, 6, 5, 5, 3, 5, 7, 4, 6, 4, 8, 6, 6, 6, 6, 6, 7, 4, 3, 4, 12, 6, 6, 6, 6, 8, 6, 6, 7, 12, 8, 5, 11, 4, 6, 6, 5, 6, 7, 6, 5, 7, 7, 10, 6, 5, 7, 6, 5, 6, 6, 6, 6, 5, 10, 19, 7, 7, 8, 5, 6, 9, 6, 6, 12, 5, 4, 5, 3, 12, 4, 6, 4, 7, 4, 9, 4, 5, 3, 4, 7, 9, 6, 5, 7, 8, 5, 6, 5, 4, 8, 7, 5, 7, 5, 4, 7, 6, 4, 7, 5, 5, 7, 6, 7, 8, 11, 5, 5, 8, 3, 5, 4, 6, 6, 3, 7, 7, 5, 6, 9, 12, 7, 5, 5, 6, 9, 5, 7, 10, 6, 9, 5, 6, 6, 6, 7, 8, 6, 5, 7, 5, 7, 5, 5, 5, 8, 6, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 6, 10, 8, 7, 7, 5, 6, 6, 5, 12, 7, 8, 6, 6, 5, 8, 5, 5, 5, 5, 6, 5, 9, 10, 6, 6, 5, 4, 5, 4, 6, 6, 5, 7, 5, 7, 7, 4, 7, 5, 9, 6, 6, 4, 6, 6, 6, 5, 6, 6, 6, 6, 4, 8, 3, 5, 9, 7, 9, 6, 4, 12, 6, 7, 6, 7, 6, 8, 16, 7, 7, 5, 10, 7, 8, 6, 6, 7, 7, 11, 6, 11, 5, 6, 9, 5, 8, 6, 7, 5, 6, 7, 6, 8, 9, 7, 2, 10, 5, 7, 6, 7, 6, 6, 6, 6, 10, 4, 4, 6, 8, 6, 6, 9, 7, 6, 2, 6, 7, 5, 7, 5, 15, 8, 6, 8, 4, 6, 7, 7, 7, 8, 8, 7, 5, 5, 6, 5, 7, 7, 7, 5, 6, 9, 8, 9, 5, 6, 4, 6, 5, 5, 7, 5, 8, 5, 4, 5, 5, 7, 7, 7, 5, 6, 8, 9, 4, 6, 6, 11, 6, 8, 9, 6, 5, 6, 6, 8, 6, 5, 7, 6, 5, 7, 4, 9, 5, 4, 6, 6, 6, 3, 7, 6, 6, 5, 13, 7, 6, 8, 5, 7, 5, 5, 8, 7, 7, 7, 7, 6, 9, 10, 8, 9, 6, 6, 7, 10, 6, 7, 7, 6, 6, 6, 6, 5, 7, 7, 5, 6, 4, 6, 7, 9, 8, 4, 9, 5, 5, 6, 4, 7, 8, 9, 7, 2, 4, 7, 7, 7, 7, 7, 11, 8, 9, 6, 4, 6, 6, 5, 8, 7, 7, 6, 7, 8, 8, 8, 6, 6, 8, 9, 8, 9, 7, 7, 5, 7, 5, 9, 6, 4, 6, 7, 4, 9, 7, 4, 4, 5, 9, 10, 8, 10, 9, 5, 6, 6, 5, 7, 7, 6, 5, 6, 9, 8, 7, 4, 6, 6, 7, 4, 5, 7, 7, 5, 9, 6, 7, 6, 9, 9, 8, 7, 8, 5, 3, 4, 11, 14, 5, 5, 4, 3, 6, 6, 5, 6, 7, 6, 8, 8, 9, 7, 6, 2, 6, 6, 5, 5, 5, 3, 8, 8, 11, 7, 6, 7, 6, 6, 7, 8, 6, 7, 7, 6, 6, 8, 8, 5, 5, 16, 8, 6, 7, 9, 5, 8, 6, 5, 4, 7, 5, 6, 8, 7, 5, 6, 7, 6, 6, 7, 10, 7, 10, 9, 8, 7, 5, 8, 5, 8, 7, 7, 9, 9, 5, 7, 6, 6, 5, 7, 5, 6, 8, 7, 6, 6, 10, 6, 5, 5, 8, 5, 7, 5, 5, 7, 6, 6, 5, 6, 0, 6, 11, 5, 5, 9, 6, 5, 9, 5, 6, 6, 6, 6, 6, 7, 7, 11, 10, 4, 7, 5, 8, 8, 8, 5, 6, 6, 7, 8, 7, 8, 5, 5, 5, 6, 7, 7, 5, 8, 5, 5, 5, 10, 8, 5, 10, 5, 4, 5, 5, 6, 5, 9, 5, 4, 5, 7, 4, 4, 6, 15, 7, 6, 8, 7, 10, 7, 7, 5, 6, 7, 6, 6, 6, 8, 5, 6, 4, 2, 6, 5, 8, 8, 5, 6, 5, 7, 10, 7, 8, 7, 6, 8, 7, 11, 7, 6, 6, 8, 7, 10, 16, 6, 7, 6, 4, 5, 4, 4, 7, 8, 8, 5, 6, 4, 6, 3, 5, 9, 8, 4, 6, 4, 6, 3, 6, 9, 7, 8, 3, 14, 6, 8, 4, 7, 6, 5, 6, 4, 8, 5, 7, 7, 7, 7, 5, 5, 7, 5, 6, 6, 7, 7, 7, 7, 4, 11, 6, 7, 4, 3, 13, 5, 8, 9, 8, 6, 8, 7, 7, 6, 7, 8, 6, 6, 9, 5, 8, 6, 7, 11, 6, 5, 8, 7, 9, 5, 7, 5, 7, 8, 4, 4, 8, 5, 7, 5, 5, 6, 7, 9, 7, 6, 11, 8, 3, 5, 7, 11, 4, 6, 7, 7, 7, 6, 5, 5, 7, 5, 6, 6, 7, 6, 5, 7, 8, 6, 5, 7, 14, 10, 6, 7, 7, 5, 6, 11, 5, 6, 5, 10, 4, 3, 6, 5, 8, 6, 6, 6, 7, 7, 8, 5, 7, 6, 5, 8, 7, 9, 6, 5, 6, 8, 5, 7, 7, 7, 6, 7, 6, 8, 5, 7, 9, 8, 5, 7, 5, 8, 8, 9, 12, 7, 4, 7, 10, 8, 7, 7, 6, 6, 7, 6, 7])\n\n\n\n\n\nRook weights are another type of contiguity weight, but consider observations as neighboring only when they share an edge. The rook neighbors of an observation may be different than its queen neighbors, depending on how the observation and its nearby polygons are configured.\nWe can construct this in the same way as the queen weights:\n\nrW = Rook.from_dataframe(df)\n\n\nrW.neighbors[100]\n\n[789, 790, 1991, 791, 3676]\n\n\n\nlen(rW.neighbors[100])\n\n5\n\n\n\ndf.iloc[rW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(rW, df)\n\n\n\n\n\n\n\n\n\npandas.Series(rW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\n\n\nIn theory, a “Bishop” weighting scheme is one that arises when only polygons that share vertexes are considered to be neighboring. But, since Queen contiguigy requires either an edge or a vertex and Rook contiguity requires only shared edges, the following relationship is true:\n\\[ \\mathcal{Q} = \\mathcal{R} \\cup \\mathcal{B} \\]\nwhere \\(\\mathcal{Q}\\) is the set of neighbor pairs via queen contiguity, \\(\\mathcal{R}\\) is the set of neighbor pairs via Rook contiguity, and \\(\\mathcal{B}\\) via Bishop contiguity. Thus:\n\\[ \\mathcal{Q} \\setminus \\mathcal{R} = \\mathcal{B}\\]\nBishop weights entail all Queen neighbor pairs that are not also Rook neighbors.\nPySAL does not have a dedicated bishop weights constructor, but you can construct very easily using the w_difference function. This function is one of a family of tools to work with weights, all defined in libpysal.weights, that conduct these types of set operations between weight objects.\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW.histogram\n\n[(0, 1624), (1, 1728), (2, 881), (3, 292), (4, 55)]\n\n\nThus, many tracts have no bishop neighbors. But, a few do. A simple way to see these observations in the dataframe is to find all elements of the dataframe that are not “islands,” the term for an observation with no neighbors:\n\nplot_spatial_weights(bW, df)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many other kinds of weighting functions in PySAL. Another separate type use a continuous measure of distance to define neighborhoods.\n\ndf.crs\n\n&lt;Projected CRS: EPSG:26911&gt;\nName: NAD83 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: North America - between 120°W and 114°W - onshore and offshore. Canada - Alberta; British Columbia; Northwest Territories; Nunavut. United States (USA) - California; Idaho; Nevada, Oregon; Washington.\n- bounds: (-120.0, 30.88, -114.0, 83.5)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nOur coordinate system (UTM 11N) measures distance in meters, so that’s how we’ll define our neighbors\n\ndist_band = DistanceBand.from_dataframe(df, threshold=2000)\n\n\nplot_spatial_weights(dist_band,df)\n\n\n\n\n\n\n\n\n\n\n\nradius_mile = libpysal.cg.sphere.RADIUS_EARTH_MILES\nradius_mile\n\n3958.755865744055\n\n\n\ndf_latlong = df.to_crs(4326)\n\n\nknn8_bad = KNN.from_dataframe(df_latlong, k=8) # ignore curvature of the earth\n\n\nknn8_bad.histogram\n\n[(8, 4580)]\n\n\n\nknn8 = KNN.from_dataframe(df_latlong, k=8, radius=radius_mile)\n\n\nknn8.histogram\n\n[(8, 4580)]\n\n\n\nknn8_bad.neighbors[1487]\n\n[501, 2296, 2960, 974, 167, 4496, 2295, 4422]\n\n\n\nknn8.neighbors[1487]\n\n[501, 2960, 2296, 974, 167, 4496, 2881, 2297]\n\n\n\nset(knn8_bad.neighbors[1487]) == set(knn8.neighbors[1487])\n\nFalse\n\n\n\nExercise:  Enumerate the tracts for which ignoring curvature results in an incorrect neighbor set for knn.\n\n\n# %load solutions/02_knn.py\n\n\n\n\nKernel Weights are continuous distance-based weights that use kernel densities to define the neighbor relationship. Typically, they estimate a bandwidth, which is a parameter governing how far out observations should be considered neighboring. Then, using this bandwidth, they evaluate a continuous kernel function to provide a weight between 0 and 1.\nMany different choices of kernel functions are supported, and bandwidths can either be fixed (constant over all units) or adaptive in function of unit density.\nFor example, if we want to use adaptive bandwidths for the map and weight according to a gaussian kernel:\n\n\nbandwidth = the distance to the kth nearest neighbor for each observation\nbandwith is changing across observations\n\nkernelWa = Kernel.from_dataframe(df, k=10, fixed=False, function='gaussian')\n\n\nplot_spatial_weights(kernelWa, df)\n\n\n\n\n\n\n\n\n\nkernelWa.bandwidth\n\narray([[1687.99751736],\n       [1997.79636883],\n       [1803.3632643 ],\n       ...,\n       [2468.39103021],\n       [3480.79114847],\n       [1749.84752448]])\n\n\n\ndf.assign(bw=kernelWa.bandwidth.flatten()).plot('bw', cmap='Reds')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw,s,e,n = df.total_bounds\n\n\nmx = (w+e)/2\nmy = (n+s)/2\n\n\nimport shapely\n\n\ncentroids = df.geometry.centroid\n\n\nlon = centroids.apply(lambda p: p.x).values\nlat = centroids.apply(lambda p: p.y).values\n\n\nnorth = lat &gt; my\nsouth = lat &lt;= my\neast = lon &gt; mx\nwest = lon &lt;= mx\n\n\nnw = west * north * 2\nne = east * north * 1\nsw = west * south * 3\nse = east * south *4\nquad = nw + ne + sw + se\n\n\nquad\n\narray([3, 2, 2, ..., 2, 4, 2])\n\n\n\ndf['quad'] = quad\ndf.plot(column=\"quad\", categorical=True)\n\n\n\n\n\n\n\n\n\nblockW = libpysal.weights.block_weights(df[\"quad\"])\n\n\nblockW.n\n\n4580\n\n\n\nblockW.pct_nonzero\n\n65.53761369920483\n\n\n\npandas.Series(blockW.cardinalities).plot.hist()\n\n\n\n\n\n\n\n\n\ndf.groupby(by='quad').count()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\nquad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n15\n15\n15\n15\n15\n15\n15\n0\n0\n0\n...\n15\n15\n15\n0\n0\n0\n0\n0\n0\n15\n\n\n2\n761\n761\n761\n761\n761\n761\n761\n0\n0\n0\n...\n761\n761\n755\n0\n0\n0\n0\n0\n0\n761\n\n\n3\n3625\n3625\n3625\n3625\n3625\n3625\n3625\n0\n0\n0\n...\n3625\n3625\n3612\n0\n0\n0\n0\n0\n0\n3625\n\n\n4\n179\n179\n179\n179\n179\n179\n179\n0\n0\n0\n...\n179\n179\n179\n0\n0\n0\n0\n0\n0\n179\n\n\n\n\n4 rows × 194 columns\n\n\n\n\n\n#plot_spatial_weights(blockW, df)\n\n\nExercise:  Which spatial weights structure would be more dense, tracts based on rook contiguity or SoCal tracts based on knn with k=4?\n\n\nExercise:  How many tracts have fewer neighbors under rook contiguity relative to knn4?\n\n\nExercise:  How many tracts have identical neighbors under queen contiguity and queen rook contiguity?\n\n\n# %load solutions/02.py\n\n\nSpatial Weights by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Spatial Weights"
    ]
  },
  {
    "objectID": "week-08/02_spatial_weights.html#weight-types",
    "href": "week-08/02_spatial_weights.html#weight-types",
    "title": "Spatial Weights",
    "section": "",
    "text": "A commonly-used type of weight is a queen contigutiy weight, which reflects adjacency relationships as a binary indicator variable denoting whether or not a polygon shares an edge or a vertex with another polygon. These weights are symmetric, in that when polygon \\(A\\) neighbors polygon \\(B\\), both \\(w_{AB} = 1\\) and \\(w_{BA} = 1\\).\nTo construct queen weights from a shapefile, we will use geopandas to read the file into a GeoDataFrame, and then use libpysal to construct the weights:\n\npath = \"~/data/scag_region.parquet\"\ndf = geopandas.read_parquet(path)\ndf.head()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n0\n06037128702\n58.0\n0.0\n223.0\n0.0\n475.0\n986.0\nNaN\nNaN\nNaN\n...\n2010\n2903.0\n64.726214\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.44870 34.16485, -118.43997 34.1...\n\n\n1\n06037131600\n83.0\n62.0\n777.0\n0.0\n135.0\n1355.0\nNaN\nNaN\nNaN\n...\n2010\n1487.0\n28.679979\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.56229 34.22033, -118.55792 34.2...\n\n\n2\n06037134104\n287.0\n17.0\n816.0\n0.0\n61.0\n1323.0\nNaN\nNaN\nNaN\n...\n2010\n1388.0\n14.846188\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.57976 34.21558, -118.57539 34.2...\n\n\n3\n06037134304\n90.0\n24.0\n298.0\n0.0\n89.0\n520.0\nNaN\nNaN\nNaN\n...\n2010\n928.0\n33.378933\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.61472 34.21952, -118.61039 34.2...\n\n\n4\n06037242000\n0.0\n229.0\n681.0\n0.0\n0.0\n1164.0\nNaN\nNaN\nNaN\n...\n2010\n1054.0\n0.058565\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-118.25416 33.93882, -118.25413 33.9...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\ndf = df.to_crs(26911)  #UTM zone 11N\n\n\nqW = Queen.from_dataframe(df)\n\n\nqW\n\n&lt;libpysal.weights.contiguity.Queen at 0x7d1d8341d930&gt;\n\n\nAll weights objects have a few traits that you can use to work with the weights object, as well as to get information about the weights object.\nTo get the neighbors & weights around an observation, use the observation’s index on the weights object, like a dictionary:\n\nqW[155] #neighbors & weights of the 156th observation (0-index remember)\n\n{4528: 1.0, 547: 1.0, 2133: 1.0, 2744: 1.0}\n\n\nBy default, the weights and the pandas dataframe will use the same index. So, we can view the observation and its neighbors in the dataframe by putting the observation’s index and its neighbors’ indexes together in one list:\n\nself_and_neighbors = [155]\nself_and_neighbors.extend(qW.neighbors[155])\nprint(self_and_neighbors)\n\n[155, 4528, 547, 2133, 2744]\n\n\nand grabbing those elements from the dataframe:\n\ndf.loc[self_and_neighbors]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n155\n06037552302\n26.0\n80.0\n630.0\n0.0\n0.0\n913.0\nNaN\nNaN\nNaN\n...\n2010\n1268.0\n22.060410\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401577.943 3752226.320, 401578.535 3...\n\n\n4528\n06037552400\n0.0\n20.0\n670.0\n0.0\n24.0\n821.0\nNaN\nNaN\nNaN\n...\n2010\n588.0\n5.576363\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401878.957 3751827.308, 402203.594 3...\n\n\n547\n06037552301\n59.0\n103.0\n1079.0\n5.0\n0.0\n1777.0\nNaN\nNaN\nNaN\n...\n2010\n1272.0\n8.472352\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400855.726 3753114.791, 400857.855 3...\n\n\n2133\n06037552200\n38.0\n141.0\n1484.0\n0.0\n52.0\n2235.0\nNaN\nNaN\nNaN\n...\n2010\n1902.0\n6.858581\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399998.860 3752819.258, 400004.819 3...\n\n\n2744\n06037504102\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n...\n2010\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401631.531 3751829.535, 401878.957 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\nA full, dense matrix describing all of the pairwise relationships is constructed using the .full method, or when libpysal.weights.full is called on a weights object:\n\nWmatrix, ids = qW.full()\n#Wmatrix, ids = libpysal.weights.full(qW)\n\n\nWmatrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nn_neighbors = Wmatrix.sum(axis=1) # how many neighbors each region has\n\n\nn_neighbors[155]\n\n4.0\n\n\n\nqW.cardinalities[155]\n\n4\n\n\nNote that this matrix is binary, in that its elements are either zero or one, since an observation is either a neighbor or it is not a neighbor.\nHowever, many common use cases of spatial weights require that the matrix is row-standardized. This is done simply in PySAL using the .transform attribute\n\nqW.transform = 'r'\n\n('WARNING: ', 4285, ' is an island (no neighbors)')\n\n\nNow, if we build a new full matrix, its rows should sum to one:\n\nWmatrix, ids = qW.full()\n\n\nWmatrix.sum(axis=1) #numpy axes are 0:column, 1:row, 2:facet, into higher dimensions\n\narray([1., 1., 1., ..., 1., 1., 1.])\n\n\nSince weight matrices are typically very sparse, there is also a sparse weights matrix constructor:\n\nqW.sparse\n\n&lt;4580x4580 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 29374 stored elements in Compressed Sparse Row format&gt;\n\n\n\nqW.pct_nonzero #Percentage of nonzero neighbor counts\n\n0.14003356152628668\n\n\nLet’s look at the neighborhoods of the 101th observation\n\ndf.iloc[100]\n\ngeoid                                                        06037910606\nn_asian_under_15                                                     0.0\nn_black_under_15                                                   210.0\nn_hispanic_under_15                                                757.0\nn_native_under_15                                                    3.0\n                                             ...                        \np_hispanic_over_60                                                   NaN\np_native_over_60                                                     NaN\np_asian_over_60                                                      NaN\np_disabled                                                           NaN\ngeometry               POLYGON ((401275.2896923868 3825401.4434467247...\nName: 100, Length: 194, dtype: object\n\n\n\nqW.neighbors[100]\n\n[789, 790, 1991, 3676, 791]\n\n\n\nlen(qW.neighbors[100])\n\n5\n\n\n\ndf.iloc[qW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(qW, df)\n\n\n\n\n\n\n\n\nBy default, PySAL assigns each observation an index according to the order in which the observation was read in. This means that, by default, all of the observations in the weights object are indexed by table order.\n\npandas.Series(qW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\nqW.cardinalities.values()\n\ndict_values([9, 9, 4, 7, 7, 5, 5, 6, 5, 8, 9, 8, 4, 3, 5, 5, 6, 6, 4, 5, 5, 6, 7, 9, 6, 4, 7, 8, 7, 5, 7, 2, 6, 6, 8, 3, 7, 7, 5, 8, 6, 5, 5, 4, 6, 6, 7, 7, 4, 6, 7, 4, 5, 6, 13, 6, 7, 6, 8, 6, 6, 6, 2, 6, 6, 8, 6, 7, 7, 6, 3, 5, 6, 6, 3, 7, 6, 5, 5, 5, 8, 8, 6, 8, 9, 7, 7, 6, 7, 5, 5, 7, 7, 6, 5, 7, 8, 8, 4, 7, 5, 4, 4, 6, 9, 6, 6, 7, 4, 8, 6, 6, 5, 6, 6, 6, 6, 7, 6, 8, 6, 6, 6, 8, 6, 6, 5, 8, 4, 5, 7, 5, 5, 5, 5, 5, 2, 4, 4, 7, 6, 8, 6, 9, 4, 6, 7, 5, 5, 6, 6, 4, 8, 9, 7, 4, 8, 4, 6, 5, 5, 4, 5, 6, 7, 8, 4, 6, 5, 6, 6, 6, 4, 6, 7, 6, 5, 6, 7, 6, 7, 7, 7, 7, 3, 10, 6, 6, 7, 7, 5, 6, 7, 8, 6, 5, 9, 7, 9, 6, 6, 4, 6, 6, 5, 7, 7, 7, 6, 4, 7, 8, 7, 5, 6, 6, 4, 6, 6, 5, 9, 7, 5, 7, 4, 7, 7, 3, 6, 7, 5, 5, 6, 6, 5, 4, 6, 5, 5, 6, 5, 10, 4, 3, 6, 1, 8, 6, 4, 5, 5, 7, 6, 4, 7, 4, 5, 6, 6, 5, 10, 3, 5, 5, 9, 5, 7, 5, 5, 7, 5, 8, 4, 6, 5, 7, 7, 7, 5, 6, 7, 5, 3, 7, 5, 4, 6, 3, 5, 6, 5, 5, 5, 4, 4, 7, 7, 5, 5, 5, 7, 9, 6, 4, 4, 5, 7, 4, 4, 7, 4, 6, 6, 4, 8, 6, 7, 5, 8, 6, 7, 6, 8, 8, 4, 5, 7, 6, 3, 5, 5, 4, 6, 6, 7, 5, 5, 5, 3, 5, 7, 6, 8, 5, 5, 5, 7, 6, 6, 7, 3, 4, 8, 4, 7, 4, 6, 6, 4, 4, 4, 5, 3, 5, 6, 4, 6, 7, 8, 5, 6, 6, 6, 7, 5, 8, 5, 3, 5, 5, 6, 4, 7, 7, 7, 4, 6, 5, 6, 9, 4, 7, 5, 5, 5, 7, 4, 7, 8, 7, 7, 6, 5, 5, 7, 5, 9, 5, 5, 6, 4, 8, 6, 4, 5, 5, 5, 7, 6, 6, 3, 6, 7, 5, 4, 5, 5, 7, 7, 5, 4, 6, 6, 6, 5, 5, 5, 17, 7, 6, 6, 14, 5, 5, 6, 6, 4, 11, 7, 6, 4, 6, 5, 7, 6, 6, 4, 7, 4, 6, 4, 7, 7, 6, 8, 5, 6, 8, 6, 5, 8, 8, 6, 4, 5, 5, 3, 6, 4, 6, 6, 6, 5, 6, 4, 4, 6, 7, 7, 8, 4, 5, 17, 7, 5, 6, 5, 7, 8, 6, 4, 7, 5, 7, 6, 4, 4, 5, 5, 4, 4, 4, 5, 3, 5, 4, 5, 3, 4, 5, 7, 7, 4, 7, 5, 4, 5, 12, 9, 9, 5, 4, 6, 7, 6, 6, 3, 8, 4, 8, 4, 6, 8, 4, 6, 5, 9, 6, 7, 4, 4, 8, 7, 4, 8, 5, 5, 7, 6, 5, 7, 4, 8, 6, 5, 8, 6, 6, 6, 6, 4, 5, 6, 8, 5, 8, 5, 8, 5, 8, 8, 6, 8, 7, 6, 7, 6, 5, 5, 7, 9, 3, 6, 8, 8, 7, 8, 5, 4, 6, 5, 6, 9, 7, 6, 7, 8, 3, 6, 6, 4, 7, 6, 5, 3, 5, 5, 8, 6, 8, 3, 8, 6, 8, 6, 6, 5, 8, 4, 8, 8, 7, 4, 5, 6, 6, 7, 7, 7, 5, 7, 4, 8, 6, 8, 8, 6, 4, 5, 7, 6, 6, 7, 5, 7, 8, 4, 6, 7, 6, 6, 8, 7, 7, 4, 4, 8, 7, 7, 8, 11, 6, 7, 6, 10, 5, 6, 6, 6, 4, 5, 5, 8, 7, 4, 7, 6, 8, 6, 7, 6, 7, 7, 6, 6, 5, 6, 6, 6, 5, 7, 9, 4, 5, 6, 6, 5, 6, 7, 5, 10, 5, 7, 4, 5, 4, 5, 5, 5, 6, 10, 6, 6, 6, 5, 6, 7, 4, 4, 6, 6, 8, 6, 7, 8, 8, 6, 3, 5, 6, 5, 7, 5, 5, 7, 8, 6, 7, 5, 5, 6, 6, 6, 5, 6, 6, 8, 5, 8, 4, 6, 8, 4, 5, 3, 5, 5, 8, 5, 8, 7, 8, 7, 6, 10, 5, 7, 4, 4, 3, 9, 4, 6, 5, 4, 7, 7, 6, 9, 4, 6, 5, 5, 7, 5, 5, 9, 5, 6, 6, 5, 6, 5, 7, 5, 8, 8, 6, 7, 5, 7, 5, 5, 7, 7, 4, 7, 8, 6, 8, 7, 5, 7, 10, 5, 4, 7, 5, 6, 7, 7, 5, 5, 4, 6, 5, 6, 6, 7, 5, 7, 7, 6, 6, 5, 6, 4, 4, 6, 5, 6, 5, 6, 6, 5, 4, 5, 9, 9, 4, 5, 8, 6, 8, 5, 7, 7, 6, 6, 4, 9, 8, 6, 10, 5, 7, 6, 3, 6, 6, 8, 7, 5, 7, 5, 6, 8, 5, 6, 5, 5, 7, 4, 5, 4, 4, 5, 4, 5, 7, 5, 7, 6, 5, 7, 7, 6, 7, 6, 7, 7, 6, 7, 4, 4, 6, 7, 4, 6, 7, 8, 6, 8, 8, 2, 7, 4, 9, 5, 7, 7, 4, 6, 6, 4, 5, 5, 5, 9, 6, 8, 6, 9, 9, 7, 7, 5, 7, 5, 5, 5, 5, 4, 7, 6, 5, 6, 9, 6, 4, 6, 5, 6, 7, 6, 6, 4, 5, 3, 7, 4, 5, 6, 10, 4, 7, 6, 4, 6, 6, 8, 5, 6, 6, 7, 6, 6, 4, 7, 6, 7, 8, 6, 6, 7, 9, 6, 6, 9, 6, 8, 3, 5, 7, 8, 9, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 7, 3, 6, 6, 5, 9, 5, 8, 7, 5, 6, 5, 7, 7, 7, 6, 6, 8, 6, 5, 6, 7, 3, 6, 6, 8, 7, 6, 8, 5, 7, 6, 5, 6, 7, 4, 4, 7, 5, 20, 6, 5, 6, 6, 6, 7, 5, 6, 7, 5, 4, 5, 4, 9, 4, 7, 6, 8, 7, 5, 5, 5, 5, 7, 5, 6, 6, 5, 3, 5, 4, 6, 8, 6, 2, 7, 5, 5, 5, 6, 8, 7, 7, 7, 8, 7, 6, 5, 5, 6, 5, 6, 5, 7, 6, 7, 4, 6, 4, 5, 7, 5, 9, 6, 7, 7, 5, 10, 8, 5, 5, 5, 7, 6, 6, 7, 6, 7, 6, 5, 4, 5, 5, 6, 7, 10, 4, 4, 6, 6, 8, 7, 5, 6, 6, 9, 5, 5, 6, 6, 8, 5, 7, 6, 4, 9, 7, 5, 5, 6, 5, 16, 5, 4, 6, 4, 8, 8, 6, 6, 7, 7, 5, 10, 7, 6, 7, 6, 7, 5, 7, 5, 7, 6, 7, 6, 4, 8, 6, 7, 6, 6, 5, 4, 4, 5, 9, 4, 8, 8, 7, 6, 6, 11, 6, 4, 6, 6, 4, 7, 5, 6, 7, 5, 5, 3, 7, 5, 5, 3, 6, 6, 9, 5, 8, 8, 3, 6, 5, 5, 8, 5, 5, 7, 5, 9, 8, 9, 6, 6, 6, 7, 8, 8, 7, 6, 7, 9, 4, 8, 9, 9, 7, 7, 8, 6, 5, 6, 5, 5, 6, 5, 5, 8, 6, 7, 5, 7, 4, 7, 5, 10, 7, 8, 5, 6, 9, 9, 9, 10, 4, 5, 6, 9, 4, 5, 6, 6, 5, 6, 8, 9, 9, 9, 10, 7, 9, 5, 3, 6, 7, 7, 9, 7, 8, 9, 8, 7, 8, 6, 7, 6, 9, 9, 5, 8, 3, 3, 7, 5, 7, 9, 9, 8, 7, 7, 10, 6, 8, 7, 6, 8, 6, 8, 5, 5, 5, 4, 5, 7, 8, 9, 7, 3, 6, 4, 7, 7, 7, 6, 10, 6, 7, 8, 7, 10, 9, 7, 6, 8, 5, 7, 7, 6, 5, 5, 5, 7, 5, 6, 4, 5, 6, 4, 9, 7, 4, 6, 4, 9, 9, 6, 8, 5, 9, 6, 9, 6, 7, 7, 6, 5, 10, 10, 7, 7, 7, 7, 8, 6, 3, 7, 4, 5, 6, 4, 10, 9, 5, 8, 6, 9, 5, 7, 5, 7, 5, 8, 8, 9, 9, 13, 7, 7, 8, 6, 1, 7, 6, 5, 6, 7, 9, 5, 7, 6, 4, 4, 3, 7, 8, 6, 7, 8, 6, 4, 6, 9, 8, 6, 6, 6, 8, 7, 7, 5, 5, 2, 3, 7, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 6, 9, 4, 5, 5, 3, 5, 10, 5, 15, 4, 6, 9, 8, 8, 7, 5, 8, 7, 8, 9, 8, 7, 5, 6, 8, 4, 9, 6, 9, 6, 6, 6, 5, 7, 10, 11, 11, 6, 5, 6, 7, 4, 5, 11, 16, 5, 7, 9, 7, 6, 7, 7, 6, 5, 9, 6, 10, 7, 5, 7, 6, 5, 3, 8, 3, 11, 5, 5, 9, 9, 7, 6, 6, 8, 10, 6, 10, 4, 8, 7, 4, 7, 4, 5, 9, 4, 5, 9, 9, 6, 6, 8, 9, 9, 4, 7, 10, 16, 7, 11, 7, 11, 5, 8, 9, 5, 11, 7, 5, 7, 5, 5, 7, 8, 9, 5, 11, 11, 7, 8, 6, 5, 6, 5, 7, 9, 4, 7, 8, 7, 8, 4, 6, 7, 9, 9, 8, 6, 4, 7, 4, 6, 7, 6, 8, 6, 7, 4, 8, 5, 7, 7, 10, 7, 7, 6, 4, 6, 6, 13, 3, 8, 9, 6, 4, 6, 5, 6, 12, 5, 2, 6, 5, 7, 6, 7, 8, 5, 7, 4, 6, 7, 10, 3, 17, 10, 8, 6, 5, 7, 14, 1, 12, 8, 4, 4, 8, 8, 4, 8, 6, 8, 6, 5, 3, 8, 6, 7, 8, 6, 9, 8, 5, 11, 5, 3, 7, 5, 4, 5, 3, 7, 7, 6, 5, 8, 2, 5, 6, 11, 8, 6, 6, 7, 6, 7, 6, 6, 4, 4, 6, 9, 6, 9, 6, 5, 4, 3, 5, 8, 7, 10, 4, 6, 7, 4, 6, 6, 4, 6, 4, 8, 6, 10, 8, 5, 8, 6, 4, 6, 6, 5, 11, 4, 7, 8, 7, 6, 5, 5, 10, 16, 8, 5, 7, 9, 3, 7, 5, 3, 4, 8, 6, 6, 7, 6, 9, 7, 7, 6, 6, 11, 7, 7, 8, 6, 6, 7, 4, 7, 6, 3, 7, 7, 6, 8, 15, 7, 6, 4, 6, 9, 6, 6, 6, 10, 7, 8, 4, 9, 11, 6, 6, 6, 4, 11, 6, 5, 5, 5, 5, 7, 5, 5, 5, 4, 4, 8, 4, 6, 5, 7, 8, 6, 7, 5, 6, 7, 7, 6, 4, 10, 5, 6, 7, 5, 4, 7, 5, 6, 5, 7, 5, 5, 10, 6, 5, 5, 3, 7, 5, 6, 6, 6, 7, 5, 4, 7, 6, 11, 6, 8, 6, 9, 5, 14, 4, 5, 13, 8, 6, 7, 4, 7, 5, 9, 5, 8, 4, 7, 7, 8, 7, 4, 7, 5, 10, 8, 9, 5, 7, 6, 4, 7, 8, 8, 12, 7, 6, 8, 10, 5, 7, 6, 6, 6, 5, 5, 5, 6, 9, 5, 7, 4, 7, 8, 5, 7, 6, 5, 7, 7, 4, 4, 6, 7, 7, 6, 8, 6, 5, 4, 6, 5, 5, 4, 6, 6, 6, 7, 5, 4, 6, 3, 6, 7, 1, 7, 11, 4, 4, 12, 7, 6, 8, 5, 7, 8, 3, 6, 5, 5, 5, 9, 6, 9, 6, 5, 3, 7, 6, 8, 8, 9, 7, 5, 7, 6, 6, 5, 5, 10, 4, 8, 5, 8, 4, 5, 6, 5, 14, 8, 1, 5, 6, 7, 8, 10, 4, 7, 5, 9, 5, 5, 7, 4, 4, 7, 5, 8, 6, 9, 6, 8, 7, 4, 6, 4, 7, 5, 4, 6, 3, 7, 6, 6, 7, 11, 8, 6, 7, 8, 5, 7, 7, 9, 6, 5, 6, 6, 8, 6, 9, 7, 7, 7, 8, 5, 5, 5, 9, 6, 5, 4, 7, 7, 5, 7, 5, 5, 6, 8, 6, 5, 6, 4, 6, 6, 7, 7, 5, 6, 9, 6, 5, 6, 5, 8, 9, 7, 11, 7, 11, 15, 6, 7, 8, 3, 10, 8, 10, 8, 8, 4, 6, 5, 7, 6, 5, 6, 6, 7, 5, 9, 9, 9, 7, 11, 6, 6, 6, 5, 6, 6, 8, 6, 7, 5, 7, 8, 10, 10, 9, 7, 3, 9, 10, 7, 7, 6, 10, 6, 6, 7, 4, 7, 4, 5, 6, 6, 5, 5, 7, 8, 7, 4, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 7, 8, 7, 7, 5, 8, 10, 8, 7, 7, 6, 6, 8, 7, 6, 5, 6, 6, 9, 6, 2, 7, 7, 7, 7, 6, 7, 7, 4, 9, 12, 5, 5, 5, 5, 4, 9, 5, 7, 6, 7, 6, 4, 8, 7, 7, 7, 3, 5, 6, 5, 5, 7, 8, 6, 9, 4, 5, 6, 7, 5, 4, 6, 9, 5, 6, 10, 10, 6, 8, 5, 6, 4, 7, 8, 8, 5, 4, 9, 6, 4, 6, 5, 7, 5, 7, 6, 4, 10, 5, 6, 5, 5, 4, 6, 4, 6, 16, 8, 3, 7, 6, 5, 5, 5, 7, 7, 6, 9, 7, 6, 6, 6, 8, 3, 8, 4, 5, 5, 6, 6, 8, 9, 4, 7, 8, 7, 5, 6, 6, 6, 12, 4, 5, 6, 7, 7, 4, 8, 5, 7, 6, 7, 7, 9, 8, 7, 6, 7, 7, 6, 7, 5, 6, 3, 6, 5, 3, 6, 10, 7, 6, 6, 6, 6, 13, 10, 6, 8, 4, 6, 5, 7, 8, 6, 7, 5, 8, 7, 7, 8, 13, 5, 10, 7, 6, 7, 7, 6, 7, 7, 7, 8, 4, 9, 7, 4, 5, 4, 6, 8, 8, 6, 10, 3, 5, 10, 8, 6, 6, 11, 7, 6, 6, 5, 5, 8, 4, 6, 4, 13, 4, 11, 7, 5, 7, 6, 6, 7, 5, 5, 6, 5, 10, 5, 5, 8, 10, 10, 6, 6, 7, 6, 8, 5, 5, 2, 5, 5, 11, 6, 6, 8, 13, 2, 3, 5, 4, 6, 5, 4, 5, 5, 5, 11, 5, 8, 7, 8, 7, 5, 6, 5, 6, 10, 3, 9, 5, 4, 6, 6, 9, 8, 6, 9, 6, 7, 5, 6, 3, 6, 9, 8, 7, 7, 4, 5, 8, 5, 8, 8, 7, 6, 8, 14, 6, 4, 7, 3, 9, 5, 6, 5, 5, 6, 7, 3, 9, 9, 5, 6, 6, 4, 4, 9, 7, 5, 4, 5, 15, 8, 7, 9, 6, 6, 5, 7, 6, 8, 4, 4, 5, 5, 3, 5, 3, 4, 4, 4, 7, 12, 8, 9, 9, 6, 3, 6, 4, 7, 7, 9, 4, 6, 9, 5, 7, 5, 10, 5, 10, 6, 9, 4, 6, 8, 5, 8, 12, 10, 5, 7, 6, 7, 10, 7, 9, 6, 7, 5, 6, 6, 8, 6, 6, 8, 4, 6, 6, 9, 6, 6, 7, 4, 4, 3, 8, 10, 6, 6, 25, 8, 8, 5, 5, 4, 7, 7, 5, 7, 6, 7, 7, 6, 6, 5, 8, 6, 6, 7, 6, 8, 5, 4, 5, 8, 6, 12, 6, 7, 8, 4, 4, 7, 7, 9, 9, 14, 3, 10, 6, 6, 5, 7, 14, 5, 8, 4, 8, 8, 6, 6, 4, 6, 10, 14, 8, 5, 7, 6, 9, 5, 6, 7, 7, 5, 7, 5, 6, 9, 6, 6, 8, 7, 3, 6, 5, 9, 4, 4, 6, 13, 4, 6, 4, 5, 5, 7, 6, 7, 14, 3, 5, 11, 6, 7, 7, 7, 5, 5, 6, 14, 7, 7, 7, 5, 3, 4, 8, 4, 6, 8, 2, 6, 10, 5, 12, 8, 9, 6, 5, 13, 6, 8, 5, 2, 5, 1, 5, 6, 5, 5, 4, 9, 6, 7, 3, 8, 5, 6, 7, 6, 7, 8, 7, 3, 8, 6, 7, 5, 7, 7, 6, 5, 7, 11, 9, 6, 6, 3, 4, 9, 8, 8, 8, 6, 5, 6, 5, 7, 15, 8, 10, 9, 10, 6, 5, 7, 6, 10, 6, 5, 12, 5, 5, 8, 8, 9, 4, 7, 4, 4, 8, 15, 6, 4, 12, 6, 6, 4, 6, 6, 8, 4, 7, 8, 6, 6, 8, 4, 5, 9, 7, 6, 6, 7, 7, 6, 6, 7, 9, 6, 7, 6, 8, 5, 5, 5, 10, 8, 6, 6, 7, 5, 6, 6, 6, 8, 8, 7, 6, 8, 5, 6, 7, 7, 7, 4, 7, 6, 6, 4, 11, 4, 7, 6, 5, 9, 10, 8, 6, 7, 6, 7, 6, 6, 7, 4, 7, 7, 5, 7, 7, 6, 9, 6, 6, 6, 7, 8, 4, 5, 3, 7, 5, 6, 8, 6, 6, 6, 16, 6, 5, 15, 10, 6, 7, 9, 7, 7, 8, 5, 9, 6, 5, 5, 4, 9, 11, 6, 6, 6, 8, 5, 5, 6, 4, 5, 7, 8, 8, 5, 4, 3, 8, 5, 4, 4, 5, 5, 6, 5, 3, 10, 8, 5, 9, 9, 6, 5, 3, 5, 6, 7, 7, 8, 5, 8, 6, 6, 4, 5, 4, 5, 9, 9, 4, 7, 5, 6, 9, 7, 4, 8, 8, 7, 6, 10, 7, 8, 11, 5, 7, 5, 6, 7, 9, 8, 7, 7, 8, 10, 3, 4, 6, 7, 7, 5, 7, 6, 5, 6, 9, 10, 3, 7, 5, 7, 8, 9, 5, 6, 2, 9, 7, 7, 4, 6, 6, 9, 7, 9, 6, 6, 7, 6, 7, 6, 5, 7, 8, 8, 5, 6, 6, 8, 6, 6, 6, 8, 7, 8, 6, 6, 8, 5, 7, 8, 4, 6, 5, 7, 8, 7, 7, 9, 5, 1, 8, 7, 5, 5, 7, 8, 5, 8, 6, 6, 4, 6, 6, 6, 7, 5, 5, 4, 7, 7, 7, 7, 6, 7, 5, 6, 6, 7, 6, 5, 6, 7, 9, 6, 7, 7, 4, 7, 7, 5, 6, 8, 5, 7, 7, 7, 7, 6, 7, 6, 14, 6, 9, 9, 10, 5, 6, 7, 7, 9, 4, 8, 10, 4, 8, 6, 5, 5, 6, 6, 6, 7, 7, 6, 3, 5, 10, 8, 9, 6, 8, 1, 4, 6, 7, 5, 7, 8, 7, 7, 7, 6, 5, 7, 5, 4, 10, 7, 9, 5, 8, 10, 7, 5, 9, 6, 6, 7, 8, 7, 7, 6, 7, 5, 6, 5, 8, 9, 6, 6, 6, 10, 5, 6, 7, 6, 8, 6, 7, 7, 6, 9, 5, 8, 4, 5, 7, 6, 5, 4, 4, 7, 8, 5, 5, 7, 5, 8, 7, 9, 6, 10, 6, 8, 8, 6, 7, 6, 6, 7, 9, 6, 5, 5, 8, 6, 7, 10, 6, 4, 14, 3, 6, 6, 8, 8, 7, 9, 6, 6, 10, 6, 8, 7, 6, 8, 6, 4, 5, 5, 5, 8, 4, 4, 6, 4, 6, 11, 4, 5, 6, 4, 6, 7, 5, 5, 3, 5, 6, 6, 9, 8, 9, 6, 6, 6, 6, 6, 8, 7, 7, 6, 4, 6, 5, 8, 6, 6, 5, 7, 8, 5, 7, 7, 6, 7, 3, 7, 4, 6, 5, 6, 5, 6, 6, 7, 10, 6, 6, 9, 7, 14, 6, 5, 7, 9, 8, 8, 9, 5, 4, 9, 4, 6, 5, 6, 8, 7, 6, 5, 5, 6, 5, 6, 7, 6, 6, 7, 7, 7, 6, 7, 6, 7, 6, 11, 6, 7, 7, 5, 4, 7, 6, 9, 7, 3, 6, 8, 5, 6, 3, 6, 7, 6, 3, 7, 11, 5, 7, 6, 6, 5, 6, 5, 7, 6, 8, 8, 9, 6, 6, 6, 7, 7, 10, 7, 9, 7, 6, 9, 8, 7, 6, 6, 5, 7, 8, 5, 7, 6, 6, 6, 9, 7, 4, 8, 7, 8, 7, 6, 6, 10, 5, 8, 4, 8, 5, 6, 7, 7, 7, 8, 7, 6, 9, 6, 7, 5, 4, 5, 7, 7, 5, 7, 7, 6, 6, 5, 7, 6, 5, 7, 5, 6, 8, 8, 5, 5, 5, 10, 6, 5, 6, 4, 9, 5, 6, 9, 8, 8, 7, 6, 8, 10, 6, 11, 6, 5, 10, 6, 8, 8, 7, 6, 10, 5, 7, 9, 6, 4, 8, 9, 7, 6, 6, 6, 5, 10, 6, 9, 5, 7, 7, 8, 6, 7, 5, 7, 6, 4, 7, 6, 8, 8, 5, 7, 6, 4, 7, 4, 5, 8, 5, 5, 9, 8, 5, 8, 6, 4, 7, 5, 3, 6, 6, 4, 7, 3, 5, 4, 4, 7, 8, 8, 7, 10, 8, 6, 6, 7, 5, 6, 5, 7, 8, 4, 7, 9, 5, 7, 5, 5, 7, 7, 7, 5, 5, 5, 6, 9, 6, 6, 8, 7, 5, 5, 7, 6, 8, 6, 8, 10, 9, 7, 8, 5, 6, 6, 7, 6, 8, 7, 6, 8, 5, 7, 5, 6, 7, 6, 5, 8, 5, 7, 3, 7, 6, 7, 8, 8, 4, 6, 2, 8, 6, 6, 5, 7, 5, 8, 8, 9, 7, 7, 7, 8, 7, 8, 6, 5, 8, 11, 10, 7, 7, 4, 6, 8, 5, 4, 8, 3, 5, 6, 8, 9, 7, 4, 5, 8, 8, 5, 5, 6, 6, 6, 7, 9, 6, 6, 11, 4, 7, 5, 9, 9, 6, 8, 6, 6, 5, 5, 8, 7, 7, 5, 7, 6, 12, 7, 6, 7, 5, 1, 10, 5, 3, 7, 5, 5, 6, 6, 7, 8, 8, 7, 5, 3, 5, 7, 7, 8, 9, 4, 5, 8, 8, 6, 5, 7, 7, 6, 7, 5, 8, 11, 5, 5, 4, 5, 5, 1, 9, 6, 9, 9, 5, 6, 7, 7, 9, 6, 7, 7, 7, 5, 4, 5, 6, 6, 5, 6, 4, 6, 6, 5, 5, 3, 5, 7, 4, 6, 4, 8, 6, 6, 6, 6, 6, 7, 4, 3, 4, 12, 6, 6, 6, 6, 8, 6, 6, 7, 12, 8, 5, 11, 4, 6, 6, 5, 6, 7, 6, 5, 7, 7, 10, 6, 5, 7, 6, 5, 6, 6, 6, 6, 5, 10, 19, 7, 7, 8, 5, 6, 9, 6, 6, 12, 5, 4, 5, 3, 12, 4, 6, 4, 7, 4, 9, 4, 5, 3, 4, 7, 9, 6, 5, 7, 8, 5, 6, 5, 4, 8, 7, 5, 7, 5, 4, 7, 6, 4, 7, 5, 5, 7, 6, 7, 8, 11, 5, 5, 8, 3, 5, 4, 6, 6, 3, 7, 7, 5, 6, 9, 12, 7, 5, 5, 6, 9, 5, 7, 10, 6, 9, 5, 6, 6, 6, 7, 8, 6, 5, 7, 5, 7, 5, 5, 5, 8, 6, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 6, 10, 8, 7, 7, 5, 6, 6, 5, 12, 7, 8, 6, 6, 5, 8, 5, 5, 5, 5, 6, 5, 9, 10, 6, 6, 5, 4, 5, 4, 6, 6, 5, 7, 5, 7, 7, 4, 7, 5, 9, 6, 6, 4, 6, 6, 6, 5, 6, 6, 6, 6, 4, 8, 3, 5, 9, 7, 9, 6, 4, 12, 6, 7, 6, 7, 6, 8, 16, 7, 7, 5, 10, 7, 8, 6, 6, 7, 7, 11, 6, 11, 5, 6, 9, 5, 8, 6, 7, 5, 6, 7, 6, 8, 9, 7, 2, 10, 5, 7, 6, 7, 6, 6, 6, 6, 10, 4, 4, 6, 8, 6, 6, 9, 7, 6, 2, 6, 7, 5, 7, 5, 15, 8, 6, 8, 4, 6, 7, 7, 7, 8, 8, 7, 5, 5, 6, 5, 7, 7, 7, 5, 6, 9, 8, 9, 5, 6, 4, 6, 5, 5, 7, 5, 8, 5, 4, 5, 5, 7, 7, 7, 5, 6, 8, 9, 4, 6, 6, 11, 6, 8, 9, 6, 5, 6, 6, 8, 6, 5, 7, 6, 5, 7, 4, 9, 5, 4, 6, 6, 6, 3, 7, 6, 6, 5, 13, 7, 6, 8, 5, 7, 5, 5, 8, 7, 7, 7, 7, 6, 9, 10, 8, 9, 6, 6, 7, 10, 6, 7, 7, 6, 6, 6, 6, 5, 7, 7, 5, 6, 4, 6, 7, 9, 8, 4, 9, 5, 5, 6, 4, 7, 8, 9, 7, 2, 4, 7, 7, 7, 7, 7, 11, 8, 9, 6, 4, 6, 6, 5, 8, 7, 7, 6, 7, 8, 8, 8, 6, 6, 8, 9, 8, 9, 7, 7, 5, 7, 5, 9, 6, 4, 6, 7, 4, 9, 7, 4, 4, 5, 9, 10, 8, 10, 9, 5, 6, 6, 5, 7, 7, 6, 5, 6, 9, 8, 7, 4, 6, 6, 7, 4, 5, 7, 7, 5, 9, 6, 7, 6, 9, 9, 8, 7, 8, 5, 3, 4, 11, 14, 5, 5, 4, 3, 6, 6, 5, 6, 7, 6, 8, 8, 9, 7, 6, 2, 6, 6, 5, 5, 5, 3, 8, 8, 11, 7, 6, 7, 6, 6, 7, 8, 6, 7, 7, 6, 6, 8, 8, 5, 5, 16, 8, 6, 7, 9, 5, 8, 6, 5, 4, 7, 5, 6, 8, 7, 5, 6, 7, 6, 6, 7, 10, 7, 10, 9, 8, 7, 5, 8, 5, 8, 7, 7, 9, 9, 5, 7, 6, 6, 5, 7, 5, 6, 8, 7, 6, 6, 10, 6, 5, 5, 8, 5, 7, 5, 5, 7, 6, 6, 5, 6, 0, 6, 11, 5, 5, 9, 6, 5, 9, 5, 6, 6, 6, 6, 6, 7, 7, 11, 10, 4, 7, 5, 8, 8, 8, 5, 6, 6, 7, 8, 7, 8, 5, 5, 5, 6, 7, 7, 5, 8, 5, 5, 5, 10, 8, 5, 10, 5, 4, 5, 5, 6, 5, 9, 5, 4, 5, 7, 4, 4, 6, 15, 7, 6, 8, 7, 10, 7, 7, 5, 6, 7, 6, 6, 6, 8, 5, 6, 4, 2, 6, 5, 8, 8, 5, 6, 5, 7, 10, 7, 8, 7, 6, 8, 7, 11, 7, 6, 6, 8, 7, 10, 16, 6, 7, 6, 4, 5, 4, 4, 7, 8, 8, 5, 6, 4, 6, 3, 5, 9, 8, 4, 6, 4, 6, 3, 6, 9, 7, 8, 3, 14, 6, 8, 4, 7, 6, 5, 6, 4, 8, 5, 7, 7, 7, 7, 5, 5, 7, 5, 6, 6, 7, 7, 7, 7, 4, 11, 6, 7, 4, 3, 13, 5, 8, 9, 8, 6, 8, 7, 7, 6, 7, 8, 6, 6, 9, 5, 8, 6, 7, 11, 6, 5, 8, 7, 9, 5, 7, 5, 7, 8, 4, 4, 8, 5, 7, 5, 5, 6, 7, 9, 7, 6, 11, 8, 3, 5, 7, 11, 4, 6, 7, 7, 7, 6, 5, 5, 7, 5, 6, 6, 7, 6, 5, 7, 8, 6, 5, 7, 14, 10, 6, 7, 7, 5, 6, 11, 5, 6, 5, 10, 4, 3, 6, 5, 8, 6, 6, 6, 7, 7, 8, 5, 7, 6, 5, 8, 7, 9, 6, 5, 6, 8, 5, 7, 7, 7, 6, 7, 6, 8, 5, 7, 9, 8, 5, 7, 5, 8, 8, 9, 12, 7, 4, 7, 10, 8, 7, 7, 6, 6, 7, 6, 7])\n\n\n\n\n\nRook weights are another type of contiguity weight, but consider observations as neighboring only when they share an edge. The rook neighbors of an observation may be different than its queen neighbors, depending on how the observation and its nearby polygons are configured.\nWe can construct this in the same way as the queen weights:\n\nrW = Rook.from_dataframe(df)\n\n\nrW.neighbors[100]\n\n[789, 790, 1991, 791, 3676]\n\n\n\nlen(rW.neighbors[100])\n\n5\n\n\n\ndf.iloc[rW.neighbors[100]]\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\n\n\n789\n06037910605\n14.0\n161.0\n860.0\n0.0\n115.0\n1571.0\nNaN\nNaN\nNaN\n...\n2010\n1295.0\n19.775281\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((401288.903 3826196.619, 402103.760 3...\n\n\n790\n06037910712\n3.0\n16.0\n375.0\n0.0\n50.0\n665.0\nNaN\nNaN\nNaN\n...\n2010\n724.0\n18.216033\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402885.213 3824547.186, 402894.296 3...\n\n\n1991\n06037910603\n72.0\n162.0\n1465.0\n0.0\n321.0\n2371.0\nNaN\nNaN\nNaN\n...\n2010\n2001.0\n21.765830\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((399656.096 3826203.299, 400480.138 3...\n\n\n791\n06037910715\n67.0\n352.0\n1261.0\n36.0\n219.0\n2282.0\nNaN\nNaN\nNaN\n...\n2010\n1934.0\n17.863582\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((400453.228 3824402.087, 400474.053 3...\n\n\n3676\n06037910711\n50.0\n440.0\n1567.0\n27.0\n162.0\n2650.0\nNaN\nNaN\nNaN\n...\n2010\n2351.0\n19.146422\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((402109.001 3827007.300, 402932.076 3...\n\n\n\n\n5 rows × 194 columns\n\n\n\n\n\nplot_spatial_weights(rW, df)\n\n\n\n\n\n\n\n\n\npandas.Series(rW.cardinalities).plot.hist(bins=9)\n\n\n\n\n\n\n\n\n\n\n\nIn theory, a “Bishop” weighting scheme is one that arises when only polygons that share vertexes are considered to be neighboring. But, since Queen contiguigy requires either an edge or a vertex and Rook contiguity requires only shared edges, the following relationship is true:\n\\[ \\mathcal{Q} = \\mathcal{R} \\cup \\mathcal{B} \\]\nwhere \\(\\mathcal{Q}\\) is the set of neighbor pairs via queen contiguity, \\(\\mathcal{R}\\) is the set of neighbor pairs via Rook contiguity, and \\(\\mathcal{B}\\) via Bishop contiguity. Thus:\n\\[ \\mathcal{Q} \\setminus \\mathcal{R} = \\mathcal{B}\\]\nBishop weights entail all Queen neighbor pairs that are not also Rook neighbors.\nPySAL does not have a dedicated bishop weights constructor, but you can construct very easily using the w_difference function. This function is one of a family of tools to work with weights, all defined in libpysal.weights, that conduct these types of set operations between weight objects.\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW = libpysal.weights.w_difference(qW, rW, constrained=False)\n\n\nbW.histogram\n\n[(0, 1624), (1, 1728), (2, 881), (3, 292), (4, 55)]\n\n\nThus, many tracts have no bishop neighbors. But, a few do. A simple way to see these observations in the dataframe is to find all elements of the dataframe that are not “islands,” the term for an observation with no neighbors:\n\nplot_spatial_weights(bW, df)",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Spatial Weights"
    ]
  },
  {
    "objectID": "week-08/02_spatial_weights.html#distance",
    "href": "week-08/02_spatial_weights.html#distance",
    "title": "Spatial Weights",
    "section": "",
    "text": "There are many other kinds of weighting functions in PySAL. Another separate type use a continuous measure of distance to define neighborhoods.\n\ndf.crs\n\n&lt;Projected CRS: EPSG:26911&gt;\nName: NAD83 / UTM zone 11N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: North America - between 120°W and 114°W - onshore and offshore. Canada - Alberta; British Columbia; Northwest Territories; Nunavut. United States (USA) - California; Idaho; Nevada, Oregon; Washington.\n- bounds: (-120.0, 30.88, -114.0, 83.5)\nCoordinate Operation:\n- name: UTM zone 11N\n- method: Transverse Mercator\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nOur coordinate system (UTM 11N) measures distance in meters, so that’s how we’ll define our neighbors\n\ndist_band = DistanceBand.from_dataframe(df, threshold=2000)\n\n\nplot_spatial_weights(dist_band,df)\n\n\n\n\n\n\n\n\n\n\n\nradius_mile = libpysal.cg.sphere.RADIUS_EARTH_MILES\nradius_mile\n\n3958.755865744055\n\n\n\ndf_latlong = df.to_crs(4326)\n\n\nknn8_bad = KNN.from_dataframe(df_latlong, k=8) # ignore curvature of the earth\n\n\nknn8_bad.histogram\n\n[(8, 4580)]\n\n\n\nknn8 = KNN.from_dataframe(df_latlong, k=8, radius=radius_mile)\n\n\nknn8.histogram\n\n[(8, 4580)]\n\n\n\nknn8_bad.neighbors[1487]\n\n[501, 2296, 2960, 974, 167, 4496, 2295, 4422]\n\n\n\nknn8.neighbors[1487]\n\n[501, 2960, 2296, 974, 167, 4496, 2881, 2297]\n\n\n\nset(knn8_bad.neighbors[1487]) == set(knn8.neighbors[1487])\n\nFalse\n\n\n\nExercise:  Enumerate the tracts for which ignoring curvature results in an incorrect neighbor set for knn.\n\n\n# %load solutions/02_knn.py\n\n\n\n\nKernel Weights are continuous distance-based weights that use kernel densities to define the neighbor relationship. Typically, they estimate a bandwidth, which is a parameter governing how far out observations should be considered neighboring. Then, using this bandwidth, they evaluate a continuous kernel function to provide a weight between 0 and 1.\nMany different choices of kernel functions are supported, and bandwidths can either be fixed (constant over all units) or adaptive in function of unit density.\nFor example, if we want to use adaptive bandwidths for the map and weight according to a gaussian kernel:\n\n\nbandwidth = the distance to the kth nearest neighbor for each observation\nbandwith is changing across observations\n\nkernelWa = Kernel.from_dataframe(df, k=10, fixed=False, function='gaussian')\n\n\nplot_spatial_weights(kernelWa, df)\n\n\n\n\n\n\n\n\n\nkernelWa.bandwidth\n\narray([[1687.99751736],\n       [1997.79636883],\n       [1803.3632643 ],\n       ...,\n       [2468.39103021],\n       [3480.79114847],\n       [1749.84752448]])\n\n\n\ndf.assign(bw=kernelWa.bandwidth.flatten()).plot('bw', cmap='Reds')",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Spatial Weights"
    ]
  },
  {
    "objectID": "week-08/02_spatial_weights.html#block-weights",
    "href": "week-08/02_spatial_weights.html#block-weights",
    "title": "Spatial Weights",
    "section": "",
    "text": "w,s,e,n = df.total_bounds\n\n\nmx = (w+e)/2\nmy = (n+s)/2\n\n\nimport shapely\n\n\ncentroids = df.geometry.centroid\n\n\nlon = centroids.apply(lambda p: p.x).values\nlat = centroids.apply(lambda p: p.y).values\n\n\nnorth = lat &gt; my\nsouth = lat &lt;= my\neast = lon &gt; mx\nwest = lon &lt;= mx\n\n\nnw = west * north * 2\nne = east * north * 1\nsw = west * south * 3\nse = east * south *4\nquad = nw + ne + sw + se\n\n\nquad\n\narray([3, 2, 2, ..., 2, 4, 2])\n\n\n\ndf['quad'] = quad\ndf.plot(column=\"quad\", categorical=True)\n\n\n\n\n\n\n\n\n\nblockW = libpysal.weights.block_weights(df[\"quad\"])\n\n\nblockW.n\n\n4580\n\n\n\nblockW.pct_nonzero\n\n65.53761369920483\n\n\n\npandas.Series(blockW.cardinalities).plot.hist()\n\n\n\n\n\n\n\n\n\ndf.groupby(by='quad').count()\n\n\n\n\n\n\n\n\n\ngeoid\nn_asian_under_15\nn_black_under_15\nn_hispanic_under_15\nn_native_under_15\nn_white_under_15\nn_persons_under_18\nn_asian_over_60\nn_black_over_60\nn_hispanic_over_60\n...\nyear\nn_total_housing_units_sample\np_nonhisp_white_persons\np_white_over_60\np_black_over_60\np_hispanic_over_60\np_native_over_60\np_asian_over_60\np_disabled\ngeometry\n\n\nquad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n15\n15\n15\n15\n15\n15\n15\n0\n0\n0\n...\n15\n15\n15\n0\n0\n0\n0\n0\n0\n15\n\n\n2\n761\n761\n761\n761\n761\n761\n761\n0\n0\n0\n...\n761\n761\n755\n0\n0\n0\n0\n0\n0\n761\n\n\n3\n3625\n3625\n3625\n3625\n3625\n3625\n3625\n0\n0\n0\n...\n3625\n3625\n3612\n0\n0\n0\n0\n0\n0\n3625\n\n\n4\n179\n179\n179\n179\n179\n179\n179\n0\n0\n0\n...\n179\n179\n179\n0\n0\n0\n0\n0\n0\n179\n\n\n\n\n4 rows × 194 columns\n\n\n\n\n\n#plot_spatial_weights(blockW, df)\n\n\nExercise:  Which spatial weights structure would be more dense, tracts based on rook contiguity or SoCal tracts based on knn with k=4?\n\n\nExercise:  How many tracts have fewer neighbors under rook contiguity relative to knn4?\n\n\nExercise:  How many tracts have identical neighbors under queen contiguity and queen rook contiguity?\n\n\n# %load solutions/02.py\n\n\nSpatial Weights by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Spatial Weights"
    ]
  },
  {
    "objectID": "week-08/01_geovisualization.html",
    "href": "week-08/01_geovisualization.html",
    "title": "Geovisualization with PySAL",
    "section": "",
    "text": "When the Python Spatial Analysis Library, PySAL, was originally planned, the intention was to focus on the computational aspects of exploratory spatial data analysis and spatial econometric methods, while relying on existing GIS packages and visualization libraries for visualization of computations. Indeed, we have partnered with esri and QGIS towards this end.\nHowever, over time we have received many requests for supporting basic geovisualization within PySAL so that the step of having to interoperate with an external package can be avoided, thereby increasing the efficiency of the spatial analytical workflow.\nIn this notebook, we demonstrate several approaches towards a particular subset of geovisualization methods, namely choropleth maps. We start with an exploratory workflow introducing mapclassify and geopandas to create different choropleth classifications and maps for quick exploratory data analysis. We then introduce the geoviews package for interactive mapping in support of exploratory spatial data analysis.\n\n\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nimport mapclassify\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as cx\n\n\ngdf = gpd.read_parquet('~/data/scag_region.parquet')\n\n\ngdf = gdf.to_crs(3857)\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', ax=ax,\n        edgecolor='white', legend=True, linewidth=0.3)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nAs a first cut, geopandas makes it very easy to plot a map quickly. If you know the area well, this may do fine for quick exploration. If you don’t know a place extremely well (or you want to make a figure easy to understand for those who don’t) it’s often a good idea to add a basemap for context. We can do that easily using the contextily package\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', alpha=0.6, ax=ax, legend=True)\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nhv = gdf['median_home_value']\n\n\nmapclassify.Quantiles(hv, k=5)\n\nQuantiles\n\n        Interval           Count\n--------------------------------\n[  10888.42,  276462.96] |   916\n( 276462.96,  368936.70] |   917\n( 368936.70,  457037.70] |   993\n( 457037.70,  628216.01] |   840\n( 628216.01, 1088952.40] |   914\n\n\n\nmapclassify.Quantiles(hv, k=10)\n\nQuantiles\n\n        Interval           Count\n--------------------------------\n[  10888.42,  203688.34] |   458\n( 203688.34,  276462.96] |   458\n( 276462.96,  325454.88] |   458\n( 325454.88,  368936.70] |   459\n( 368936.70,  406233.29] |   457\n( 406233.29,  457037.70] |   536\n( 457037.70,  512928.74] |   380\n( 512928.74,  628216.01] |   460\n( 628216.01,  778142.83] |   456\n( 778142.83, 1088952.40] |   458\n\n\n\nq10 = mapclassify.Quantiles(hv, k=10)\n\n\ndir(q10)\n\n['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_classify',\n '_fmt',\n '_set_bins',\n '_summary',\n '_update',\n 'adcm',\n 'bins',\n 'classes',\n 'counts',\n 'find_bin',\n 'fmt',\n 'gadf',\n 'get_adcm',\n 'get_fmt',\n 'get_gadf',\n 'get_legend_classes',\n 'get_tss',\n 'k',\n 'make',\n 'name',\n 'plot',\n 'set_fmt',\n 'table',\n 'tss',\n 'update',\n 'y',\n 'yb']\n\n\n\nq10.bins\n\narray([ 203688.34269663,  276462.9588015 ,  325454.87827715,\n        368936.70411985,  406233.28651685,  457037.69973266,\n        512928.73595506,  628216.01123596,  778142.82771536,\n       1088952.39981273])\n\n\n\nq10.counts\n\narray([458, 458, 458, 459, 457, 536, 380, 460, 456, 458])\n\n\n\nfj10 = mapclassify.FisherJenks(hv, k=10)\n\n\nfj10\n\nFisherJenks\n\n        Interval           Count\n--------------------------------\n[  10888.42,  191982.12] |   428\n( 191982.12,  278662.64] |   510\n( 278662.64,  348355.52] |   663\n( 348355.52,  417721.72] |   800\n( 417721.72,  492314.89] |   679\n( 492314.89,  580955.52] |   394\n( 580955.52,  680594.57] |   392\n( 680594.57,  804626.12] |   297\n( 804626.12,  975482.58] |   184\n( 975482.58, 1088952.40] |   233\n\n\n\nfj10.adcm\n\n98231639.16239837\n\n\n\nq10.adcm\n\n127632537.27184954\n\n\n\nbins = [100000, 500000, 1000000, 1500000]\n\n\nud4 = mapclassify.UserDefined(hv, bins=bins)\nud4\n\nUserDefined\n\n        Interval           Count\n--------------------------------\n[  10888.42,  100000.00] |    61\n( 100000.00,  500000.00] |  3054\n( 500000.00, 1000000.00] |  1251\n(1000000.00, 1500000.00] |   214\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', ax=ax, alpha=0.6, legend=True)\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True)\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\ngdf.geoid\n\n0       06037128702\n1       06037131600\n2       06037134104\n3       06037134304\n4       06037242000\n           ...     \n4575    06037920029\n4576    06037542000\n4577    06111004304\n4578    06065044804\n4579    06037124400\nName: geoid, Length: 4580, dtype: object\n\n\n\nimport numpy\n\n\ncounties = set([geoid[:5] for geoid in gdf.geoid])\n\n\ncounties\n\n{'06025', '06037', '06059', '06065', '06071', '06073', '06111'}\n\n\n\nfor county in counties:\n    cgdf = gdf[gdf['geoid'].str.match(f'^{county}')]\n    f, ax = plt.subplots(1, figsize=(12, 8))\n    cgdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        edgecolor='grey', legend=True, cmap='Blues', alpha=0.6)\n    plt.title(county)\n    ax.set_axis_off()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngdf.explore(column='median_home_value')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.explore(column='median_home_value', tooltip=['median_home_value', 'geoid'])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nExercise:  Create choropleth maps for each of the counties that use the FisherJenks classification for k=6 defined on the entire Southern California region.\n\n\n# %load solutions/01.py\n\n\nGeovisualization by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Geovisualization with PySAL"
    ]
  },
  {
    "objectID": "week-08/01_geovisualization.html#introduction",
    "href": "week-08/01_geovisualization.html#introduction",
    "title": "Geovisualization with PySAL",
    "section": "",
    "text": "When the Python Spatial Analysis Library, PySAL, was originally planned, the intention was to focus on the computational aspects of exploratory spatial data analysis and spatial econometric methods, while relying on existing GIS packages and visualization libraries for visualization of computations. Indeed, we have partnered with esri and QGIS towards this end.\nHowever, over time we have received many requests for supporting basic geovisualization within PySAL so that the step of having to interoperate with an external package can be avoided, thereby increasing the efficiency of the spatial analytical workflow.\nIn this notebook, we demonstrate several approaches towards a particular subset of geovisualization methods, namely choropleth maps. We start with an exploratory workflow introducing mapclassify and geopandas to create different choropleth classifications and maps for quick exploratory data analysis. We then introduce the geoviews package for interactive mapping in support of exploratory spatial data analysis.\n\n\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nimport mapclassify\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as cx\n\n\ngdf = gpd.read_parquet('~/data/scag_region.parquet')\n\n\ngdf = gdf.to_crs(3857)\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', ax=ax,\n        edgecolor='white', legend=True, linewidth=0.3)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\nAs a first cut, geopandas makes it very easy to plot a map quickly. If you know the area well, this may do fine for quick exploration. If you don’t know a place extremely well (or you want to make a figure easy to understand for those who don’t) it’s often a good idea to add a basemap for context. We can do that easily using the contextily package\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', alpha=0.6, ax=ax, legend=True)\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nhv = gdf['median_home_value']\n\n\nmapclassify.Quantiles(hv, k=5)\n\nQuantiles\n\n        Interval           Count\n--------------------------------\n[  10888.42,  276462.96] |   916\n( 276462.96,  368936.70] |   917\n( 368936.70,  457037.70] |   993\n( 457037.70,  628216.01] |   840\n( 628216.01, 1088952.40] |   914\n\n\n\nmapclassify.Quantiles(hv, k=10)\n\nQuantiles\n\n        Interval           Count\n--------------------------------\n[  10888.42,  203688.34] |   458\n( 203688.34,  276462.96] |   458\n( 276462.96,  325454.88] |   458\n( 325454.88,  368936.70] |   459\n( 368936.70,  406233.29] |   457\n( 406233.29,  457037.70] |   536\n( 457037.70,  512928.74] |   380\n( 512928.74,  628216.01] |   460\n( 628216.01,  778142.83] |   456\n( 778142.83, 1088952.40] |   458\n\n\n\nq10 = mapclassify.Quantiles(hv, k=10)\n\n\ndir(q10)\n\n['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_classify',\n '_fmt',\n '_set_bins',\n '_summary',\n '_update',\n 'adcm',\n 'bins',\n 'classes',\n 'counts',\n 'find_bin',\n 'fmt',\n 'gadf',\n 'get_adcm',\n 'get_fmt',\n 'get_gadf',\n 'get_legend_classes',\n 'get_tss',\n 'k',\n 'make',\n 'name',\n 'plot',\n 'set_fmt',\n 'table',\n 'tss',\n 'update',\n 'y',\n 'yb']\n\n\n\nq10.bins\n\narray([ 203688.34269663,  276462.9588015 ,  325454.87827715,\n        368936.70411985,  406233.28651685,  457037.69973266,\n        512928.73595506,  628216.01123596,  778142.82771536,\n       1088952.39981273])\n\n\n\nq10.counts\n\narray([458, 458, 458, 459, 457, 536, 380, 460, 456, 458])\n\n\n\nfj10 = mapclassify.FisherJenks(hv, k=10)\n\n\nfj10\n\nFisherJenks\n\n        Interval           Count\n--------------------------------\n[  10888.42,  191982.12] |   428\n( 191982.12,  278662.64] |   510\n( 278662.64,  348355.52] |   663\n( 348355.52,  417721.72] |   800\n( 417721.72,  492314.89] |   679\n( 492314.89,  580955.52] |   394\n( 580955.52,  680594.57] |   392\n( 680594.57,  804626.12] |   297\n( 804626.12,  975482.58] |   184\n( 975482.58, 1088952.40] |   233\n\n\n\nfj10.adcm\n\n98231639.16239837\n\n\n\nq10.adcm\n\n127632537.27184954\n\n\n\nbins = [100000, 500000, 1000000, 1500000]\n\n\nud4 = mapclassify.UserDefined(hv, bins=bins)\nud4\n\nUserDefined\n\n        Interval           Count\n--------------------------------\n[  10888.42,  100000.00] |    61\n( 100000.00,  500000.00] |  3054\n( 500000.00, 1000000.00] |  1251\n(1000000.00, 1500000.00] |   214",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Geovisualization with PySAL"
    ]
  },
  {
    "objectID": "week-08/01_geovisualization.html#geopandas-choropleths",
    "href": "week-08/01_geovisualization.html#geopandas-choropleths",
    "title": "Geovisualization with PySAL",
    "section": "",
    "text": "f, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='QUANTILES', ax=ax, alpha=0.6, legend=True)\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True)\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\n\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\nf, ax = plt.subplots(1, figsize=(12, 8))\ngdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        alpha=0.6, legend=True, cmap='Blues')\ncx.add_basemap(ax, crs=gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax.set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\ngdf.geoid\n\n0       06037128702\n1       06037131600\n2       06037134104\n3       06037134304\n4       06037242000\n           ...     \n4575    06037920029\n4576    06037542000\n4577    06111004304\n4578    06065044804\n4579    06037124400\nName: geoid, Length: 4580, dtype: object\n\n\n\nimport numpy\n\n\ncounties = set([geoid[:5] for geoid in gdf.geoid])\n\n\ncounties\n\n{'06025', '06037', '06059', '06065', '06071', '06073', '06111'}\n\n\n\nfor county in counties:\n    cgdf = gdf[gdf['geoid'].str.match(f'^{county}')]\n    f, ax = plt.subplots(1, figsize=(12, 8))\n    cgdf.plot(column='median_home_value', scheme='FisherJenks', ax=ax,\n        edgecolor='grey', legend=True, cmap='Blues', alpha=0.6)\n    plt.title(county)\n    ax.set_axis_off()\n    plt.show()",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Geovisualization with PySAL"
    ]
  },
  {
    "objectID": "week-08/01_geovisualization.html#explore",
    "href": "week-08/01_geovisualization.html#explore",
    "title": "Geovisualization with PySAL",
    "section": "",
    "text": "gdf.explore(column='median_home_value')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.explore(column='median_home_value', tooltip=['median_home_value', 'geoid'])\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nExercise:  Create choropleth maps for each of the counties that use the FisherJenks classification for k=6 defined on the entire Southern California region.\n\n\n# %load solutions/01.py\n\n\nGeovisualization by Serge Rey is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Week 8 3/05, 3/07",
      "Geovisualization with PySAL"
    ]
  },
  {
    "objectID": "week-09/index.html",
    "href": "week-09/index.html",
    "title": "Exercise 2 collaboration",
    "section": "",
    "text": "Zoom recording (Canvas access)",
    "crumbs": [
      "Week 9 3/12, 3/14",
      "Exercise 2 collaboration"
    ]
  },
  {
    "objectID": "week-07/index.html",
    "href": "week-07/index.html",
    "title": "Spatial Research and PySAL",
    "section": "",
    "text": "Intersectional Urban Dyanmics\nIntroduction to PySAL",
    "crumbs": [
      "Week 7 2/27, 2/29",
      "Spatial Research and PySAL"
    ]
  },
  {
    "objectID": "week-02/files.html",
    "href": "week-02/files.html",
    "title": "Notes on files and directories (Notebook)",
    "section": "",
    "text": "This is a test\n\n\nCode\nfor i in range(10):\n    print(i*i)\n\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81"
  },
  {
    "objectID": "week-02/files.html#file-and-directory-notes",
    "href": "week-02/files.html#file-and-directory-notes",
    "title": "Notes on files and directories (Notebook)",
    "section": "",
    "text": "This is a test\n\n\nCode\nfor i in range(10):\n    print(i*i)\n\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81"
  },
  {
    "objectID": "week-02/files.html#file-commands",
    "href": "week-02/files.html#file-commands",
    "title": "Notes on files and directories (Notebook)",
    "section": "File Commands",
    "text": "File Commands\n\ncat\ntouch"
  },
  {
    "objectID": "week-02/jupyterhub.html",
    "href": "week-02/jupyterhub.html",
    "title": "Introduction to Computational Environment (Zoom)",
    "section": "",
    "text": "Introduction to Computational Environment (Zoom)\nRecording (SDSU Account Required)"
  },
  {
    "objectID": "index.html#class-meetings",
    "href": "index.html#class-meetings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Class Meetings",
    "text": "Class Meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nGMCS 307\nTue & Thu 2:00 - 3:15pm"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nSergio Rey\nTue 9:00 - 10:00 (by appointment)\nPSFA 361G\n\n\nJin Huang\nFri 10:30am (virtual)\nPSFA 361F"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nWelcome to 385: Spatial Data Analysis!\nThe purpose of this course is to introduce you to methods of spatial data analysis. The focus is on both the conceptual and applied aspects of spatial statistical methods. We will place particular emphasis on the computational aspects of Exploratory Spatial Data Analysis (ESDA) methods for diﬀerent types of spatial data including point processes, lattice data, geostatistical data, network data, and spatial interaction. Throughout the course you will gain valuable hands-on experience with several specialized software packages for spatial data analysis. The overriding goal of the course is for you to acquire familiarity with the fundamental methodological and operational issues in the statistical analysis of geographic information and the ability to extend these methods in your own research.\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts. Put differently, students will learn to code. But this is a means to the end goal: students will code to learn spatial data analysis.\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nGEOG 101 or GEOG 102\nSTAT 250 or comparable course in statistics.\n\nAll students are required to complete the prerequisite assessment quiz before 2024-01-30 2:00pm."
  },
  {
    "objectID": "index.html#computational-learning",
    "href": "index.html#computational-learning",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Computational Learning",
    "text": "Computational Learning\nWe will be using open source geospatial software throughout the course together with Jupyter Notebooks, and Python as our scripting language.\nAll software for the course will be made available through a web-based framework. Students wishing to install these materials on their own machines will be given instructions to do so, but this is not required."
  },
  {
    "objectID": "index.html#readings",
    "href": "index.html#readings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Readings",
    "text": "Readings\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore being prepared for class means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings.\n\n\n\nAbbreviation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press."
  },
  {
    "objectID": "index.html#schedule-planned",
    "href": "index.html#schedule-planned",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)\n\n\n\nWeek\nDates\nTopic\nReading\nDue\n\n\n\n\n1\nJan-18\nCourse Introduction\n\n\n\n\n2\nJan-23\nComputational Environment I\n\n\n\n\n\nJan-25\nComputational Environment II\n\n\n\n\n3\nJan-30\nPython: Introduction\nGDA 1 GDS 2\nQuiz 1\n\n\n\nFeb-01\nPython: Programming Concepts\nGDA 2\n\n\n\n4\nFeb-06\nPython: Scripting\nGDA 2\nQuiz 2\n\n\n\nFeb-08\nPython: Functions\nGDA 2\n\n\n\n5\nFeb-13\nExericse 1 Collaboration\n\nExercise 1\n\n\n\nFeb-15\nPython: Data Analysis/Visualization\nGDA 3,4\nQuiz 3\n\n\n6\nFeb-20\nGeopandas\nGDA 5\nQuiz 4\n\n\n\nFeb-22\nGeoprocessing\nGDA 6\n\n\n\n7\nFeb-27\nResearch: Intersectional Urban Dynamics\nRey 2023\n\n\n\n\nFeb-29\nPySAL\nGDS 3\nQuiz 5\n\n\n8\nMar-05\nGeoVisualization\nGDS 5\nQuiz 6\n\n\n\nMar-07\nSpatial Weights\nGDS 4\n\n\n\n9\nMar-12\nSpatial Dependence\nGDS 6\nQuiz 7\n\n\n\nMar-14\nGlobal Autocorrelation\nGDS 6\nExercise 2\n\n\n10\nMar-19\nGlobal Autocorrelation Tests\nGDS 6\nQuiz 8\n\n\n\nMar-21\nLocal Autocorrelation\nGDS 7\n\n\n\n11\nMar-26\nLocal Autocorrelation Tests\nGDS 7\nQuiz 9\n\n\n\nMar-28\nPoint Pattern Data\nGDS 8\nExercise 3\n\n\n\nApr-02\nSpring Break\n\n\n\n\n\nApr-04\nSpring Break\n\n\n\n\n12\nApr-09\nCentrography\nGDS 8\nQuiz 10\n\n\n\nApr-11\nPoint Processes\nGDS 8\n\n\n\n13\nApr-16\nAAG (No Class)\nGDS 8\n\n\n\n\nApr-18\nExercise 3\nGDS 8\n\n\n\n14\nApr-23\nQuadrat Statistics\nGDS 8\nQuiz 11\n\n\n\nApr-25\nDistance Based Statistics\nDS 6.1-6.5\n\n\n\n15\nApr-30\nDistance Based Statistics\nDS 6.5\n\n\n\n\nMay-02\nWrap up/Review\n\n\n\n\n16\nMay-07\nFinal Exam (1-3pm)"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course.\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade.\n\nFor Step 1, the base grade is determined using the following specification:\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 12 of 14 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 10 of 14 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 8 of 14 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 6 of 14 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles\n\n\n\nFor Step 2, your final course grade is determined as follows:\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade."
  },
  {
    "objectID": "index.html#quizzes",
    "href": "index.html#quizzes",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Quizzes",
    "text": "Quizzes\nStarting in week three, there will be a quiz given during the first five minutes of the class meeting. The quiz will cover the assigned reading that is required before our work in class. Quizzes are graded on a pass/fail basis."
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise.\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work."
  },
  {
    "objectID": "index.html#final-exam",
    "href": "index.html#final-exam",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on May 7 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class."
  },
  {
    "objectID": "index.html#tokens",
    "href": "index.html#tokens",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester.\nUsing Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam. (Requests required by 2024-04-15 17:00.)\n\nRemaining Tokens\nEach token that remains unused after 2024-05-02 will be counted as a passed quiz. Tokens cannot be exchanged with other students."
  },
  {
    "objectID": "index.html#policies",
    "href": "index.html#policies",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Policies",
    "text": "Policies\n\nAccommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center.\n\n\nPrivacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use Canvas to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise.\n\n\nAcademic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: https://sacd.sdsu.edu/student-rights/academic-dishonesty.\n\n\nCode of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form.\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct."
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html",
    "href": "week-11/04_local_spatial_autocorrelation.html",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "In this notebook we introduce methods of exploratory spatial data analysis that are intended to complement geovizualization through formal univariate and multivariate statistical tests for spatial clustering.\n\n\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport esda\nimport libpysal as lps\nimport contextily as cx\n\n\nimport seaborn as sns\nsns.set_context('notebook')\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n\n\nFor this exercise, we’ll use two datasets:\n\na set of polygons (census tracts) for the city of San Diego from the US Census American Community Survey 5-year estimates.\n\n\n\n\nscag = gpd.read_parquet(\"~/data/scag_region.parquet\")\n\n\nsan_diego = scag[scag.geoid.str[:5]=='06073']\n\n\nsan_diego.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 627 entries, 158 to 4567\nColumns: 194 entries, geoid to geometry\ndtypes: float64(191), geometry(1), int64(1), object(1)\nmemory usage: 955.2+ KB\n\n\n\nsan_diego = san_diego.dropna(subset=['median_home_value'])\n\n\nsan_diego = san_diego.to_crs(epsg=3857)\n\n\nf, ax = plt.subplots(figsize=(10,10))\n\nsan_diego.plot('median_home_value', ax=ax, alpha=0.6)\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\n\n\n\n\n\n\n\n\nsan_diego.median_home_value.hist()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12,12))\n\nsan_diego.dropna(subset=['median_home_value']).to_crs(epsg=3857).plot('median_home_value', legend=True, scheme='quantiles', cmap='GnBu', k=5, ax=ax, alpha=0.7)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Median Home Value (Quintiles)\", fontsize=16)\n\nplt.axis('off')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nVisual inspection of the map pattern for the prices allows us to search for spatial structure. If the spatial distribution of the prices was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the darker clusters along the coast, and a concentration of the lighter hues (lower prices) in the north central and south east. In the point data, the trees are too dense to make any sense of the pattern\nOur brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes.\nThe concept of spatial autocorrelation relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\nLet’s use PySAL to generate these two types of similarity measures.\n\n\nWe have already encountered spatial weights in a previous notebook. In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:\n\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'r'\n\n\n\n\nSo the spatial weight between neighborhoods \\(i\\) and \\(j\\) indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood \\(i\\) the spatial lag is defined as: \\[ylag_i = \\sum_j w_{i,j} y_j\\]\n\ny = san_diego['median_home_value']\nylag = lps.weights.lag_spatial(wq, y)\n\n\nf, ax = plt.subplots(1, figsize=(12, 12))\n\nsan_diego.assign(cl=ylag).plot(column='cl', scheme='quantiles', \\\n        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n        edgecolor='white', legend=True)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Spatial Lag Median Home Val (Quintiles)\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother.\n\nsan_diego['lag_median_pri'] = ylag\n\nf,ax = plt.subplots(1,2,figsize=(12,4))\n\nsan_diego.plot(column='median_home_value', ax=ax[0],\n        scheme=\"quantiles\",  k=5, cmap='GnBu')\n\n#ax[0].axis(san_diego.total_bounds[np.asarray([0,2,1,3])])\nax[0].set_title(\"Price\", fontsize=16)\n\nsan_diego.plot(column='lag_median_pri', ax=ax[1],\n        scheme='quantiles', cmap='GnBu', k=5)\n\ncx.add_basemap(ax[0], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\ncx.add_basemap(ax[1], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax[1].set_title(\"Spatial Lag Price\", fontsize=16)\nax[0].axis('off')\nax[1].axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nHowever, we still have the challenge of visually associating the value of the prices in a neighborhod with the value of the spatial lag of values for the focal unit. The latter is a weighted average of home prices in the focal county’s neighborhood.\nTo complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation.\n\n\n\n\n\n\n\nOne way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called joins. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\nEach unit can take on one of two values “Black” or “White”, analogous to the layout of a chessboard\n\nnrows, ncols = 9,9\nimage = np.zeros(nrows*ncols)\n\n# Set every other cell to 1\nimage[::2] = 1\n\n# Reshape things into a 9x9 grid.\nimage = image.reshape((nrows, ncols))\nplt.matshow(image, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nand so for a given pair of neighboring locations there are three different types of joins that can arise:\n\nBlack Black (BB)\nWhite White (WW)\nBlack White (or White Black) (BW)\n\nWe can use the esda package from PySAL to carry out join count analysis. In the case of our point data, the join counts can help us determine whether different varieties of trees tend to grow together, spread randomly through space, or compete with one another for precious resources\n\n\nWith polygon data, we can conduct an analysis using a contiguity matrix. For our housing price data, we need to first discretize the variable we’re using; to keep things simple, we’ll binarize our price data using the median so that “high” values are tracts whose median home price is above the city’s median and “low” values are those below\n\ny.median()\n\n405416.57303370786\n\n\n\nsan_diego.shape\n\n(627, 195)\n\n\n\nyb = y &gt; y.median()\nsum(yb)\n\n313\n\n\n\nyb = y &gt; y.median()\nlabels = [\"0 Low\", \"1 High\"]\nyb = [labels[i] for i in 1*yb] \nsan_diego['yb'] = yb\n\n\nfig, ax = plt.subplots(figsize=(12,12))\nsan_diego.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)\n\n\n\n\n\n\n\n\nThe spatial distribution of the binary variable immediately raises questions about the juxtaposition of the “black” and “white” areas.\nGiven that we have 308 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map?\n\nyb = 1 * (y &gt; y.median()) # convert back to binary\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'b'\nnp.random.seed(12345)\njc = esda.join_counts.Join_Counts(yb, wq)\n\nThe resulting object stores the observed counts for the different types of joins:\n\njc.bb\n\n754.0\n\n\n\njc.ww\n\n745.0\n\n\n\njc.bw\n\n475.0\n\n\nNote that the three cases exhaust all possibilities:\n\njc.bb + jc.ww + jc.bw\n\n1974.0\n\n\nand\n\nwq.s0 / 2\n\n1974.0\n\n\nwhich is the unique number of joins in the spatial weights object.\nOur object tells us we have observed 736 BB joins:\n\njc.bb\n\n754.0\n\n\nThe critical question for us, is whether this is a departure from what we would expect if the process generating the spatial distribution of the Black polygons were a completely random one? To answer this, PySAL uses random spatial permutations of the observed attribute values to generate a realization under the null of complete spatial randomness (CSR). This is repeated a large number of times (999 default) to construct a reference distribution to evaluate the statistical significance of our observed counts.\nThe average number of BB joins from the synthetic realizations is:\n\njc.mean_bb\n\n490.03103103103103\n\n\nwhich is less than our observed count. The question is whether our observed value is so different from the expectation that we would reject the null of CSR?\n\nimport seaborn as sbn\nsbn.kdeplot(jc.sim_bb, shade=True)\nplt.vlines(jc.bb, 0, 0.005, color='r')\nplt.vlines(jc.mean_bb, 0,0.005)\nplt.xlabel('BB Counts')\n\nText(0.5, 0, 'BB Counts')\n\n\n\n\n\n\n\n\n\nThe density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for our prices. Clearly our observed value is extremely high. A pseudo p-value summarizes this:\n\njc.p_sim_bb\n\n0.001\n\n\nSince this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in market prices.\n\n\n\nThe join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\nFirst, we transform our weights to be row-standardized, from the current binary state:\n\nwq.transform = 'r'\n\n\ny = san_diego['median_home_value']\n\nMoran’s I is a test for global autocorrelation for a continuous attribute:\n\nnp.random.seed(12345)\nmi = esda.moran.Moran(y, wq)\nmi.I\n\n0.660917168991019\n\n\nAgain, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations.\n\nfrom splot.esda import plot_moran\nplot_moran(mi, zstandard=True, figsize=(10,4))\nplt.show()\n\n\n\n\n\n\n\n\n\nmi.p_sim\n\n0.001\n\n\nOn the left, we have the reference distribution versus the observed statistic; on the right, we have a plot of the focal value against its spatial lag, for which the Moran I statistic serves as the slope\nHere our observed value is again in the upper tail\n\nmi.p_sim\n\n0.001\n\n\n\n\n\n\nIn addition to the Global autocorrelation statistics, PySAL has many local autocorrelation statistics. Let’s compute a local Moran statistic for the same data\n\nnp.random.seed(12345)\n\n\nwq.transform = 'r'\nlag_price = lps.weights.lag_spatial(wq, san_diego['median_home_value'])\n\n\nli = esda.moran.Moran_Local(y, wq)\n\nNow, instead of a single \\(I\\) statistic, we have an array of local \\(I_i\\) statistics, stored in the .Is attribute, and p-values from the simulation are in p_sim.\n\nfrom splot.esda import moran_scatterplot\n\nfig, ax = moran_scatterplot(li)\nax.set_xlabel('Price')\nax.set_ylabel('Spatial Lag of Price')\nplt.show()\n\n\n\n\n\n\n\n\n\nli.q\n\narray([4, 1, 2, 3, 3, 3, 3, 4, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 2, 3, 2, 2, 1, 1, 2, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 2, 1, 1, 1,\n       3, 1, 3, 3, 4, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 2,\n       2, 1, 2, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1,\n       3, 3, 2, 2, 2, 1, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 2,\n       2, 4, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3,\n       1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1,\n       1, 3, 3, 3, 3, 1, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1,\n       1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 3, 3, 2, 3, 4, 4, 2, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 1, 3, 3, 2, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 3, 3, 3, 4, 1, 3, 3, 1, 2, 4, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2,\n       2, 2, 3, 3, 3, 3, 3, 3, 1, 1, 1, 2, 4, 3, 3, 1, 3, 3, 3, 3, 3, 1,\n       3, 3, 3, 2, 1, 1, 1, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4,\n       1, 1, 1, 1, 3, 1, 3, 4, 4, 4, 3, 2, 1, 1, 2, 3, 3, 2, 3, 4, 3, 3,\n       3, 3, 3, 4, 3, 3, 4, 1, 3, 3, 3, 4, 1, 3, 3, 3, 3, 3, 4, 4, 3, 2,\n       3, 3, 1, 1, 4, 4, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 1, 2, 3, 4,\n       1, 1, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 4, 3, 1, 3, 3, 2, 1, 2, 3, 3,\n       4, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 4, 1, 1, 1, 1, 2, 1,\n       1, 2, 3, 3, 3, 3, 4, 2, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 2, 4, 1, 3,\n       3, 3, 2, 1, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 4, 2, 1, 1, 3, 3, 1,\n       3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 2, 1,\n       1, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1,\n       1, 1, 3, 1, 3, 3, 2, 3, 1, 1, 3, 3, 3, 3, 4, 2, 1, 1, 1, 2, 3, 1,\n       3, 3, 1, 1, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1,\n       3, 3, 1, 4, 1, 3, 1, 2, 1, 3, 1, 3, 3, 3, 3, 4, 3, 3, 4, 1, 4, 3,\n       3, 3, 3, 2, 3, 1, 1, 4, 1, 1, 1, 1, 3, 3, 1, 3, 3, 2, 2, 3, 2, 4,\n       1, 1, 1, 1, 3, 3, 4, 1, 3, 1, 2, 3, 1, 1, 1, 1, 2, 2, 3, 3, 3, 1,\n       3, 2, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 4,\n       4, 3, 3, 3, 3, 3, 1, 3, 4, 3, 3])\n\n\nWe can again test for local clustering using permutations, but here we use conditional random permutations (different distributions for each focal location)\n\n(li.p_sim &lt; 0.05).sum()\n\n246\n\n\n\nfig, ax = moran_scatterplot(li, p=0.05)\nax.set_xlabel('Price')\nax.set_ylabel('Spatial Lag of Price')\nplt.show()\n\n\n\n\n\n\n\n\nWe can distinguish the specific type of local spatial association reflected in the four quadrants of the Moran Scatterplot above: - High-High (upper right) - Low-Low (bottom left) - High-Low (lower right) - Low-High (upper left)\nUsing splot, we can also plot these hotspots on the original geodataframe\n\nfrom splot.esda import lisa_cluster\nlisa_cluster(li, san_diego, p=0.05, figsize = (9,9))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom splot.esda import plot_local_autocorrelation\nplot_local_autocorrelation(li, san_diego, 'median_home_value')\n\n(&lt;Figure size 1500x400 with 3 Axes&gt;,\n array([&lt;Axes: title={'center': 'Moran Local Scatterplot'}, xlabel='Attribute', ylabel='Spatial Lag'&gt;,\n        &lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object))\n\n\n\n\n\n\n\n\n\n\nli.Is.shape\n\n(627,)\n\n\n\ndir(li)\n\n['EI',\n 'EI_sim',\n 'EIc',\n 'Is',\n 'VI',\n 'VI_sim',\n 'VIc',\n '_Moran_Local__calc',\n '_Moran_Local__moments',\n '_Moran_Local__quads',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_statistic',\n 'by_col',\n 'den',\n 'geoda_quads',\n 'n',\n 'n_1',\n 'p_sim',\n 'p_z_sim',\n 'permutations',\n 'q',\n 'quads',\n 'rlisas',\n 'seI_sim',\n 'sim',\n 'w',\n 'y',\n 'z',\n 'z_sim']\n\n\n\nnp.random.seed(12345)\n\n\nli = esda.moran.Moran_Local(y, wq, keep_simulations=True)\n\n\nsig = np.where(li.p_sim&lt;0.05)\n\n\ndir(li)\n\n['EI',\n 'EI_sim',\n 'EIc',\n 'Is',\n 'VI',\n 'VI_sim',\n 'VIc',\n '_Moran_Local__calc',\n '_Moran_Local__moments',\n '_Moran_Local__quads',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_statistic',\n 'by_col',\n 'den',\n 'geoda_quads',\n 'n',\n 'n_1',\n 'p_sim',\n 'p_z_sim',\n 'permutations',\n 'q',\n 'quads',\n 'rlisas',\n 'seI_sim',\n 'sim',\n 'w',\n 'y',\n 'z',\n 'z_sim']\n\n\n\nli.rlisas.shape\n\n(627, 999)\n\n\n\nli.rlisas[sig[0][-1],:]\n\narray([ 3.42798494e-01, -4.45389092e-02,  6.44999365e-01,  1.01595189e-01,\n        5.57418460e-02, -9.65445924e-02, -5.18269142e-01, -1.46339409e-01,\n        7.45014339e-01,  1.78287038e-02,  4.44180655e-01, -3.04512933e-01,\n       -3.15555070e-01, -1.76574745e-01,  1.14373127e-01,  3.18195714e-04,\n        1.73582307e-01,  5.62915572e-01, -2.63419473e-02,  3.00282170e-02,\n        5.59182100e-01, -1.74789220e-01,  1.55283563e-01, -3.12980257e-01,\n        2.44676548e-01, -1.70737909e-01,  5.17219982e-01, -4.43597899e-01,\n       -1.74552329e-02,  1.29990607e-01, -8.35037335e-02, -3.71612590e-01,\n       -4.82192709e-02,  4.67159910e-01, -6.86726804e-02, -2.59765665e-01,\n        1.54093090e-02, -1.90459810e-01,  1.67164711e-01,  2.80294950e-02,\n        6.73604594e-01, -8.76417696e-01,  1.27280453e-02, -8.95977933e-01,\n        1.12422846e-01, -6.19964999e-01, -8.18470009e-01, -8.77104622e-02,\n        6.84672003e-02, -5.20740069e-01,  2.41837006e-01,  2.38471623e-01,\n       -1.70214403e-01, -5.94699356e-02, -8.28698638e-02,  2.93632353e-01,\n       -3.11796852e-02,  3.59415072e-01, -3.10717332e-01, -4.33553283e-01,\n       -1.74261570e-01, -2.65915145e-01, -9.77115738e-01,  3.02465957e-01,\n       -5.16532058e-01,  8.90796449e-02,  4.74048429e-01,  3.79134113e-01,\n        9.51936301e-03, -6.54197254e-01,  6.84122816e-02, -3.71556619e-01,\n       -3.86808345e-01, -2.49879853e-01, -3.46421415e-01, -2.12385049e-01,\n       -2.83320485e-01, -1.87196735e-01, -1.73898150e-01,  5.57026152e-01,\n       -3.91014317e-01, -5.49817798e-01,  1.58333442e-01,  2.33475607e-01,\n        1.07955444e+00, -2.08737258e-02,  3.63253712e-01,  5.01679305e-02,\n       -4.94763520e-01,  9.62836678e-02,  2.27271208e-01,  1.29909658e-02,\n        1.56545056e-01, -2.92838735e-01,  1.93564794e-01,  8.56177145e-01,\n        2.88266439e-01,  3.53104979e-01,  1.15687204e-01,  6.92534873e-01,\n        1.44292959e-01,  1.32041387e-01, -5.36618136e-01, -1.01644725e-01,\n       -1.35401388e-01,  1.18527271e-01, -5.43875794e-01, -8.65321923e-01,\n        2.82692523e-01, -1.92087583e-01,  2.49619454e-01,  1.16897164e-01,\n        1.70480371e-01,  2.89760090e-02, -8.46568495e-03,  2.47779010e-01,\n        4.30876876e-01,  2.43990620e-01, -4.61159940e-01, -3.19501212e-01,\n        4.44020568e-01, -1.33298024e-01, -5.84627952e-01, -3.01093914e-01,\n       -3.06195098e-01, -2.09071724e-01,  6.30328399e-01, -3.27175632e-01,\n        1.07379441e-01,  2.27847298e-01,  1.81628202e-01, -9.76985824e-02,\n       -3.95640667e-01,  2.35997835e-01, -3.56498603e-02, -2.07494201e-01,\n       -1.20573195e-01,  5.17427984e-01,  1.45684891e-02, -7.46165808e-01,\n       -2.20584149e-03,  3.85549374e-01, -1.03009577e-01,  3.33648859e-01,\n        9.00792688e-02, -1.27409130e-01, -5.03017416e-01, -1.97924419e-01,\n        5.55553797e-01, -2.49667707e-01, -3.46736920e-01, -1.87617934e-01,\n        5.27421299e-01, -1.34454874e-01,  6.98338612e-02,  1.13531781e-01,\n        1.36931709e-01,  7.19903355e-02, -4.11836046e-01,  3.76241987e-01,\n        1.30961341e-02,  4.91874441e-01, -1.97188241e-01,  2.94578867e-01,\n        2.06973742e-01, -1.84305135e-01,  2.33212686e-01,  7.04128123e-02,\n        1.65692882e-01,  4.02007674e-01,  6.53833495e-01, -3.49208899e-01,\n        2.38419039e-01, -5.61571368e-02,  3.60677091e-01, -2.78143023e-02,\n        4.76835386e-01, -3.46264189e-01, -8.71822866e-02,  4.36398207e-01,\n        1.79839816e-01,  4.13524120e-01, -1.14000708e-01,  1.68377006e-01,\n       -6.69894631e-02, -4.71126701e-02, -3.79656149e-01, -1.40765493e-01,\n        4.76940555e-01,  4.50806253e-01, -2.55768747e-01, -5.97460809e-01,\n       -3.65246526e-01, -2.93206823e-01, -1.25989359e-01,  1.04905653e-01,\n        1.41664279e-01, -4.90588080e-02, -2.85634186e-01,  6.68873424e-02,\n       -4.62159564e-01,  6.11766209e-01, -1.05008299e-01, -1.61937402e-02,\n       -3.14876145e-01, -5.36095155e-01,  6.23542714e-01, -5.47663659e-01,\n        1.95244625e-01, -2.06021846e-01,  9.25749415e-03,  1.03329938e-01,\n        2.54299439e-01, -6.56246108e-02,  5.23764631e-02, -5.14904285e-01,\n       -8.83911952e-02,  7.05705646e-02, -1.48810336e-01,  2.41363749e-01,\n        1.43581527e-02,  6.05666452e-01, -2.67285193e-01,  1.42505099e-01,\n       -2.13962046e-01,  1.84305322e-01, -2.03077662e-01,  1.62222331e-01,\n        8.95534277e-02,  2.69809417e-01, -2.01289802e-01, -1.32983045e-01,\n       -7.45219294e-01, -1.14000182e-01, -2.93837833e-01, -4.19829357e-01,\n       -1.62902877e-01,  4.73732924e-01,  2.93106512e-01,  4.23147012e-01,\n        2.81064751e-01, -1.94348700e-01, -4.99178776e-01, -3.18638045e-02,\n        3.47607867e-02, -1.55015261e-01, -6.62874914e-01, -4.20637216e-03,\n        5.64254394e-02,  1.55861988e-01, -1.86646674e-02,  5.42145723e-02,\n       -1.70738435e-01,  1.31831051e-01, -8.66559196e-02, -5.21842001e-01,\n        6.93271577e-01,  2.60136276e-01, -6.91481195e-01, -2.31107326e-01,\n       -3.62196648e-01,  2.16752052e-01, -2.31209634e-01,  1.37718137e-01,\n        3.29336962e-01, -3.31172550e-01, -6.82514817e-02, -6.19911889e-01,\n       -2.73540367e-01, -4.71626512e-01,  1.69796777e-01,  4.14888972e-01,\n       -4.99565963e-03, -2.13856878e-01, -7.40067103e-01, -1.75838568e-01,\n       -3.54046637e-01, -3.28385066e-01,  7.78261199e-02, -2.43464591e-01,\n        4.44334023e-02, -1.49389287e-01, -2.92790820e-01,  5.90101556e-01,\n       -2.28741041e-01, -2.91263546e-01,  1.22891752e-01, -5.57389384e-01,\n       -6.90795267e-01, -7.14142086e-01, -1.82308747e-01, -1.48918364e-01,\n        4.61110403e-01,  1.38351480e-01, -3.24967625e-01, -3.99847395e-01,\n       -2.15325738e-03, -1.35401388e-01,  4.89505822e-01,  3.64434782e-02,\n       -4.30188426e-01,  3.72322398e-02,  3.83130505e-01,  5.23754114e-02,\n        2.40785324e-01,  3.01151880e-01, -3.75079754e-01, -6.11550490e-01,\n       -5.23316691e-01, -3.28017503e-01, -2.59344992e-01,  4.69210691e-01,\n        1.19155420e-01,  5.42775858e-01,  2.63709660e-01, -1.15577179e-01,\n       -1.95978281e-01,  1.97193098e-01, -3.62197174e-01, -3.87227209e-01,\n       -2.50718864e-01,  9.55474903e-02, -8.30281420e-02,  5.42986195e-01,\n        2.31215016e-01, -5.26865856e-02, -6.56616649e-01, -5.77319813e-01,\n        3.54945423e-01,  3.26497420e-01,  3.56417778e-01, -1.12264906e-01,\n        4.38554155e-01, -3.46106437e-01,  2.00137808e-01,  5.09016861e-01,\n       -5.59440690e-01, -3.21549132e-01,  7.48298774e-02, -1.01171468e-01,\n        2.83981096e-02, -8.88249120e-01,  5.46244075e-01,  2.92422918e-01,\n       -7.70330278e-02,  1.80050678e-01,  2.19173255e-01,  5.18955257e-01,\n       -1.96820153e-01, -1.72454223e-02, -2.52562168e-01,  3.80553884e-01,\n        6.06981055e-01, -6.85380156e-01,  7.73357174e-01,  1.31094873e-01,\n        5.50056685e-02, -1.45392369e-01, -5.15006593e-01,  3.62094527e-01,\n        1.77156218e-01,  3.64147642e-01, -7.61390980e-02, -1.44995362e+00,\n       -7.12512504e-01, -5.04332019e-01, -1.21992966e-01, -3.79104511e-02,\n        4.73785508e-01,  4.30140698e-01,  5.46404162e-01, -4.59165130e-01,\n       -1.29774889e-01, -3.30120868e-01,  8.01934564e-02, -4.86189976e-01,\n        4.66968535e-02,  4.11263003e-01, -7.33486455e-04, -2.23269959e-01,\n        1.43502388e-01, -1.32929935e-01, -5.36461435e-01,  2.73492639e-01,\n       -1.88778643e-02,  3.64515731e-01,  1.21472856e-02,  1.96165635e-02,\n        9.20248808e-02,  3.77819510e-01, -3.37274641e-01,  2.34580399e-01,\n        2.01820499e-01,  1.55230979e-01,  3.13561730e-01,  1.34933513e-01,\n       -1.42765498e-01,  1.69060074e-01, -3.53257349e-01,  3.48582746e-01,\n        4.29299352e-01,  2.28480642e-01, -2.39236041e-02, -4.64420155e-01,\n        2.23486026e-02, -2.90209529e-01, -1.06219543e-01,  5.51452236e-01,\n        7.82999027e-02, -2.21324347e-01,  4.69315859e-01, -1.77521785e-01,\n       -7.73405428e-01, -9.56187789e-01, -5.55601525e-01, -3.41952292e-01,\n        6.85646881e-01,  6.00302873e-01,  1.08641459e-01,  7.93521107e-02,\n       -4.62527127e-01,  3.05200856e-01,  2.04081616e-01, -5.06382799e-01,\n        9.89659831e-02,  6.07454312e-01, -4.53248104e-02, -6.16258840e-02,\n        4.69602999e-02,  3.35224048e-01, -2.59605578e-01,  3.01202130e-01,\n       -4.44598280e-01, -5.30020902e-02,  2.11705786e-01,  3.81027141e-01,\n        2.06079812e-01, -1.15892684e-01,  1.42029508e-01,  4.56982813e-02,\n       -4.12520165e-01, -1.10003264e-01, -3.14137633e-01, -4.40126822e-01,\n        3.54261830e-01,  1.65482546e-01, -1.60694870e-01, -3.10615024e-01,\n        3.35171464e-01, -8.00380549e-01, -3.87962861e-01,  4.96184004e-01,\n       -3.94851379e-01, -8.36140603e-01,  1.64380614e-01, -1.84304609e-01,\n       -1.82148661e-01, -1.58383504e-01, -2.66758826e-01, -1.06140912e+00,\n        3.64883820e-01, -7.44038224e-02,  1.79735174e-01,  3.25708659e-01,\n        3.00468287e-01,  7.14616340e-02,  4.33874170e-01, -1.78233217e-02,\n       -2.03605311e-01,  2.01399301e-01,  1.24361773e-01, -4.19829357e-01,\n        4.78675830e-01, -5.84182534e-02, -1.10897719e-01,  1.64117694e-01,\n        3.90965537e-01,  2.70863434e-01,  4.55749159e-01,  6.76779128e-02,\n        1.38561817e-01, -8.36666970e-01, -2.83932533e-02, -1.20257165e-01,\n       -1.02512789e+00, -1.31300354e-01,  4.98339952e-01, -1.17580570e-01,\n        7.90401008e-04, -3.02777131e-01, -2.20324723e-01, -3.46895198e-01,\n        4.65424635e-01, -7.33388921e-01, -6.78648864e-01,  1.45871008e-01,\n        1.65274544e-01,  1.03540275e-01, -6.90164784e-01, -6.40104713e-01,\n        3.55997105e-01,  5.40987999e-01,  2.41836480e-01,  2.72651293e-01,\n        1.35196434e-01,  3.33175602e-01,  6.74709386e-01,  1.11636419e-01,\n       -5.02596217e-01,  1.16160987e-01, -7.36229746e-01,  3.33543691e-01,\n        6.82037539e-02,  1.57439512e-01, -1.49496790e-01, -7.62151903e-01,\n        1.21209061e-01,  4.01482359e-01,  6.33693782e-01, -3.39163934e-02,\n        2.46779912e-01, -5.25786335e-01, -1.65006241e-01, -4.77303787e-01,\n        5.82476860e-01,  5.10092763e-02, -3.37430059e-01,  1.36668789e-01,\n        1.84152239e-01,  2.05395693e-01,  9.65471142e-02,  4.76887971e-01,\n        3.04254343e-01,  1.66852067e-01,  5.22181849e-02, -3.04459297e-01,\n       -6.67658260e-01,  6.13711821e-01, -5.74321467e-01,  2.40890492e-01,\n       -3.29857947e-01, -2.77119945e-02, -3.36535603e-01, -5.21736833e-01,\n       -4.07787596e-01,  3.61991694e-01, -1.32929935e-01,  1.59170118e-01,\n       -1.89405793e-01,  2.28164612e-01,  4.69053812e-02, -1.31667916e-01,\n       -2.01920285e-01,  2.81852986e-01, -9.98568654e-02, -6.22067838e-01,\n        1.48184709e-01, -6.15207158e-02,  2.68654375e-01,  1.85122972e-02,\n        7.75093324e-02, -1.23199144e+00, -2.42938224e-01, -1.51755572e-01,\n        3.76003286e-02,  6.97583474e-01, -4.65683225e-01, -2.75854068e-01,\n        1.42137536e-01, -4.52063415e-01, -2.71489587e-01, -1.98607487e-01,\n        3.53523318e-01, -3.88124000e-01, -2.01079991e-01, -1.96137085e-01,\n       -5.33989982e-01, -7.04992977e-01,  1.64615170e-02, -2.88684064e-01,\n       -4.95864577e-02, -2.17327955e-01, -6.56563539e-01, -6.37319038e-01,\n       -2.62129090e-01,  8.40846805e-02, -1.64848489e-01, -7.83481564e-02,\n       -1.16103546e-01,  7.89267687e-02, -8.17130135e-02,  2.50355631e-01,\n       -2.03392640e-01,  2.08756932e-01, -2.57663825e-02,  1.18790192e-01,\n       -3.41268699e-01, -1.87617408e-01, -4.28715545e-01,  5.81688099e-01,\n        4.38186067e-01,  5.92678177e-01, -1.85514569e-01, -7.58787571e-01,\n       -2.67598363e-01, -3.90152432e-02,  8.31515198e-01, -9.03899172e-02,\n        3.78187073e-01,  3.12825026e-01,  1.24645989e-02, -5.68594468e-01,\n        4.33188242e-01,  4.25460712e-01, -7.91178330e-01,  2.86954171e-01,\n        4.57905107e-01, -1.24622172e-01, -5.77581682e-01,  1.02489118e-01,\n        3.85601432e-01,  5.20269860e-01, -4.25350162e-01,  5.65595027e-01,\n        3.18160767e-02, -9.82244235e-02, -4.19146289e-01,  4.00483261e-01,\n        4.52015687e-01,  1.35617107e-01, -1.17627960e-01,  2.49796167e-02,\n        3.16033183e-01, -6.73049677e-02,  6.66267565e-02,  2.21749876e-01,\n        2.51775402e-01,  1.76945881e-01, -8.28150679e-01, -3.49136828e-02,\n       -1.29617136e-01,  4.67633167e-01, -3.16238663e-01,  4.04058980e-01,\n       -8.57329665e-01, -1.10161542e-01,  1.66192693e-02,  2.20327771e-01,\n       -3.57674415e-01, -1.44393271e-01, -4.86927731e-01,  2.65447270e-01,\n       -2.83401434e-02, -3.57781917e-01, -1.21887272e-01,  1.97193098e-01,\n        1.52178766e-01,  3.67721027e-01,  1.26151441e-01, -4.03583727e-01,\n        2.97050320e-01,  1.80471351e-01,  3.75137721e-01, -2.05391362e-01,\n       -1.58590980e-01, -3.05512031e-01,  3.55484967e-02,  2.49408592e-01,\n       -5.72637376e-02,  3.81290061e-01, -3.49944551e-01, -7.95599254e-02,\n       -6.42785976e-01,  5.66491291e-01,  2.45202389e-01,  7.16669170e-01,\n        1.96667257e-01,  3.74420504e-02,  7.21996203e-02, -2.09808427e-01,\n        1.02699455e-01, -6.06766914e-01,  2.12284211e-01,  2.47673842e-01,\n        1.03751137e-01, -3.39378005e-01,  5.14433024e-01,  3.11931622e-01,\n        8.08244657e-02,  3.82815001e-01,  4.85669517e-01, -1.65689835e-01,\n        1.43576268e-02,  1.23943435e-01, -9.25211202e-03, -1.41296003e-01,\n       -3.48262385e-01, -4.70309575e-01, -5.92200590e-01,  2.92422918e-01,\n        2.41574085e-01,  1.47921788e-01,  2.90264109e-01,  2.76435015e-01,\n        3.59677993e-01,  8.62648485e-03,  1.31305210e-01,  2.63764579e-01,\n       -1.38766771e-01,  6.32852436e-01,  1.84099655e-01, -5.98354213e-01,\n       -4.66997302e-01,  1.69165242e-01,  8.25597413e-02,  1.49393617e-01,\n       -1.64953657e-01, -9.16593764e-01, -1.38714713e-01,  3.72245595e-01,\n        6.82544419e-01, -1.72210264e-01,  3.26129332e-01,  3.42349457e-02,\n        2.59452682e-01,  4.86424655e-02,  4.15683563e-03,  3.66564176e-01,\n       -5.32937774e-01, -1.06743575e-01,  9.34972359e-02,  4.61189542e-02,\n       -3.55571576e-01, -2.41465869e-01, -8.50811570e-01, -8.15209794e-01,\n        7.33603588e-01,  2.96311808e-01,  2.27586712e-01,  7.05179805e-02,\n        7.28316813e-02, -3.68067107e-02,  1.24627028e-01,  1.29727161e-01,\n        3.37645251e-01,  7.79351762e-01, -3.61565639e-01, -2.93942475e-01,\n       -1.23572298e-01, -1.79574374e-01,  1.16265629e-01,  2.44098122e-01,\n        4.77043388e-01,  5.62151030e-02,  2.02764679e-01, -1.64112837e-01,\n        9.34446518e-02,  3.41483891e-01, -2.83639376e-01,  1.06800489e-01,\n        7.28293467e-02,  1.96982761e-01, -7.46061165e-01,  5.17903575e-01,\n       -1.79572039e-01, -3.78971504e-01,  3.03936503e-01,  5.34046896e-01,\n       -7.85656999e-01,  5.78977945e-02,  1.06169480e-01,  1.74424179e-01,\n       -3.86648784e-01,  7.35969873e-01, -1.93772083e-01,  4.96078836e-01,\n       -4.97967942e-02, -6.05793968e-02,  1.05907085e-01,  3.02361315e-01,\n        2.58558752e-01,  5.90942902e-01, -6.58718961e-01,  7.35601784e-01,\n        2.39628473e-01,  4.59061958e-01, -1.96138368e-01, -4.55954639e-01,\n       -4.09259951e-01, -5.08909171e-01,  1.35141515e-01,  2.43467113e-01,\n       -4.29613859e-02, -3.37643255e-01,  1.81733370e-01,  1.07063936e-01,\n       -2.45197007e-01, -2.26900071e-01, -1.05218110e-01, -3.49734214e-01,\n       -1.83674651e-01, -6.11445322e-01,  2.14335517e-01, -4.51800495e-01,\n       -5.26345273e-02,  7.22876430e-01, -3.20129361e-01, -4.65892510e-01,\n       -8.32036183e-01,  4.84407498e-01, -3.34958080e-01,  7.51974403e-02,\n       -7.40749645e-01,  4.66371149e-01, -3.81213134e-02,  2.39786226e-01,\n        1.37773055e-01,  1.96351752e-01,  3.88651837e-01,  7.02550599e-02,\n       -4.74990086e-01, -2.02867325e-01, -2.44460829e-01,  1.37667361e-01,\n        4.63005766e-01, -4.79039589e-01,  4.06109761e-01, -6.13629635e-02,\n       -6.69603346e-01,  2.16281129e-01, -1.13158310e-01,  6.22177862e-01,\n        1.21892654e-01, -1.67530278e-01, -3.24021111e-01, -2.63393443e-01,\n        5.03916202e-01, -8.01360160e-02, -2.12122654e-01,  3.22921701e-01,\n        4.25618465e-01, -2.87527740e-01, -8.38689619e-02, -2.98152064e-01,\n        5.19165594e-01,  1.06012254e-01, -2.66338153e-01, -6.14495200e-01,\n        2.40101730e-01, -5.12639782e-01,  1.67233859e-02,  7.39861971e-02,\n        7.10045907e-01,  3.34987681e-02, -2.70595657e-01,  2.11127886e-01,\n        6.39952165e-02,  6.33586279e-01,  1.66589147e-01,  1.82099124e-01,\n        2.15124279e-01, -2.36047898e-01, -2.61290604e-01, -2.89527745e-01,\n       -3.14871476e-01,  3.53419958e-01,  1.74075051e-02, -2.10228574e-01,\n       -1.50809058e-01, -2.57640479e-02,  8.65035494e-02,  2.45988816e-01,\n       -2.44829444e-01, -1.01734311e+00,  7.75828627e-01,  4.74411322e-01,\n       -1.74051234e-01, -8.61903430e-01,  3.33017850e-01,  3.87337234e-01,\n       -2.46071975e-02,  1.44135732e-01,  2.47774341e-01,  1.51129419e-01,\n        3.90176776e-01, -1.64561349e-02,  4.53804421e-02,  2.72178036e-01,\n        1.51970764e-01, -1.40081374e-01, -3.67087496e-01,  4.03007298e-01,\n        1.76178416e-02,  2.54562360e-01, -2.25167656e-01, -1.40819886e-01,\n       -3.22313674e-02, -2.00872988e-02,  9.85973685e-02,  4.96449259e-01,\n        5.70485349e-01, -1.74471381e-01, -1.75365836e-01,  1.34723177e-01,\n        7.62738679e-03,  1.10797408e-01,  2.25220428e-01,  2.63869747e-01,\n        4.52278608e-01, -1.48390715e-01,  5.92467841e-01,  3.12036791e-01,\n        6.74681022e-02, -5.74794198e-01,  7.86159332e-02, -3.35957178e-01,\n       -2.91681358e-01,  2.38103008e-01,  4.91348600e-01, -1.46444051e-01,\n       -2.38150736e-01, -5.34883912e-01,  2.39628473e-01, -9.46492300e-02,\n        4.73154499e-01, -4.90765319e-01,  1.05591581e-01, -4.65342449e-02,\n       -3.16974315e-01,  3.95380268e-01,  5.61863890e-01,  1.49131223e-01,\n       -4.06472467e-01,  6.40496093e-02, -1.29775414e-01,  7.49168484e-01,\n       -2.68544351e-01, -3.26023976e-01,  6.36848828e-01,  3.95750691e-01,\n        2.05133298e-01,  3.74033454e-01, -2.74696691e-01,  5.41198335e-01,\n       -1.39240028e-01, -7.06254995e-01, -6.22910235e-01, -4.18304943e-01,\n        3.00203032e-01, -6.08764584e-01,  4.82356718e-01, -3.35641673e-01,\n        2.13914844e-01,  6.65215882e-02,  4.63584191e-01,  2.57033287e-01,\n       -2.10386327e-01,  1.45500585e-01,  1.63279843e-03,  7.71961623e-02,\n       -3.11453509e-01,  1.46396849e-01,  3.09775674e-01, -5.26942660e-01,\n       -2.53820800e-01, -5.27786866e-01, -2.97097522e-01,  5.10593858e-01,\n       -4.35657173e-01, -1.23360153e-01, -1.37820783e-01, -2.23112207e-01,\n        2.58611336e-01,  1.57965353e-01, -5.15795354e-01, -1.31930837e-01,\n       -1.36085507e-01, -6.39262315e-01,  2.12387571e-01,  4.05426167e-01,\n       -1.83253453e-01,  1.51128893e-01,  5.02338679e-01, -1.39240028e-01,\n        4.72758046e-02,  4.37526693e-02,  3.28072609e-01, -3.29149608e-02,\n       -5.77266177e-01,  4.73572837e-01, -3.12322694e-02,  2.12337321e-01,\n        3.60834317e-01, -1.61431048e-01,  2.41205996e-01,  3.70981242e-01,\n        2.24957507e-01,  5.41117386e-02, -4.32239206e-01])\n\n\n\nsig[0][-1]\n\n626\n\n\n\nli.Is[626]\n\n1.1256660313590514\n\n\n\nobserved = li.Is[626]\nnull = li.rlisas[626,:]\n\n\n(null &gt;= observed).sum()\n\n0\n\n\n\nli.p_sim[626]\n\n0.001\n\n\n\nimport seaborn as sbn\nsbn.kdeplot(null, shade=True)\nplt.vlines(observed,0, 0.2, color='r')\nplt.xlabel(\"Local I for tract 626\")\n\nText(0.5, 0, 'Local I for tract 626')\n\n\n\n\n\n\n\n\n\n\nnsig = np.where(li.p_sim &gt; 0.05)\n\n\nnsig[0][0]\n\n0\n\n\n\nobserved = li.Is[0]\nnull = li.rlisas[0,:]\n\n\nimport seaborn as sbn\nsbn.kdeplot(null, shade=True)\nplt.vlines(observed,0, 2.0, color='r')\nplt.xlabel(\"Local I for tract 0\")\n\nText(0.5, 0, 'Local I for tract 0')\n\n\n\n\n\n\n\n\n\n\nli.p_sim[0]\n\n0.46",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html#imports",
    "href": "week-11/04_local_spatial_autocorrelation.html#imports",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport esda\nimport libpysal as lps\nimport contextily as cx\n\n\nimport seaborn as sns\nsns.set_context('notebook')\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(\"ignore\")",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html#data",
    "href": "week-11/04_local_spatial_autocorrelation.html#data",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "For this exercise, we’ll use two datasets:\n\na set of polygons (census tracts) for the city of San Diego from the US Census American Community Survey 5-year estimates.\n\n\n\n\nscag = gpd.read_parquet(\"~/data/scag_region.parquet\")\n\n\nsan_diego = scag[scag.geoid.str[:5]=='06073']\n\n\nsan_diego.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 627 entries, 158 to 4567\nColumns: 194 entries, geoid to geometry\ndtypes: float64(191), geometry(1), int64(1), object(1)\nmemory usage: 955.2+ KB\n\n\n\nsan_diego = san_diego.dropna(subset=['median_home_value'])\n\n\nsan_diego = san_diego.to_crs(epsg=3857)\n\n\nf, ax = plt.subplots(figsize=(10,10))\n\nsan_diego.plot('median_home_value', ax=ax, alpha=0.6)\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\n\n\n\n\n\n\n\n\nsan_diego.median_home_value.hist()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12,12))\n\nsan_diego.dropna(subset=['median_home_value']).to_crs(epsg=3857).plot('median_home_value', legend=True, scheme='quantiles', cmap='GnBu', k=5, ax=ax, alpha=0.7)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Median Home Value (Quintiles)\", fontsize=16)\n\nplt.axis('off')\nplt.tight_layout()",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html#spatial-autocorrelation",
    "href": "week-11/04_local_spatial_autocorrelation.html#spatial-autocorrelation",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "Visual inspection of the map pattern for the prices allows us to search for spatial structure. If the spatial distribution of the prices was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the darker clusters along the coast, and a concentration of the lighter hues (lower prices) in the north central and south east. In the point data, the trees are too dense to make any sense of the pattern\nOur brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes.\nThe concept of spatial autocorrelation relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\nLet’s use PySAL to generate these two types of similarity measures.\n\n\nWe have already encountered spatial weights in a previous notebook. In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:\n\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'r'\n\n\n\n\nSo the spatial weight between neighborhoods \\(i\\) and \\(j\\) indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood \\(i\\) the spatial lag is defined as: \\[ylag_i = \\sum_j w_{i,j} y_j\\]\n\ny = san_diego['median_home_value']\nylag = lps.weights.lag_spatial(wq, y)\n\n\nf, ax = plt.subplots(1, figsize=(12, 12))\n\nsan_diego.assign(cl=ylag).plot(column='cl', scheme='quantiles', \\\n        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n        edgecolor='white', legend=True)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Spatial Lag Median Home Val (Quintiles)\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother.\n\nsan_diego['lag_median_pri'] = ylag\n\nf,ax = plt.subplots(1,2,figsize=(12,4))\n\nsan_diego.plot(column='median_home_value', ax=ax[0],\n        scheme=\"quantiles\",  k=5, cmap='GnBu')\n\n#ax[0].axis(san_diego.total_bounds[np.asarray([0,2,1,3])])\nax[0].set_title(\"Price\", fontsize=16)\n\nsan_diego.plot(column='lag_median_pri', ax=ax[1],\n        scheme='quantiles', cmap='GnBu', k=5)\n\ncx.add_basemap(ax[0], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\ncx.add_basemap(ax[1], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax[1].set_title(\"Spatial Lag Price\", fontsize=16)\nax[0].axis('off')\nax[1].axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nHowever, we still have the challenge of visually associating the value of the prices in a neighborhod with the value of the spatial lag of values for the focal unit. The latter is a weighted average of home prices in the focal county’s neighborhood.\nTo complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation.",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html#join-counts",
    "href": "week-11/04_local_spatial_autocorrelation.html#join-counts",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "One way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called joins. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\nEach unit can take on one of two values “Black” or “White”, analogous to the layout of a chessboard\n\nnrows, ncols = 9,9\nimage = np.zeros(nrows*ncols)\n\n# Set every other cell to 1\nimage[::2] = 1\n\n# Reshape things into a 9x9 grid.\nimage = image.reshape((nrows, ncols))\nplt.matshow(image, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nand so for a given pair of neighboring locations there are three different types of joins that can arise:\n\nBlack Black (BB)\nWhite White (WW)\nBlack White (or White Black) (BW)\n\nWe can use the esda package from PySAL to carry out join count analysis. In the case of our point data, the join counts can help us determine whether different varieties of trees tend to grow together, spread randomly through space, or compete with one another for precious resources\n\n\nWith polygon data, we can conduct an analysis using a contiguity matrix. For our housing price data, we need to first discretize the variable we’re using; to keep things simple, we’ll binarize our price data using the median so that “high” values are tracts whose median home price is above the city’s median and “low” values are those below\n\ny.median()\n\n405416.57303370786\n\n\n\nsan_diego.shape\n\n(627, 195)\n\n\n\nyb = y &gt; y.median()\nsum(yb)\n\n313\n\n\n\nyb = y &gt; y.median()\nlabels = [\"0 Low\", \"1 High\"]\nyb = [labels[i] for i in 1*yb] \nsan_diego['yb'] = yb\n\n\nfig, ax = plt.subplots(figsize=(12,12))\nsan_diego.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)\n\n\n\n\n\n\n\n\nThe spatial distribution of the binary variable immediately raises questions about the juxtaposition of the “black” and “white” areas.\nGiven that we have 308 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map?\n\nyb = 1 * (y &gt; y.median()) # convert back to binary\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'b'\nnp.random.seed(12345)\njc = esda.join_counts.Join_Counts(yb, wq)\n\nThe resulting object stores the observed counts for the different types of joins:\n\njc.bb\n\n754.0\n\n\n\njc.ww\n\n745.0\n\n\n\njc.bw\n\n475.0\n\n\nNote that the three cases exhaust all possibilities:\n\njc.bb + jc.ww + jc.bw\n\n1974.0\n\n\nand\n\nwq.s0 / 2\n\n1974.0\n\n\nwhich is the unique number of joins in the spatial weights object.\nOur object tells us we have observed 736 BB joins:\n\njc.bb\n\n754.0\n\n\nThe critical question for us, is whether this is a departure from what we would expect if the process generating the spatial distribution of the Black polygons were a completely random one? To answer this, PySAL uses random spatial permutations of the observed attribute values to generate a realization under the null of complete spatial randomness (CSR). This is repeated a large number of times (999 default) to construct a reference distribution to evaluate the statistical significance of our observed counts.\nThe average number of BB joins from the synthetic realizations is:\n\njc.mean_bb\n\n490.03103103103103\n\n\nwhich is less than our observed count. The question is whether our observed value is so different from the expectation that we would reject the null of CSR?\n\nimport seaborn as sbn\nsbn.kdeplot(jc.sim_bb, shade=True)\nplt.vlines(jc.bb, 0, 0.005, color='r')\nplt.vlines(jc.mean_bb, 0,0.005)\nplt.xlabel('BB Counts')\n\nText(0.5, 0, 'BB Counts')\n\n\n\n\n\n\n\n\n\nThe density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for our prices. Clearly our observed value is extremely high. A pseudo p-value summarizes this:\n\njc.p_sim_bb\n\n0.001\n\n\nSince this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in market prices.\n\n\n\nThe join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\nFirst, we transform our weights to be row-standardized, from the current binary state:\n\nwq.transform = 'r'\n\n\ny = san_diego['median_home_value']\n\nMoran’s I is a test for global autocorrelation for a continuous attribute:\n\nnp.random.seed(12345)\nmi = esda.moran.Moran(y, wq)\nmi.I\n\n0.660917168991019\n\n\nAgain, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations.\n\nfrom splot.esda import plot_moran\nplot_moran(mi, zstandard=True, figsize=(10,4))\nplt.show()\n\n\n\n\n\n\n\n\n\nmi.p_sim\n\n0.001\n\n\nOn the left, we have the reference distribution versus the observed statistic; on the right, we have a plot of the focal value against its spatial lag, for which the Moran I statistic serves as the slope\nHere our observed value is again in the upper tail\n\nmi.p_sim\n\n0.001",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-11/04_local_spatial_autocorrelation.html#local-autocorrelation-hot-spots-cold-spots-and-spatial-outliers",
    "href": "week-11/04_local_spatial_autocorrelation.html#local-autocorrelation-hot-spots-cold-spots-and-spatial-outliers",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "In addition to the Global autocorrelation statistics, PySAL has many local autocorrelation statistics. Let’s compute a local Moran statistic for the same data\n\nnp.random.seed(12345)\n\n\nwq.transform = 'r'\nlag_price = lps.weights.lag_spatial(wq, san_diego['median_home_value'])\n\n\nli = esda.moran.Moran_Local(y, wq)\n\nNow, instead of a single \\(I\\) statistic, we have an array of local \\(I_i\\) statistics, stored in the .Is attribute, and p-values from the simulation are in p_sim.\n\nfrom splot.esda import moran_scatterplot\n\nfig, ax = moran_scatterplot(li)\nax.set_xlabel('Price')\nax.set_ylabel('Spatial Lag of Price')\nplt.show()\n\n\n\n\n\n\n\n\n\nli.q\n\narray([4, 1, 2, 3, 3, 3, 3, 4, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 2, 3, 2, 2, 1, 1, 2, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 2, 1, 1, 1,\n       3, 1, 3, 3, 4, 3, 1, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 2,\n       2, 1, 2, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1,\n       3, 3, 2, 2, 2, 1, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 2,\n       2, 4, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3,\n       1, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 1,\n       1, 3, 3, 3, 3, 1, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1,\n       1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 3, 3, 2, 3, 4, 4, 2, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 1, 3, 3, 2, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 3, 3, 3, 4, 1, 3, 3, 1, 2, 4, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2,\n       2, 2, 3, 3, 3, 3, 3, 3, 1, 1, 1, 2, 4, 3, 3, 1, 3, 3, 3, 3, 3, 1,\n       3, 3, 3, 2, 1, 1, 1, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4,\n       1, 1, 1, 1, 3, 1, 3, 4, 4, 4, 3, 2, 1, 1, 2, 3, 3, 2, 3, 4, 3, 3,\n       3, 3, 3, 4, 3, 3, 4, 1, 3, 3, 3, 4, 1, 3, 3, 3, 3, 3, 4, 4, 3, 2,\n       3, 3, 1, 1, 4, 4, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 1, 2, 3, 4,\n       1, 1, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 4, 3, 1, 3, 3, 2, 1, 2, 3, 3,\n       4, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 1, 3, 3, 4, 1, 1, 1, 1, 2, 1,\n       1, 2, 3, 3, 3, 3, 4, 2, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 2, 4, 1, 3,\n       3, 3, 2, 1, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 4, 2, 1, 1, 3, 3, 1,\n       3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 2, 1,\n       1, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1,\n       1, 1, 3, 1, 3, 3, 2, 3, 1, 1, 3, 3, 3, 3, 4, 2, 1, 1, 1, 2, 3, 1,\n       3, 3, 1, 1, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1,\n       3, 3, 1, 4, 1, 3, 1, 2, 1, 3, 1, 3, 3, 3, 3, 4, 3, 3, 4, 1, 4, 3,\n       3, 3, 3, 2, 3, 1, 1, 4, 1, 1, 1, 1, 3, 3, 1, 3, 3, 2, 2, 3, 2, 4,\n       1, 1, 1, 1, 3, 3, 4, 1, 3, 1, 2, 3, 1, 1, 1, 1, 2, 2, 3, 3, 3, 1,\n       3, 2, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 4,\n       4, 3, 3, 3, 3, 3, 1, 3, 4, 3, 3])\n\n\nWe can again test for local clustering using permutations, but here we use conditional random permutations (different distributions for each focal location)\n\n(li.p_sim &lt; 0.05).sum()\n\n246\n\n\n\nfig, ax = moran_scatterplot(li, p=0.05)\nax.set_xlabel('Price')\nax.set_ylabel('Spatial Lag of Price')\nplt.show()\n\n\n\n\n\n\n\n\nWe can distinguish the specific type of local spatial association reflected in the four quadrants of the Moran Scatterplot above: - High-High (upper right) - Low-Low (bottom left) - High-Low (lower right) - Low-High (upper left)\nUsing splot, we can also plot these hotspots on the original geodataframe\n\nfrom splot.esda import lisa_cluster\nlisa_cluster(li, san_diego, p=0.05, figsize = (9,9))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom splot.esda import plot_local_autocorrelation\nplot_local_autocorrelation(li, san_diego, 'median_home_value')\n\n(&lt;Figure size 1500x400 with 3 Axes&gt;,\n array([&lt;Axes: title={'center': 'Moran Local Scatterplot'}, xlabel='Attribute', ylabel='Spatial Lag'&gt;,\n        &lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object))\n\n\n\n\n\n\n\n\n\n\nli.Is.shape\n\n(627,)\n\n\n\ndir(li)\n\n['EI',\n 'EI_sim',\n 'EIc',\n 'Is',\n 'VI',\n 'VI_sim',\n 'VIc',\n '_Moran_Local__calc',\n '_Moran_Local__moments',\n '_Moran_Local__quads',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_statistic',\n 'by_col',\n 'den',\n 'geoda_quads',\n 'n',\n 'n_1',\n 'p_sim',\n 'p_z_sim',\n 'permutations',\n 'q',\n 'quads',\n 'rlisas',\n 'seI_sim',\n 'sim',\n 'w',\n 'y',\n 'z',\n 'z_sim']\n\n\n\nnp.random.seed(12345)\n\n\nli = esda.moran.Moran_Local(y, wq, keep_simulations=True)\n\n\nsig = np.where(li.p_sim&lt;0.05)\n\n\ndir(li)\n\n['EI',\n 'EI_sim',\n 'EIc',\n 'Is',\n 'VI',\n 'VI_sim',\n 'VIc',\n '_Moran_Local__calc',\n '_Moran_Local__moments',\n '_Moran_Local__quads',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_statistic',\n 'by_col',\n 'den',\n 'geoda_quads',\n 'n',\n 'n_1',\n 'p_sim',\n 'p_z_sim',\n 'permutations',\n 'q',\n 'quads',\n 'rlisas',\n 'seI_sim',\n 'sim',\n 'w',\n 'y',\n 'z',\n 'z_sim']\n\n\n\nli.rlisas.shape\n\n(627, 999)\n\n\n\nli.rlisas[sig[0][-1],:]\n\narray([ 3.42798494e-01, -4.45389092e-02,  6.44999365e-01,  1.01595189e-01,\n        5.57418460e-02, -9.65445924e-02, -5.18269142e-01, -1.46339409e-01,\n        7.45014339e-01,  1.78287038e-02,  4.44180655e-01, -3.04512933e-01,\n       -3.15555070e-01, -1.76574745e-01,  1.14373127e-01,  3.18195714e-04,\n        1.73582307e-01,  5.62915572e-01, -2.63419473e-02,  3.00282170e-02,\n        5.59182100e-01, -1.74789220e-01,  1.55283563e-01, -3.12980257e-01,\n        2.44676548e-01, -1.70737909e-01,  5.17219982e-01, -4.43597899e-01,\n       -1.74552329e-02,  1.29990607e-01, -8.35037335e-02, -3.71612590e-01,\n       -4.82192709e-02,  4.67159910e-01, -6.86726804e-02, -2.59765665e-01,\n        1.54093090e-02, -1.90459810e-01,  1.67164711e-01,  2.80294950e-02,\n        6.73604594e-01, -8.76417696e-01,  1.27280453e-02, -8.95977933e-01,\n        1.12422846e-01, -6.19964999e-01, -8.18470009e-01, -8.77104622e-02,\n        6.84672003e-02, -5.20740069e-01,  2.41837006e-01,  2.38471623e-01,\n       -1.70214403e-01, -5.94699356e-02, -8.28698638e-02,  2.93632353e-01,\n       -3.11796852e-02,  3.59415072e-01, -3.10717332e-01, -4.33553283e-01,\n       -1.74261570e-01, -2.65915145e-01, -9.77115738e-01,  3.02465957e-01,\n       -5.16532058e-01,  8.90796449e-02,  4.74048429e-01,  3.79134113e-01,\n        9.51936301e-03, -6.54197254e-01,  6.84122816e-02, -3.71556619e-01,\n       -3.86808345e-01, -2.49879853e-01, -3.46421415e-01, -2.12385049e-01,\n       -2.83320485e-01, -1.87196735e-01, -1.73898150e-01,  5.57026152e-01,\n       -3.91014317e-01, -5.49817798e-01,  1.58333442e-01,  2.33475607e-01,\n        1.07955444e+00, -2.08737258e-02,  3.63253712e-01,  5.01679305e-02,\n       -4.94763520e-01,  9.62836678e-02,  2.27271208e-01,  1.29909658e-02,\n        1.56545056e-01, -2.92838735e-01,  1.93564794e-01,  8.56177145e-01,\n        2.88266439e-01,  3.53104979e-01,  1.15687204e-01,  6.92534873e-01,\n        1.44292959e-01,  1.32041387e-01, -5.36618136e-01, -1.01644725e-01,\n       -1.35401388e-01,  1.18527271e-01, -5.43875794e-01, -8.65321923e-01,\n        2.82692523e-01, -1.92087583e-01,  2.49619454e-01,  1.16897164e-01,\n        1.70480371e-01,  2.89760090e-02, -8.46568495e-03,  2.47779010e-01,\n        4.30876876e-01,  2.43990620e-01, -4.61159940e-01, -3.19501212e-01,\n        4.44020568e-01, -1.33298024e-01, -5.84627952e-01, -3.01093914e-01,\n       -3.06195098e-01, -2.09071724e-01,  6.30328399e-01, -3.27175632e-01,\n        1.07379441e-01,  2.27847298e-01,  1.81628202e-01, -9.76985824e-02,\n       -3.95640667e-01,  2.35997835e-01, -3.56498603e-02, -2.07494201e-01,\n       -1.20573195e-01,  5.17427984e-01,  1.45684891e-02, -7.46165808e-01,\n       -2.20584149e-03,  3.85549374e-01, -1.03009577e-01,  3.33648859e-01,\n        9.00792688e-02, -1.27409130e-01, -5.03017416e-01, -1.97924419e-01,\n        5.55553797e-01, -2.49667707e-01, -3.46736920e-01, -1.87617934e-01,\n        5.27421299e-01, -1.34454874e-01,  6.98338612e-02,  1.13531781e-01,\n        1.36931709e-01,  7.19903355e-02, -4.11836046e-01,  3.76241987e-01,\n        1.30961341e-02,  4.91874441e-01, -1.97188241e-01,  2.94578867e-01,\n        2.06973742e-01, -1.84305135e-01,  2.33212686e-01,  7.04128123e-02,\n        1.65692882e-01,  4.02007674e-01,  6.53833495e-01, -3.49208899e-01,\n        2.38419039e-01, -5.61571368e-02,  3.60677091e-01, -2.78143023e-02,\n        4.76835386e-01, -3.46264189e-01, -8.71822866e-02,  4.36398207e-01,\n        1.79839816e-01,  4.13524120e-01, -1.14000708e-01,  1.68377006e-01,\n       -6.69894631e-02, -4.71126701e-02, -3.79656149e-01, -1.40765493e-01,\n        4.76940555e-01,  4.50806253e-01, -2.55768747e-01, -5.97460809e-01,\n       -3.65246526e-01, -2.93206823e-01, -1.25989359e-01,  1.04905653e-01,\n        1.41664279e-01, -4.90588080e-02, -2.85634186e-01,  6.68873424e-02,\n       -4.62159564e-01,  6.11766209e-01, -1.05008299e-01, -1.61937402e-02,\n       -3.14876145e-01, -5.36095155e-01,  6.23542714e-01, -5.47663659e-01,\n        1.95244625e-01, -2.06021846e-01,  9.25749415e-03,  1.03329938e-01,\n        2.54299439e-01, -6.56246108e-02,  5.23764631e-02, -5.14904285e-01,\n       -8.83911952e-02,  7.05705646e-02, -1.48810336e-01,  2.41363749e-01,\n        1.43581527e-02,  6.05666452e-01, -2.67285193e-01,  1.42505099e-01,\n       -2.13962046e-01,  1.84305322e-01, -2.03077662e-01,  1.62222331e-01,\n        8.95534277e-02,  2.69809417e-01, -2.01289802e-01, -1.32983045e-01,\n       -7.45219294e-01, -1.14000182e-01, -2.93837833e-01, -4.19829357e-01,\n       -1.62902877e-01,  4.73732924e-01,  2.93106512e-01,  4.23147012e-01,\n        2.81064751e-01, -1.94348700e-01, -4.99178776e-01, -3.18638045e-02,\n        3.47607867e-02, -1.55015261e-01, -6.62874914e-01, -4.20637216e-03,\n        5.64254394e-02,  1.55861988e-01, -1.86646674e-02,  5.42145723e-02,\n       -1.70738435e-01,  1.31831051e-01, -8.66559196e-02, -5.21842001e-01,\n        6.93271577e-01,  2.60136276e-01, -6.91481195e-01, -2.31107326e-01,\n       -3.62196648e-01,  2.16752052e-01, -2.31209634e-01,  1.37718137e-01,\n        3.29336962e-01, -3.31172550e-01, -6.82514817e-02, -6.19911889e-01,\n       -2.73540367e-01, -4.71626512e-01,  1.69796777e-01,  4.14888972e-01,\n       -4.99565963e-03, -2.13856878e-01, -7.40067103e-01, -1.75838568e-01,\n       -3.54046637e-01, -3.28385066e-01,  7.78261199e-02, -2.43464591e-01,\n        4.44334023e-02, -1.49389287e-01, -2.92790820e-01,  5.90101556e-01,\n       -2.28741041e-01, -2.91263546e-01,  1.22891752e-01, -5.57389384e-01,\n       -6.90795267e-01, -7.14142086e-01, -1.82308747e-01, -1.48918364e-01,\n        4.61110403e-01,  1.38351480e-01, -3.24967625e-01, -3.99847395e-01,\n       -2.15325738e-03, -1.35401388e-01,  4.89505822e-01,  3.64434782e-02,\n       -4.30188426e-01,  3.72322398e-02,  3.83130505e-01,  5.23754114e-02,\n        2.40785324e-01,  3.01151880e-01, -3.75079754e-01, -6.11550490e-01,\n       -5.23316691e-01, -3.28017503e-01, -2.59344992e-01,  4.69210691e-01,\n        1.19155420e-01,  5.42775858e-01,  2.63709660e-01, -1.15577179e-01,\n       -1.95978281e-01,  1.97193098e-01, -3.62197174e-01, -3.87227209e-01,\n       -2.50718864e-01,  9.55474903e-02, -8.30281420e-02,  5.42986195e-01,\n        2.31215016e-01, -5.26865856e-02, -6.56616649e-01, -5.77319813e-01,\n        3.54945423e-01,  3.26497420e-01,  3.56417778e-01, -1.12264906e-01,\n        4.38554155e-01, -3.46106437e-01,  2.00137808e-01,  5.09016861e-01,\n       -5.59440690e-01, -3.21549132e-01,  7.48298774e-02, -1.01171468e-01,\n        2.83981096e-02, -8.88249120e-01,  5.46244075e-01,  2.92422918e-01,\n       -7.70330278e-02,  1.80050678e-01,  2.19173255e-01,  5.18955257e-01,\n       -1.96820153e-01, -1.72454223e-02, -2.52562168e-01,  3.80553884e-01,\n        6.06981055e-01, -6.85380156e-01,  7.73357174e-01,  1.31094873e-01,\n        5.50056685e-02, -1.45392369e-01, -5.15006593e-01,  3.62094527e-01,\n        1.77156218e-01,  3.64147642e-01, -7.61390980e-02, -1.44995362e+00,\n       -7.12512504e-01, -5.04332019e-01, -1.21992966e-01, -3.79104511e-02,\n        4.73785508e-01,  4.30140698e-01,  5.46404162e-01, -4.59165130e-01,\n       -1.29774889e-01, -3.30120868e-01,  8.01934564e-02, -4.86189976e-01,\n        4.66968535e-02,  4.11263003e-01, -7.33486455e-04, -2.23269959e-01,\n        1.43502388e-01, -1.32929935e-01, -5.36461435e-01,  2.73492639e-01,\n       -1.88778643e-02,  3.64515731e-01,  1.21472856e-02,  1.96165635e-02,\n        9.20248808e-02,  3.77819510e-01, -3.37274641e-01,  2.34580399e-01,\n        2.01820499e-01,  1.55230979e-01,  3.13561730e-01,  1.34933513e-01,\n       -1.42765498e-01,  1.69060074e-01, -3.53257349e-01,  3.48582746e-01,\n        4.29299352e-01,  2.28480642e-01, -2.39236041e-02, -4.64420155e-01,\n        2.23486026e-02, -2.90209529e-01, -1.06219543e-01,  5.51452236e-01,\n        7.82999027e-02, -2.21324347e-01,  4.69315859e-01, -1.77521785e-01,\n       -7.73405428e-01, -9.56187789e-01, -5.55601525e-01, -3.41952292e-01,\n        6.85646881e-01,  6.00302873e-01,  1.08641459e-01,  7.93521107e-02,\n       -4.62527127e-01,  3.05200856e-01,  2.04081616e-01, -5.06382799e-01,\n        9.89659831e-02,  6.07454312e-01, -4.53248104e-02, -6.16258840e-02,\n        4.69602999e-02,  3.35224048e-01, -2.59605578e-01,  3.01202130e-01,\n       -4.44598280e-01, -5.30020902e-02,  2.11705786e-01,  3.81027141e-01,\n        2.06079812e-01, -1.15892684e-01,  1.42029508e-01,  4.56982813e-02,\n       -4.12520165e-01, -1.10003264e-01, -3.14137633e-01, -4.40126822e-01,\n        3.54261830e-01,  1.65482546e-01, -1.60694870e-01, -3.10615024e-01,\n        3.35171464e-01, -8.00380549e-01, -3.87962861e-01,  4.96184004e-01,\n       -3.94851379e-01, -8.36140603e-01,  1.64380614e-01, -1.84304609e-01,\n       -1.82148661e-01, -1.58383504e-01, -2.66758826e-01, -1.06140912e+00,\n        3.64883820e-01, -7.44038224e-02,  1.79735174e-01,  3.25708659e-01,\n        3.00468287e-01,  7.14616340e-02,  4.33874170e-01, -1.78233217e-02,\n       -2.03605311e-01,  2.01399301e-01,  1.24361773e-01, -4.19829357e-01,\n        4.78675830e-01, -5.84182534e-02, -1.10897719e-01,  1.64117694e-01,\n        3.90965537e-01,  2.70863434e-01,  4.55749159e-01,  6.76779128e-02,\n        1.38561817e-01, -8.36666970e-01, -2.83932533e-02, -1.20257165e-01,\n       -1.02512789e+00, -1.31300354e-01,  4.98339952e-01, -1.17580570e-01,\n        7.90401008e-04, -3.02777131e-01, -2.20324723e-01, -3.46895198e-01,\n        4.65424635e-01, -7.33388921e-01, -6.78648864e-01,  1.45871008e-01,\n        1.65274544e-01,  1.03540275e-01, -6.90164784e-01, -6.40104713e-01,\n        3.55997105e-01,  5.40987999e-01,  2.41836480e-01,  2.72651293e-01,\n        1.35196434e-01,  3.33175602e-01,  6.74709386e-01,  1.11636419e-01,\n       -5.02596217e-01,  1.16160987e-01, -7.36229746e-01,  3.33543691e-01,\n        6.82037539e-02,  1.57439512e-01, -1.49496790e-01, -7.62151903e-01,\n        1.21209061e-01,  4.01482359e-01,  6.33693782e-01, -3.39163934e-02,\n        2.46779912e-01, -5.25786335e-01, -1.65006241e-01, -4.77303787e-01,\n        5.82476860e-01,  5.10092763e-02, -3.37430059e-01,  1.36668789e-01,\n        1.84152239e-01,  2.05395693e-01,  9.65471142e-02,  4.76887971e-01,\n        3.04254343e-01,  1.66852067e-01,  5.22181849e-02, -3.04459297e-01,\n       -6.67658260e-01,  6.13711821e-01, -5.74321467e-01,  2.40890492e-01,\n       -3.29857947e-01, -2.77119945e-02, -3.36535603e-01, -5.21736833e-01,\n       -4.07787596e-01,  3.61991694e-01, -1.32929935e-01,  1.59170118e-01,\n       -1.89405793e-01,  2.28164612e-01,  4.69053812e-02, -1.31667916e-01,\n       -2.01920285e-01,  2.81852986e-01, -9.98568654e-02, -6.22067838e-01,\n        1.48184709e-01, -6.15207158e-02,  2.68654375e-01,  1.85122972e-02,\n        7.75093324e-02, -1.23199144e+00, -2.42938224e-01, -1.51755572e-01,\n        3.76003286e-02,  6.97583474e-01, -4.65683225e-01, -2.75854068e-01,\n        1.42137536e-01, -4.52063415e-01, -2.71489587e-01, -1.98607487e-01,\n        3.53523318e-01, -3.88124000e-01, -2.01079991e-01, -1.96137085e-01,\n       -5.33989982e-01, -7.04992977e-01,  1.64615170e-02, -2.88684064e-01,\n       -4.95864577e-02, -2.17327955e-01, -6.56563539e-01, -6.37319038e-01,\n       -2.62129090e-01,  8.40846805e-02, -1.64848489e-01, -7.83481564e-02,\n       -1.16103546e-01,  7.89267687e-02, -8.17130135e-02,  2.50355631e-01,\n       -2.03392640e-01,  2.08756932e-01, -2.57663825e-02,  1.18790192e-01,\n       -3.41268699e-01, -1.87617408e-01, -4.28715545e-01,  5.81688099e-01,\n        4.38186067e-01,  5.92678177e-01, -1.85514569e-01, -7.58787571e-01,\n       -2.67598363e-01, -3.90152432e-02,  8.31515198e-01, -9.03899172e-02,\n        3.78187073e-01,  3.12825026e-01,  1.24645989e-02, -5.68594468e-01,\n        4.33188242e-01,  4.25460712e-01, -7.91178330e-01,  2.86954171e-01,\n        4.57905107e-01, -1.24622172e-01, -5.77581682e-01,  1.02489118e-01,\n        3.85601432e-01,  5.20269860e-01, -4.25350162e-01,  5.65595027e-01,\n        3.18160767e-02, -9.82244235e-02, -4.19146289e-01,  4.00483261e-01,\n        4.52015687e-01,  1.35617107e-01, -1.17627960e-01,  2.49796167e-02,\n        3.16033183e-01, -6.73049677e-02,  6.66267565e-02,  2.21749876e-01,\n        2.51775402e-01,  1.76945881e-01, -8.28150679e-01, -3.49136828e-02,\n       -1.29617136e-01,  4.67633167e-01, -3.16238663e-01,  4.04058980e-01,\n       -8.57329665e-01, -1.10161542e-01,  1.66192693e-02,  2.20327771e-01,\n       -3.57674415e-01, -1.44393271e-01, -4.86927731e-01,  2.65447270e-01,\n       -2.83401434e-02, -3.57781917e-01, -1.21887272e-01,  1.97193098e-01,\n        1.52178766e-01,  3.67721027e-01,  1.26151441e-01, -4.03583727e-01,\n        2.97050320e-01,  1.80471351e-01,  3.75137721e-01, -2.05391362e-01,\n       -1.58590980e-01, -3.05512031e-01,  3.55484967e-02,  2.49408592e-01,\n       -5.72637376e-02,  3.81290061e-01, -3.49944551e-01, -7.95599254e-02,\n       -6.42785976e-01,  5.66491291e-01,  2.45202389e-01,  7.16669170e-01,\n        1.96667257e-01,  3.74420504e-02,  7.21996203e-02, -2.09808427e-01,\n        1.02699455e-01, -6.06766914e-01,  2.12284211e-01,  2.47673842e-01,\n        1.03751137e-01, -3.39378005e-01,  5.14433024e-01,  3.11931622e-01,\n        8.08244657e-02,  3.82815001e-01,  4.85669517e-01, -1.65689835e-01,\n        1.43576268e-02,  1.23943435e-01, -9.25211202e-03, -1.41296003e-01,\n       -3.48262385e-01, -4.70309575e-01, -5.92200590e-01,  2.92422918e-01,\n        2.41574085e-01,  1.47921788e-01,  2.90264109e-01,  2.76435015e-01,\n        3.59677993e-01,  8.62648485e-03,  1.31305210e-01,  2.63764579e-01,\n       -1.38766771e-01,  6.32852436e-01,  1.84099655e-01, -5.98354213e-01,\n       -4.66997302e-01,  1.69165242e-01,  8.25597413e-02,  1.49393617e-01,\n       -1.64953657e-01, -9.16593764e-01, -1.38714713e-01,  3.72245595e-01,\n        6.82544419e-01, -1.72210264e-01,  3.26129332e-01,  3.42349457e-02,\n        2.59452682e-01,  4.86424655e-02,  4.15683563e-03,  3.66564176e-01,\n       -5.32937774e-01, -1.06743575e-01,  9.34972359e-02,  4.61189542e-02,\n       -3.55571576e-01, -2.41465869e-01, -8.50811570e-01, -8.15209794e-01,\n        7.33603588e-01,  2.96311808e-01,  2.27586712e-01,  7.05179805e-02,\n        7.28316813e-02, -3.68067107e-02,  1.24627028e-01,  1.29727161e-01,\n        3.37645251e-01,  7.79351762e-01, -3.61565639e-01, -2.93942475e-01,\n       -1.23572298e-01, -1.79574374e-01,  1.16265629e-01,  2.44098122e-01,\n        4.77043388e-01,  5.62151030e-02,  2.02764679e-01, -1.64112837e-01,\n        9.34446518e-02,  3.41483891e-01, -2.83639376e-01,  1.06800489e-01,\n        7.28293467e-02,  1.96982761e-01, -7.46061165e-01,  5.17903575e-01,\n       -1.79572039e-01, -3.78971504e-01,  3.03936503e-01,  5.34046896e-01,\n       -7.85656999e-01,  5.78977945e-02,  1.06169480e-01,  1.74424179e-01,\n       -3.86648784e-01,  7.35969873e-01, -1.93772083e-01,  4.96078836e-01,\n       -4.97967942e-02, -6.05793968e-02,  1.05907085e-01,  3.02361315e-01,\n        2.58558752e-01,  5.90942902e-01, -6.58718961e-01,  7.35601784e-01,\n        2.39628473e-01,  4.59061958e-01, -1.96138368e-01, -4.55954639e-01,\n       -4.09259951e-01, -5.08909171e-01,  1.35141515e-01,  2.43467113e-01,\n       -4.29613859e-02, -3.37643255e-01,  1.81733370e-01,  1.07063936e-01,\n       -2.45197007e-01, -2.26900071e-01, -1.05218110e-01, -3.49734214e-01,\n       -1.83674651e-01, -6.11445322e-01,  2.14335517e-01, -4.51800495e-01,\n       -5.26345273e-02,  7.22876430e-01, -3.20129361e-01, -4.65892510e-01,\n       -8.32036183e-01,  4.84407498e-01, -3.34958080e-01,  7.51974403e-02,\n       -7.40749645e-01,  4.66371149e-01, -3.81213134e-02,  2.39786226e-01,\n        1.37773055e-01,  1.96351752e-01,  3.88651837e-01,  7.02550599e-02,\n       -4.74990086e-01, -2.02867325e-01, -2.44460829e-01,  1.37667361e-01,\n        4.63005766e-01, -4.79039589e-01,  4.06109761e-01, -6.13629635e-02,\n       -6.69603346e-01,  2.16281129e-01, -1.13158310e-01,  6.22177862e-01,\n        1.21892654e-01, -1.67530278e-01, -3.24021111e-01, -2.63393443e-01,\n        5.03916202e-01, -8.01360160e-02, -2.12122654e-01,  3.22921701e-01,\n        4.25618465e-01, -2.87527740e-01, -8.38689619e-02, -2.98152064e-01,\n        5.19165594e-01,  1.06012254e-01, -2.66338153e-01, -6.14495200e-01,\n        2.40101730e-01, -5.12639782e-01,  1.67233859e-02,  7.39861971e-02,\n        7.10045907e-01,  3.34987681e-02, -2.70595657e-01,  2.11127886e-01,\n        6.39952165e-02,  6.33586279e-01,  1.66589147e-01,  1.82099124e-01,\n        2.15124279e-01, -2.36047898e-01, -2.61290604e-01, -2.89527745e-01,\n       -3.14871476e-01,  3.53419958e-01,  1.74075051e-02, -2.10228574e-01,\n       -1.50809058e-01, -2.57640479e-02,  8.65035494e-02,  2.45988816e-01,\n       -2.44829444e-01, -1.01734311e+00,  7.75828627e-01,  4.74411322e-01,\n       -1.74051234e-01, -8.61903430e-01,  3.33017850e-01,  3.87337234e-01,\n       -2.46071975e-02,  1.44135732e-01,  2.47774341e-01,  1.51129419e-01,\n        3.90176776e-01, -1.64561349e-02,  4.53804421e-02,  2.72178036e-01,\n        1.51970764e-01, -1.40081374e-01, -3.67087496e-01,  4.03007298e-01,\n        1.76178416e-02,  2.54562360e-01, -2.25167656e-01, -1.40819886e-01,\n       -3.22313674e-02, -2.00872988e-02,  9.85973685e-02,  4.96449259e-01,\n        5.70485349e-01, -1.74471381e-01, -1.75365836e-01,  1.34723177e-01,\n        7.62738679e-03,  1.10797408e-01,  2.25220428e-01,  2.63869747e-01,\n        4.52278608e-01, -1.48390715e-01,  5.92467841e-01,  3.12036791e-01,\n        6.74681022e-02, -5.74794198e-01,  7.86159332e-02, -3.35957178e-01,\n       -2.91681358e-01,  2.38103008e-01,  4.91348600e-01, -1.46444051e-01,\n       -2.38150736e-01, -5.34883912e-01,  2.39628473e-01, -9.46492300e-02,\n        4.73154499e-01, -4.90765319e-01,  1.05591581e-01, -4.65342449e-02,\n       -3.16974315e-01,  3.95380268e-01,  5.61863890e-01,  1.49131223e-01,\n       -4.06472467e-01,  6.40496093e-02, -1.29775414e-01,  7.49168484e-01,\n       -2.68544351e-01, -3.26023976e-01,  6.36848828e-01,  3.95750691e-01,\n        2.05133298e-01,  3.74033454e-01, -2.74696691e-01,  5.41198335e-01,\n       -1.39240028e-01, -7.06254995e-01, -6.22910235e-01, -4.18304943e-01,\n        3.00203032e-01, -6.08764584e-01,  4.82356718e-01, -3.35641673e-01,\n        2.13914844e-01,  6.65215882e-02,  4.63584191e-01,  2.57033287e-01,\n       -2.10386327e-01,  1.45500585e-01,  1.63279843e-03,  7.71961623e-02,\n       -3.11453509e-01,  1.46396849e-01,  3.09775674e-01, -5.26942660e-01,\n       -2.53820800e-01, -5.27786866e-01, -2.97097522e-01,  5.10593858e-01,\n       -4.35657173e-01, -1.23360153e-01, -1.37820783e-01, -2.23112207e-01,\n        2.58611336e-01,  1.57965353e-01, -5.15795354e-01, -1.31930837e-01,\n       -1.36085507e-01, -6.39262315e-01,  2.12387571e-01,  4.05426167e-01,\n       -1.83253453e-01,  1.51128893e-01,  5.02338679e-01, -1.39240028e-01,\n        4.72758046e-02,  4.37526693e-02,  3.28072609e-01, -3.29149608e-02,\n       -5.77266177e-01,  4.73572837e-01, -3.12322694e-02,  2.12337321e-01,\n        3.60834317e-01, -1.61431048e-01,  2.41205996e-01,  3.70981242e-01,\n        2.24957507e-01,  5.41117386e-02, -4.32239206e-01])\n\n\n\nsig[0][-1]\n\n626\n\n\n\nli.Is[626]\n\n1.1256660313590514\n\n\n\nobserved = li.Is[626]\nnull = li.rlisas[626,:]\n\n\n(null &gt;= observed).sum()\n\n0\n\n\n\nli.p_sim[626]\n\n0.001\n\n\n\nimport seaborn as sbn\nsbn.kdeplot(null, shade=True)\nplt.vlines(observed,0, 0.2, color='r')\nplt.xlabel(\"Local I for tract 626\")\n\nText(0.5, 0, 'Local I for tract 626')\n\n\n\n\n\n\n\n\n\n\nnsig = np.where(li.p_sim &gt; 0.05)\n\n\nnsig[0][0]\n\n0\n\n\n\nobserved = li.Is[0]\nnull = li.rlisas[0,:]\n\n\nimport seaborn as sbn\nsbn.kdeplot(null, shade=True)\nplt.vlines(observed,0, 2.0, color='r')\nplt.xlabel(\"Local I for tract 0\")\n\nText(0.5, 0, 'Local I for tract 0')\n\n\n\n\n\n\n\n\n\n\nli.p_sim[0]\n\n0.46",
    "crumbs": [
      "Week 11 3/26, 3/28",
      "Local Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-10/03_spatial_autocorrelation.html",
    "href": "week-10/03_spatial_autocorrelation.html",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "In this notebook we introduce methods of exploratory spatial data analysis that are intended to complement geovizualization through formal univariate and multivariate statistical tests for spatial clustering.\n\n\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport esda\nimport libpysal as lps\nimport contextily as cx\n\n\nimport seaborn as sns\nsns.set_context('notebook')\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n\n\nFor this exercise, we’ll use two datasets:\n\na set of polygons (census tracts) for the city of San Diego from the US Census American Community Survey 5-year estimates.\n\n\n\n\nscag = gpd.read_parquet(\"~/data/scag_region.parquet\")\n\n\nsan_diego = scag[scag.geoid.str[:5]=='06073']\n\n\nsan_diego.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 627 entries, 158 to 4567\nColumns: 194 entries, geoid to geometry\ndtypes: float64(191), geometry(1), int64(1), object(1)\nmemory usage: 955.2+ KB\n\n\n\nsan_diego = san_diego.dropna(subset=['median_home_value'])\n\n\nsan_diego = san_diego.to_crs(epsg=3857)\n\n\nf, ax = plt.subplots(figsize=(10,10))\n\nsan_diego.plot('median_home_value', ax=ax, alpha=0.6)\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\n\n\n\n\n\n\n\n\nsan_diego.median_home_value.hist()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12,12))\n\nsan_diego.dropna(subset=['median_home_value']).to_crs(epsg=3857).plot('median_home_value', legend=True, scheme='quantiles', cmap='GnBu', k=5, ax=ax, alpha=0.7)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Median Home Value (Quintiles)\", fontsize=16)\n\nplt.axis('off')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nVisual inspection of the map pattern for the prices allows us to search for spatial structure. If the spatial distribution of the prices was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the darker clusters along the coast, and a concentration of the lighter hues (lower prices) in the north central and south east. In the point data, the trees are too dense to make any sense of the pattern\nOur brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes.\nThe concept of spatial autocorrelation relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\nLet’s use PySAL to generate these two types of similarity measures.\n\n\nWe have already encountered spatial weights in a previous notebook. In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:\n\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'r'\n\n\n\n\nSo the spatial weight between neighborhoods \\(i\\) and \\(j\\) indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood \\(i\\) the spatial lag is defined as: \\[ylag_i = \\sum_j w_{i,j} y_j\\]\n\ny = san_diego['median_home_value']\nylag = lps.weights.lag_spatial(wq, y)\n\n\nf, ax = plt.subplots(1, figsize=(12, 12))\n\nsan_diego.assign(cl=ylag).plot(column='cl', scheme='quantiles', \\\n        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n        edgecolor='white', legend=True)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Spatial Lag Median Home Val (Quintiles)\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother.\n\nsan_diego['lag_median_pri'] = ylag\n\nf,ax = plt.subplots(1,2,figsize=(12,4))\n\nsan_diego.plot(column='median_home_value', ax=ax[0],\n        scheme=\"quantiles\",  k=5, cmap='GnBu')\n\n#ax[0].axis(san_diego.total_bounds[np.asarray([0,2,1,3])])\nax[0].set_title(\"Price\", fontsize=16)\n\nsan_diego.plot(column='lag_median_pri', ax=ax[1],\n        scheme='quantiles', cmap='GnBu', k=5)\n\ncx.add_basemap(ax[0], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\ncx.add_basemap(ax[1], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax[1].set_title(\"Spatial Lag Price\", fontsize=16)\nax[0].axis('off')\nax[1].axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nHowever, we still have the challenge of visually associating the value of the prices in a neighborhod with the value of the spatial lag of values for the focal unit. The latter is a weighted average of home prices in the focal county’s neighborhood.\nTo complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation.\n\n\n\n\n\n\n\nOne way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called joins. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\nEach unit can take on one of two values “Black” or “White”, analogous to the layout of a chessboard\n\nnrows, ncols = 9,9\nimage = np.zeros(nrows*ncols)\n\n# Set every other cell to 1\nimage[::2] = 1\n\n# Reshape things into a 9x9 grid.\nimage = image.reshape((nrows, ncols))\nplt.matshow(image, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nand so for a given pair of neighboring locations there are three different types of joins that can arise:\n\nBlack Black (BB)\nWhite White (WW)\nBlack White (or White Black) (BW)\n\nWe can use the esda package from PySAL to carry out join count analysis. In the case of our point data, the join counts can help us determine whether different varieties of trees tend to grow together, spread randomly through space, or compete with one another for precious resources\n\n\nWith polygon data, we can conduct an analysis using a contiguity matrix. For our housing price data, we need to first discretize the variable we’re using; to keep things simple, we’ll binarize our price data using the median so that “high” values are tracts whose median home price is above the city’s median and “low” values are those below\n\ny.median()\n\n405416.57303370786\n\n\n\nsan_diego.shape\n\n(627, 195)\n\n\n\nyb = y &gt; y.median()\nsum(yb)\n\n313\n\n\n\nyb = y &gt; y.median()\nlabels = [\"0 Low\", \"1 High\"]\nyb = [labels[i] for i in 1*yb] \nsan_diego['yb'] = yb\n\n\nfig, ax = plt.subplots(figsize=(12,12))\nsan_diego.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)\n\n\n\n\n\n\n\n\nThe spatial distribution of the binary variable immediately raises questions about the juxtaposition of the “black” and “white” areas.\nGiven that we have 308 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map?\n\nyb = 1 * (y &gt; y.median()) # convert back to binary\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'b'\nnp.random.seed(12345)\njc = esda.join_counts.Join_Counts(yb, wq)\n\nThe resulting object stores the observed counts for the different types of joins:\n\njc.bb\n\n754.0\n\n\n\njc.ww\n\n745.0\n\n\n\njc.bw\n\n475.0\n\n\nNote that the three cases exhaust all possibilities:\n\njc.bb + jc.ww + jc.bw\n\n1974.0\n\n\nand\n\nwq.s0 / 2\n\n1974.0\n\n\nwhich is the unique number of joins in the spatial weights object.\nOur object tells us we have observed 736 BB joins:\n\njc.bb\n\n754.0\n\n\nThe critical question for us, is whether this is a departure from what we would expect if the process generating the spatial distribution of the Black polygons were a completely random one? To answer this, PySAL uses random spatial permutations of the observed attribute values to generate a realization under the null of complete spatial randomness (CSR). This is repeated a large number of times (999 default) to construct a reference distribution to evaluate the statistical significance of our observed counts.\nThe average number of BB joins from the synthetic realizations is:\n\njc.mean_bb\n\n490.03103103103103\n\n\nwhich is less than our observed count. The question is whether our observed value is so different from the expectation that we would reject the null of CSR?\n\nimport seaborn as sbn\nsbn.kdeplot(jc.sim_bb, shade=True)\nplt.vlines(jc.bb, 0, 0.005, color='r')\nplt.vlines(jc.mean_bb, 0,0.005)\nplt.xlabel('BB Counts')\n\nText(0.5, 0, 'BB Counts')\n\n\n\n\n\n\n\n\n\nThe density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for our prices. Clearly our observed value is extremely high. A pseudo p-value summarizes this:\n\njc.p_sim_bb\n\n0.001\n\n\nSince this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in market prices.\n\n\n\nThe join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\nFirst, we transform our weights to be row-standardized, from the current binary state:\n\nwq.transform = 'r'\n\n\ny = san_diego['median_home_value']\n\nMoran’s I is a test for global autocorrelation for a continuous attribute:\n\nnp.random.seed(12345)\nmi = esda.moran.Moran(y, wq)\nmi.I\n\n0.660917168991019\n\n\nAgain, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations.\n\nfrom splot.esda import plot_moran\nplot_moran(mi, zstandard=True, figsize=(10,4))\nplt.show()\n\n\n\n\n\n\n\n\nOn the left, we have the reference distribution versus the observed statistic; on the right, we have a plot of the focal value against its spatial lag, for which the Moran I statistic serves as the slope\nHere our observed value is again in the upper tail\n\nmi.p_sim\n\n0.001",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-10/03_spatial_autocorrelation.html#imports",
    "href": "week-10/03_spatial_autocorrelation.html#imports",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport esda\nimport libpysal as lps\nimport contextily as cx\n\n\nimport seaborn as sns\nsns.set_context('notebook')\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(\"ignore\")",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-10/03_spatial_autocorrelation.html#data",
    "href": "week-10/03_spatial_autocorrelation.html#data",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "For this exercise, we’ll use two datasets:\n\na set of polygons (census tracts) for the city of San Diego from the US Census American Community Survey 5-year estimates.\n\n\n\n\nscag = gpd.read_parquet(\"~/data/scag_region.parquet\")\n\n\nsan_diego = scag[scag.geoid.str[:5]=='06073']\n\n\nsan_diego.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 627 entries, 158 to 4567\nColumns: 194 entries, geoid to geometry\ndtypes: float64(191), geometry(1), int64(1), object(1)\nmemory usage: 955.2+ KB\n\n\n\nsan_diego = san_diego.dropna(subset=['median_home_value'])\n\n\nsan_diego = san_diego.to_crs(epsg=3857)\n\n\nf, ax = plt.subplots(figsize=(10,10))\n\nsan_diego.plot('median_home_value', ax=ax, alpha=0.6)\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\n\n\n\n\n\n\n\n\nsan_diego.median_home_value.hist()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12,12))\n\nsan_diego.dropna(subset=['median_home_value']).to_crs(epsg=3857).plot('median_home_value', legend=True, scheme='quantiles', cmap='GnBu', k=5, ax=ax, alpha=0.7)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Median Home Value (Quintiles)\", fontsize=16)\n\nplt.axis('off')\nplt.tight_layout()",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-10/03_spatial_autocorrelation.html#spatial-autocorrelation",
    "href": "week-10/03_spatial_autocorrelation.html#spatial-autocorrelation",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "Visual inspection of the map pattern for the prices allows us to search for spatial structure. If the spatial distribution of the prices was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the darker clusters along the coast, and a concentration of the lighter hues (lower prices) in the north central and south east. In the point data, the trees are too dense to make any sense of the pattern\nOur brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes.\nThe concept of spatial autocorrelation relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\nLet’s use PySAL to generate these two types of similarity measures.\n\n\nWe have already encountered spatial weights in a previous notebook. In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:\n\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'r'\n\n\n\n\nSo the spatial weight between neighborhoods \\(i\\) and \\(j\\) indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood \\(i\\) the spatial lag is defined as: \\[ylag_i = \\sum_j w_{i,j} y_j\\]\n\ny = san_diego['median_home_value']\nylag = lps.weights.lag_spatial(wq, y)\n\n\nf, ax = plt.subplots(1, figsize=(12, 12))\n\nsan_diego.assign(cl=ylag).plot(column='cl', scheme='quantiles', \\\n        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n        edgecolor='white', legend=True)\n\ncx.add_basemap(ax, crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\nax.axis('off')\n\nplt.title(\"Spatial Lag Median Home Val (Quintiles)\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother.\n\nsan_diego['lag_median_pri'] = ylag\n\nf,ax = plt.subplots(1,2,figsize=(12,4))\n\nsan_diego.plot(column='median_home_value', ax=ax[0],\n        scheme=\"quantiles\",  k=5, cmap='GnBu')\n\n#ax[0].axis(san_diego.total_bounds[np.asarray([0,2,1,3])])\nax[0].set_title(\"Price\", fontsize=16)\n\nsan_diego.plot(column='lag_median_pri', ax=ax[1],\n        scheme='quantiles', cmap='GnBu', k=5)\n\ncx.add_basemap(ax[0], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\ncx.add_basemap(ax[1], crs=san_diego.crs.to_string(), source=cx.providers.CartoDB.Positron)\n\nax[1].set_title(\"Spatial Lag Price\", fontsize=16)\nax[0].axis('off')\nax[1].axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nHowever, we still have the challenge of visually associating the value of the prices in a neighborhod with the value of the spatial lag of values for the focal unit. The latter is a weighted average of home prices in the focal county’s neighborhood.\nTo complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation.",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Spatial Autocorrelation"
    ]
  },
  {
    "objectID": "week-10/03_spatial_autocorrelation.html#join-counts",
    "href": "week-10/03_spatial_autocorrelation.html#join-counts",
    "title": "Spatial Autocorrelation",
    "section": "",
    "text": "One way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called joins. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\nEach unit can take on one of two values “Black” or “White”, analogous to the layout of a chessboard\n\nnrows, ncols = 9,9\nimage = np.zeros(nrows*ncols)\n\n# Set every other cell to 1\nimage[::2] = 1\n\n# Reshape things into a 9x9 grid.\nimage = image.reshape((nrows, ncols))\nplt.matshow(image, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nand so for a given pair of neighboring locations there are three different types of joins that can arise:\n\nBlack Black (BB)\nWhite White (WW)\nBlack White (or White Black) (BW)\n\nWe can use the esda package from PySAL to carry out join count analysis. In the case of our point data, the join counts can help us determine whether different varieties of trees tend to grow together, spread randomly through space, or compete with one another for precious resources\n\n\nWith polygon data, we can conduct an analysis using a contiguity matrix. For our housing price data, we need to first discretize the variable we’re using; to keep things simple, we’ll binarize our price data using the median so that “high” values are tracts whose median home price is above the city’s median and “low” values are those below\n\ny.median()\n\n405416.57303370786\n\n\n\nsan_diego.shape\n\n(627, 195)\n\n\n\nyb = y &gt; y.median()\nsum(yb)\n\n313\n\n\n\nyb = y &gt; y.median()\nlabels = [\"0 Low\", \"1 High\"]\nyb = [labels[i] for i in 1*yb] \nsan_diego['yb'] = yb\n\n\nfig, ax = plt.subplots(figsize=(12,12))\nsan_diego.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)\n\n\n\n\n\n\n\n\nThe spatial distribution of the binary variable immediately raises questions about the juxtaposition of the “black” and “white” areas.\nGiven that we have 308 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map?\n\nyb = 1 * (y &gt; y.median()) # convert back to binary\nwq =  lps.weights.Queen.from_dataframe(san_diego)\nwq.transform = 'b'\nnp.random.seed(12345)\njc = esda.join_counts.Join_Counts(yb, wq)\n\nThe resulting object stores the observed counts for the different types of joins:\n\njc.bb\n\n754.0\n\n\n\njc.ww\n\n745.0\n\n\n\njc.bw\n\n475.0\n\n\nNote that the three cases exhaust all possibilities:\n\njc.bb + jc.ww + jc.bw\n\n1974.0\n\n\nand\n\nwq.s0 / 2\n\n1974.0\n\n\nwhich is the unique number of joins in the spatial weights object.\nOur object tells us we have observed 736 BB joins:\n\njc.bb\n\n754.0\n\n\nThe critical question for us, is whether this is a departure from what we would expect if the process generating the spatial distribution of the Black polygons were a completely random one? To answer this, PySAL uses random spatial permutations of the observed attribute values to generate a realization under the null of complete spatial randomness (CSR). This is repeated a large number of times (999 default) to construct a reference distribution to evaluate the statistical significance of our observed counts.\nThe average number of BB joins from the synthetic realizations is:\n\njc.mean_bb\n\n490.03103103103103\n\n\nwhich is less than our observed count. The question is whether our observed value is so different from the expectation that we would reject the null of CSR?\n\nimport seaborn as sbn\nsbn.kdeplot(jc.sim_bb, shade=True)\nplt.vlines(jc.bb, 0, 0.005, color='r')\nplt.vlines(jc.mean_bb, 0,0.005)\nplt.xlabel('BB Counts')\n\nText(0.5, 0, 'BB Counts')\n\n\n\n\n\n\n\n\n\nThe density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for our prices. Clearly our observed value is extremely high. A pseudo p-value summarizes this:\n\njc.p_sim_bb\n\n0.001\n\n\nSince this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in market prices.\n\n\n\nThe join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\nFirst, we transform our weights to be row-standardized, from the current binary state:\n\nwq.transform = 'r'\n\n\ny = san_diego['median_home_value']\n\nMoran’s I is a test for global autocorrelation for a continuous attribute:\n\nnp.random.seed(12345)\nmi = esda.moran.Moran(y, wq)\nmi.I\n\n0.660917168991019\n\n\nAgain, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations.\n\nfrom splot.esda import plot_moran\nplot_moran(mi, zstandard=True, figsize=(10,4))\nplt.show()\n\n\n\n\n\n\n\n\nOn the left, we have the reference distribution versus the observed statistic; on the right, we have a plot of the focal value against its spatial lag, for which the Moran I statistic serves as the slope\nHere our observed value is again in the upper tail\n\nmi.p_sim\n\n0.001",
    "crumbs": [
      "Week 10 3/19, 3/21",
      "Spatial Autocorrelation"
    ]
  }
]